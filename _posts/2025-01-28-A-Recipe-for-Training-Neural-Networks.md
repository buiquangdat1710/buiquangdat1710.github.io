---
title: "A Recipe for Training Neural Networks"
date: 2025-01-28 00:00:00  + 0800
categories: [Deep Learning]
tags: [deep learning]
---
---


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>


Blog nÃ y Ä‘Æ°á»£c tÃ´i dá»‹ch sang tiáº¿ng viá»‡t tá»« blog [A Recipe for Training Neural Networks - Andrej Karpathy](https://karpathy.github.io/2019/04/25/recipe/). Náº¿u cÃ³ thá»i gian, hÃ£y xem thá»­ blog cháº¥t lÆ°á»£ng cá»§a Andrej Karpathy - Ä‘á»“ng sÃ¡ng láº­p OpenAI nhÃ©.


- Má»™t vÃ i tuáº§n trÆ°á»›c, tÃ´i Ä‘Ã£ Ä‘Äƒng má»™t tweet vá» ["the most common neural net mistakes"](https://x.com/karpathy/status/1013244313327681536?lang=en), liá»‡t kÃª má»™t vÃ i váº¥n Ä‘á» thÆ°á»ng gáº·p liÃªn quan Ä‘áº¿n viá»‡c training neural nets. Tweet Ä‘Ã³ nháº­n Ä‘Æ°á»£c nhiá»u sá»± chÃº Ã½ hÆ¡n tÃ´i mong Ä‘á»£i (bao gá»“m cáº£ má»™t [webinar](https://www.bigmarker.com/missinglink-ai/PyTorch-Code-to-Unpack-Andrej-Karpathy-s-6-Most-Common-NN-Mistakes) :)). RÃµ rÃ ng, ráº¥t nhiá»u ngÆ°á»i Ä‘Ã£ tá»«ng tráº£i qua khoáº£ng cÃ¡ch lá»›n giá»¯a "here is how a convolutional layer works" vÃ  "our convnet achieves state of the art results".

- VÃ¬ váº­y, tÃ´i nghÄ© cÃ³ thá»ƒ lÃ  má»™t Ã½ hay khi "phá»§i bá»¥i" blog cá»§a mÃ¬nh Ä‘á»ƒ má»Ÿ rá»™ng tweet nÃ y thÃ nh má»™t bÃ i viáº¿t dÃ i hÆ¡n, Ä‘Ãºng nhÆ° chá»§ Ä‘á» nÃ y xá»©ng Ä‘Ã¡ng. Tuy nhiÃªn, thay vÃ¬ Ä‘i sÃ¢u vÃ o liá»‡t kÃª cÃ¡c lá»—i phá»• biáº¿n hÆ¡n hoáº·c má»Ÿ rá»™ng chÃºng, tÃ´i muá»‘n Ä‘Ã o sÃ¢u hÆ¡n má»™t chÃºt vÃ  nÃ³i vá» cÃ¡ch chÃºng ta cÃ³ thá»ƒ trÃ¡nh nhá»¯ng lá»—i nÃ y hoÃ n toÃ n (hoáº·c sá»­a chÃºng ráº¥t nhanh). BÃ­ quyáº¿t Ä‘á»ƒ lÃ m nhÆ° váº­y lÃ  tuÃ¢n theo má»™t quy trÃ¬nh nháº¥t Ä‘á»‹nh, theo nhÆ° tÃ´i biáº¿t thÃ¬ quy trÃ¬nh nÃ y khÃ´ng thÆ°á»ng Ä‘Æ°á»£c ghi chÃ©p láº¡i. ChÃºng ta hÃ£y báº¯t Ä‘áº§u vá»›i hai quan sÃ¡t quan trá»ng thÃºc Ä‘áº©y quy trÃ¬nh nÃ y.

## 1. Huáº¥n luyá»‡n má»™t máº¡ng neuron lÃ  má»™t sá»± trá»«u tÆ°á»£ng khÃ´ng rÃµ rÃ ng (Neural net training is a leaky)

- NgÆ°á»i ta cho ráº±ng viá»‡c báº¯t Ä‘áº§u vá»›i Ä‘Ã o táº¡o máº¡ng nÆ¡-ron lÃ  dá»… dÃ ng. Nhiá»u thÆ° viá»‡n vÃ  framework tá»± hÃ o khi hiá»ƒn thá»‹ cÃ¡c Ä‘oáº¡n code tháº§n ká»³ dÃ i 30 dÃ²ng Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» dá»¯ liá»‡u cá»§a báº¡n, táº¡o áº¥n tÆ°á»£ng (thá»±c táº¿ khÃ´ng pháº£i nhÆ° váº­y) ráº±ng má»i thá»© Ä‘á»u cÃ³ thá»ƒ cáº¯m vÃ o vÃ  cháº¡y ngay. VÃ­ dá»¥ phá»• biáº¿n nhÆ° sau:

```python
>>> your_data = # thÃªm bá»™ dá»¯ liá»‡u tuyá»‡t vá»i cá»§a báº¡n vÃ o Ä‘Ã¢y
>>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)
# chinh phá»¥c tháº¿ giá»›i á»Ÿ Ä‘Ã¢y
```

- Nhá»¯ng thÆ° viá»‡n vÃ  vÃ­ dá»¥ nÃ y kÃ­ch hoáº¡t pháº§n nÃ£o bá»™ cá»§a chÃºng ta quen thuá»™c vá»›i pháº§n má»m tiÃªu chuáº©n - má»™t nÆ¡i mÃ  cÃ¡c API gá»n gÃ ng vÃ  trá»«u tÆ°á»£ng thÆ°á»ng Ä‘áº¡t Ä‘Æ°á»£c. Äá»ƒ minh há»a, thÆ° viá»‡n [Requests](https://docs.python-requests.org/en/latest/) Ä‘Æ°á»£c sá»­ dá»¥ng:

```python
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
```


- Äiá»u Ä‘Ã³ tháº­t tuyá»‡t vá»i! Má»™t nhÃ  phÃ¡t triá»ƒn dÅ©ng cáº£m Ä‘Ã£ giÃºp báº¡n xá»­ lÃ½ nhá»¯ng thá»© phá»©c táº¡p nhÆ° chuá»—i truy váº¥n, URL, yÃªu cáº§u GET/POST, káº¿t ná»‘i HTTP, vÃ  giáº¥u Ä‘i sá»± phá»©c táº¡p chá»‰ trong vÃ i dÃ²ng mÃ£. ÄÃ¢y lÃ  Ä‘iá»u chÃºng ta thÆ°á»ng quen thuá»™c vÃ  mong Ä‘á»£i. NhÆ°ng tháº­t khÃ´ng may, neural nets khÃ´ng hoáº¡t Ä‘á»™ng theo cÃ¡ch nÃ y. ChÃºng khÃ´ng pháº£i lÃ  "cÃ´ng nghá»‡ cÃ³ sáºµn" mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng ngay. Ngay khi báº¡n chá»‰ thay Ä‘á»•i má»™t chÃºt trong viá»‡c Ä‘Ã o táº¡o mÃ´ hÃ¬nh ImageNet, báº¡n sáº½ nháº­n ra Ä‘iá»u nÃ y.

- TÃ´i Ä‘Ã£ cá»‘ gáº¯ng lÃ m rÃµ quan Ä‘iá»ƒm nÃ y trong bÃ i viáº¿t cá»§a mÃ¬nh â€œYes you should understand backpropâ€, báº±ng cÃ¡ch chá»‰ ra ráº±ng thuáº­t toÃ¡n lan truyá»n ngÆ°á»£c chá»‰ lÃ  má»™t "leaky abstraction". Tuy nhiÃªn, thá»±c táº¿ cÃ²n tá»‡ hÆ¡n nhiá»u. Backpropagation + SGD khÃ´ng tháº§n ká»³ lÃ m cho máº¡ng há»™i tá»¥ nhanh hÆ¡n. Batch normalization cÅ©ng khÃ´ng lÃ m cho nÃ³ há»™i tá»¥ tháº§n ká»³. RNNs cÅ©ng khÃ´ng chá»‰ cáº§n â€œthÃªm vÃ o lÃ  cháº¡yâ€. VÃ  viá»‡c báº¡n cÃ³ thá»ƒ Ä‘áº·t bÃ i toÃ¡n cá»§a mÃ¬nh theo cÃ¡ch reinforcement learning (RL) khÃ´ng cÃ³ nghÄ©a lÃ  báº¡n nÃªn lÃ m váº­y. Náº¿u báº¡n sá»­ dá»¥ng cÃ´ng nghá»‡ mÃ  khÃ´ng hiá»ƒu cÃ¡ch nÃ³ hoáº¡t Ä‘á»™ng, báº¡n ráº¥t dá»… tháº¥t báº¡i. Äiá»u nÃ y dáº«n chÃºng ta Ä‘áº¿n...


## 2. Neural net training fails silently

- Khi báº¡n lÃ m sai hoáº·c cáº¥u hÃ¬nh sai code, thÆ°á»ng báº¡n sáº½ nháº­n Ä‘Æ°á»£c má»™t lá»—i hiá»ƒn thá»‹ rÃµ rÃ ng. VÃ­ dá»¥: báº¡n nháº­p nháº§m má»™t sá»‘ nguyÃªn vÃ o chá»— cáº§n má»™t chuá»—i kÃ½ tá»±, hÃ m chá»‰ nháº­n Ä‘Æ°á»£c 3 tham sá»‘ thay vÃ¬ 4, hoáº·c má»™t khÃ³a nÃ o Ä‘Ã³ khÃ´ng tá»“n táº¡i. Nhá»¯ng lá»—i nÃ y ráº¥t rÃµ rÃ ng vÃ  báº¡n thÆ°á»ng cÃ³ thá»ƒ táº¡o bÃ i kiá»ƒm thá»­ (unit test) Ä‘á»ƒ kiá»ƒm tra cÃ¡c chá»©c nÄƒng.
- NhÆ°ng khi  training neural nets, má»i thá»© trá»Ÿ nÃªn phá»©c táº¡p hÆ¡n nhiá»u. Code cá»§a báº¡n cÃ³ thá»ƒ hoÃ n toÃ n Ä‘Ãºng vá» cÃº phÃ¡p, nhÆ°ng toÃ n bá»™ há»‡ thá»‘ng láº¡i hoáº¡t Ä‘á»™ng khÃ´ng nhÆ° mong muá»‘n, vÃ  Ä‘iá»u Ä‘Ã³ ráº¥t khÃ³ phÃ¡t hiá»‡n. CÃ¡i â€œpossible error surfaceâ€ ráº¥t rá»™ng (bao gá»“m cáº£ lá»—i logic láº«n lá»—i cÃº phÃ¡p), vÃ  viá»‡c kiá»ƒm tra chÃºng lÃ  ráº¥t khÃ³. VÃ­ dá»¥, báº¡n cÃ³ thá»ƒ quÃªn láº­t labels khi báº¡n Ä‘Ã£ láº­t cÃ¡c hÃ¬nh áº£nh (features) tring quÃ¡ trÃ¬nh tÄƒng cÆ°á»ng dá»¯ liá»‡u. MÃ´ hÃ¬nh cá»§a báº¡n váº«n cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng á»•n bá»Ÿi vÃ¬ nÃ³ cÃ³ thá»ƒ tá»± phÃ¡t hiá»‡n áº£nh bá»‹ láº­t vÃ  sá»­a láº¡i dá»± Ä‘oÃ¡n. Hoáº·c, mÃ´ hÃ¬nh autoregressive cá»§a báº¡n vÃ´ tÃ¬nh Ä‘Æ°a dá»¯ liá»‡u sai lÃ m Ä‘áº§u vÃ o do lá»—i nhá» khÃ³ phÃ¡t hiá»‡n. CÃ³ thá»ƒ báº¡n cá»‘ gáº¯ng gá»t gradients nhÆ°ng láº¡i xÃ³a máº¥t hÃ m loss, khiáº¿n má»™t sá»‘ vÃ­ dá»¥ ngoáº¡i lá»‡ bá»‹ bá» qua trong khi huáº¥n luyá»‡n. Báº¡n sá»­ dá»¥ng trá»ng sá»‘ tá»« má»™t checkpoint Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, nhÆ°ng láº¡i khÃ´ng dÃ¹ng Ä‘Ãºng trung bÃ¬nh (mean) ban Ä‘áº§u. Hoáº·c báº¡n chá»‰ lÃ m há»ng cÃ i Ä‘áº·t cho regularization strengths, learning rate, decay rate, model size,... Káº¿t quáº£ lÃ , máº¡ng nÆ¡-ron bá»‹ cáº¥u hÃ¬nh sai sáº½ khÃ´ng luÃ´n Ä‘Æ°a ra lá»—i rÃµ rÃ ng. Náº¿u may máº¯n, nÃ³ sáº½ bÃ¡o lá»—i; cÃ²n khÃ´ng, nÃ³ sáº½ tiáº¿p tá»¥c huáº¥n luyá»‡n nhÆ°ng hoáº¡t Ä‘á»™ng Ã¢m tháº§m... vÃ  sai.

- Káº¿t luáº­n: (vÃ  Ä‘iá»u nÃ y ráº¥t quan trá»ng Ä‘á»ƒ nháº¥n máº¡nh) cÃ¡ch tiáº¿p cáº­n kiá»ƒu "nhanh vÃ  liá»u" vá»›i viá»‡c huáº¥n luyá»‡n máº¡ng nÆ¡-ron khÃ´ng hiá»‡u quáº£ vÃ  chá»‰ dáº«n Ä‘áº¿n khá»• sá»Ÿ. Tuy nhiÃªn, "khá»• sá»Ÿ" lÃ  má»™t pháº§n hoÃ n toÃ n tá»± nhiÃªn trong quÃ¡ trÃ¬nh lÃ m cho máº¡ng hoáº¡t Ä‘á»™ng tá»‘t, vÃ  báº¡n cÃ³ thá»ƒ giáº£m bá»›t Ä‘iá»u Ä‘Ã³ báº±ng cÃ¡ch cáº©n tháº­n, phÃ²ng thá»§, nghi ngá», vÃ  bá»‹ Ã¡m áº£nh vá»›i viá»‡c trá»±c quan hÃ³a má»i thá»© cÃ³ thá»ƒ.

- Nhá»¯ng pháº©m cháº¥t quan trá»ng nháº¥t Ä‘á»ƒ thÃ nh cÃ´ng trong viá»‡c há»c sÃ¢u lÃ  kiÃªn nháº«n vÃ  chÃº Ã½ Ä‘áº¿n chi tiáº¿t.


# CÃ´ng thá»©c 


- Dá»±a trÃªn hai thá»±c táº¿ trÃªn, tÃ´i Ä‘Ã£ phÃ¡t triá»ƒn má»™t quy trÃ¬nh cá»¥ thá»ƒ cho báº£n thÃ¢n khi Ã¡p dá»¥ng neural net vÃ o má»™t váº¥n Ä‘á» má»›i, vÃ  tÃ´i sáº½ cá»‘ gáº¯ng mÃ´ táº£ nÃ³. Báº¡n sáº½ tháº¥y ráº±ng quy trÃ¬nh nÃ y tuÃ¢n theo hai nguyÃªn táº¯c trÃªn má»™t cÃ¡ch nghiÃªm tÃºc. Cá»¥ thá»ƒ, nÃ³ báº¯t Ä‘áº§u tá»« Ä‘Æ¡n giáº£n Ä‘áº¿n phá»©c táº¡p, vÃ  á»Ÿ má»—i bÆ°á»›c, chÃºng ta Ä‘Æ°a ra cÃ¡c giáº£ thuyáº¿t cá»¥ thá»ƒ vá» Ä‘iá»u gÃ¬ sáº½ xáº£y ra, sau Ä‘Ã³ hoáº·c xÃ¡c nháº­n báº±ng thá»­ nghiá»‡m hoáº·c Ä‘iá»u tra cho Ä‘áº¿n khi tÃ¬m tháº¥y váº¥n Ä‘á». Äiá»u chÃºng ta cá»‘ gáº¯ng ngÄƒn cháº·n lÃ  viá»‡c giá»›i thiá»‡u quÃ¡ nhiá»u sá»± phá»©c táº¡p â€œchÆ°a Ä‘Æ°á»£c xÃ¡c minhâ€ cÃ¹ng lÃºc, vÃ¬ Ä‘iá»u Ä‘Ã³ sáº½ dá»… dáº«n Ä‘áº¿n lá»—i/sai cáº¥u hÃ¬nh mÃ  cÃ³ thá»ƒ máº¥t ráº¥t nhiá»u thá»i gian Ä‘á»ƒ phÃ¡t hiá»‡n (náº¿u cÃ³ thá»ƒ phÃ¡t hiá»‡n Ä‘Æ°á»£c). Náº¿u báº¡n viáº¿t code neural net nhÆ° thá»ƒ Ä‘ang huáº¥n luyá»‡n má»™t cÃ¡i má»›i, báº¡n nÃªn sá»­ dá»¥ng learning rate ráº¥t nhá» vÃ  thá»­ nghiá»‡m tá»«ng bÆ°á»›c, sau Ä‘Ã³ Ä‘Ã¡nh giÃ¡ toÃ n bá»™ táº­p test sau má»—i láº§n láº·p.

## 1.  Become one with the data

- BÆ°á»›c Ä‘áº§u tiÃªn khÃ´ng pháº£i lÃ  viáº¿t code mÃ  lÃ  kiá»ƒm tra dá»¯ liá»‡u. Äiá»u nÃ y ráº¥t quan trá»ng! TÃ´i thÆ°á»ng dÃ nh ráº¥t nhiá»u thá»i gian (tÃ­nh báº±ng giá») Ä‘á»ƒ xem xÃ©t hÃ ng nghÃ¬n máº«u dá»¯ liá»‡u, hiá»ƒu phÃ¢n bá»‘ cá»§a chÃºng vÃ  tÃ¬m kiáº¿m cÃ¡c Ä‘áº·c Ä‘iá»ƒm báº¥t thÆ°á»ng.

- VÃ­ dá»¥:
  - CÃ³ dá»¯ liá»‡u nÃ o bá»‹ trÃ¹ng láº·p khÃ´ng?
  - CÃ³ hÃ¬nh áº£nh hay nhÃ£n nÃ o bá»‹ lá»—i khÃ´ng?
  - Dá»¯ liá»‡u cÃ³ bá»‹ máº¥t cÃ¢n báº±ng khÃ´ng?
  - Dá»¯ liá»‡u cÃ³ chá»©a thiÃªn lá»‡ch nÃ o khÃ´ng?

- TÃ´i cÅ©ng thá»­ tá»± phÃ¢n loáº¡i dá»¯ liá»‡u báº±ng máº¯t thÆ°á»ng Ä‘á»ƒ xem cáº§n loáº¡i kiáº¿n trÃºc nÃ o. VÃ­ dá»¥:
  - Chá»‰ cáº§n Ä‘áº·c trÆ°ng cá»¥c bá»™ hay cáº§n cáº£ bá»‘i cáº£nh tá»•ng thá»ƒ ?
  - Biáº¿n Ä‘á»•i dá»¯ liá»‡u cÃ³ lá»›n khÃ´ng? CÃ³ thá»ƒ loáº¡i bá» biáº¿n Ä‘á»•i nÃ o trÆ°á»›c khi Ä‘Æ°a vÃ o model ?
  - áº¢nh cÃ³ thá»ƒ giáº£m kÃ­ch thÆ°á»›c Ä‘Æ°á»£c khÃ´ng mÃ  váº«n giá»¯ Ä‘á»§ thÃ´ng tin ?
  - NhÃ£n cÃ³ nhiá»u nhiá»…u khÃ´ng ?
- NgoÃ i ra, neural net thá»±c cháº¥t lÃ  má»™t phiÃªn báº£n nÃ©n cá»§a dá»¯ liá»‡u, nÃªn náº¿u model dá»± Ä‘oÃ¡n sai, hÃ£y kiá»ƒm tra xem sai láº§m Ä‘Ã³ Ä‘áº¿n tá»« Ä‘Ã¢u. Náº¿u model Ä‘Æ°a ra dá»± Ä‘oÃ¡n khÃ´ng khá»›p vá»›i nhá»¯ng gÃ¬ báº¡n quan sÃ¡t Ä‘Æ°á»£c tá»« dá»¯ liá»‡u, thÃ¬ cháº¯c cháº¯n cÃ³ váº¥n Ä‘á».

- Cuá»‘i cÃ¹ng, hÃ£y viáº¿t má»™t sá»‘ Ä‘oáº¡n code Ä‘Æ¡n giáº£n Ä‘á»ƒ tÃ¬m kiáº¿m, lá»c, hoáº·c sáº¯p xáº¿p dá»¯ liá»‡u dá»±a trÃªn cÃ¡c yáº¿u tá»‘ nhÆ° loáº¡i nhÃ£n, sá»‘ lÆ°á»£ng annotation, kÃ­ch thÆ°á»›c annotation,... vÃ  váº½ biá»ƒu Ä‘á»“ Ä‘á»ƒ xem phÃ¢n bá»‘ dá»¯ liá»‡u. Nhá»¯ng giÃ¡ trá»‹ báº¥t thÆ°á»ng (outliers) thÆ°á»ng sáº½ giÃºp phÃ¡t hiá»‡n lá»—i trong dá»¯ liá»‡u hoáº·c quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½.

## 2. Set up the end-to-end training/evaluation skeleton + get dumb baselines

- Sau khi Ä‘Ã£ hiá»ƒu dá»¯ liá»‡u, chÃºng ta cÃ³ thá»ƒ ngay láº­p tá»©c huáº¥n luyá»‡n má»™t model hoÃ nh trÃ¡ng nhÆ° Multi-scale ASPP FPN ResNet khÃ´ng? KhÃ´ng! ÄÃ³ lÃ  con Ä‘Æ°á»ng dáº«n Ä‘áº¿n Ä‘au khá»•.

- BÆ°á»›c tiáº¿p theo lÃ  thiáº¿t láº­p má»™t quy trÃ¬nh huáº¥n luyá»‡n + Ä‘Ã¡nh giÃ¡ hoÃ n chá»‰nh Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng model hoáº¡t Ä‘á»™ng Ä‘Ãºng báº±ng má»™t loáº¡t thá»­ nghiá»‡m. á» giai Ä‘oáº¡n nÃ y, tá»‘t nháº¥t nÃªn chá»n má»™t model Ä‘Æ¡n giáº£n mÃ  báº¡n gáº§n nhÆ° khÃ´ng thá»ƒ lÃ m sai, vÃ­ dá»¥ nhÆ° má»™t linear classifier hoáº·c má»™t ConvNet nhá». Sau Ä‘Ã³, huáº¥n luyá»‡n model, theo dÃµi loss, accuracy, kiá»ƒm tra dá»± Ä‘oÃ¡n cá»§a model vÃ  thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m nhá» Ä‘á»ƒ kiá»ƒm chá»©ng tá»«ng giáº£ thuyáº¿t.

#### Máº¹o & lÆ°u Ã½ quan trá»ng:
- **fix random seed:** LuÃ´n Ä‘áº·t giÃ¡ trá»‹ random seed cá»‘ Ä‘á»‹nh Ä‘á»ƒ khi cháº¡y code hai láº§n, báº¡n sáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ giá»‘ng nhau. Äiá»u nÃ y giÃºp loáº¡i bá» cÃ¡c yáº¿u tá»‘ ngáº«u nhiÃªn vÃ  giÃºp báº¡n giá»¯ Ä‘Æ°á»£c sá»± tá»‰nh tÃ¡o.
- **simplify:** Táº¯t táº¥t cáº£ nhá»¯ng thá»© khÃ´ng cáº§n thiáº¿t, vÃ­ dá»¥ nhÆ° data augmentation. Data augmentation lÃ  má»™t phÆ°Æ¡ng phÃ¡p regularization há»¯u Ã­ch, nhÆ°ng á»Ÿ giai Ä‘oáº¡n nÃ y, nÃ³ cÃ³ thá»ƒ chá»‰ lÃ m phá»©c táº¡p váº¥n Ä‘á» mÃ  thÃ´i.
- **add significant digits to your eval:** Khi váº½ test loss, hÃ£y cháº¡y Ä‘Ã¡nh giÃ¡ trÃªn toÃ n bá»™ test set thay vÃ¬ chá»‰ hiá»ƒn thá»‹ giÃ¡ trá»‹ loss trung bÃ¬nh trÃªn má»™t batch. Äiá»u nÃ y giÃºp Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c thay vÃ¬ chá»‰ dá»±a vÃ o smoothing trÃªn TensorBoard.
- **verify loss @ init:** Kiá»ƒm tra giÃ¡ trá»‹ loss ban Ä‘áº§u xem cÃ³ Ä‘Ãºng khÃ´ng. Loss ban Ä‘áº§u lÃ  loss mÃ  mÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n gÃ¬ (mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ngáº«u nhiÃªn). VÃ­ dá»¥, náº¿u layer cuá»‘i cÃ¹ng lÃ  softmax, thÃ¬ loss lÃºc khá»Ÿi táº¡o pháº£i lÃ  -log(1/n_classes). CÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng tá»± cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n cho L2 regression, Huber loss, v.v.
- **init well:** Khá»Ÿi táº¡o lá»›p cuá»‘i cÃ¹ng cÃ³ trá»ng sá»‘ tháº­t chÃ­nh xÃ¡c. VÃ­ dá»¥ náº¿u báº¡n Ä‘ang lÃ m há»“i quy má»™t vÃ i giÃ¡ trá»‹ cÃ³ trung bÃ¬nh lÃ  50 thÃ¬ nÃªn khá»Ÿi táº¡o giÃ¡ trá»‹ bias cuá»‘i cÃ¹ng lÃ  50. Náº¿u báº¡n cÃ³ má»™t dataset bá»‹ máº¥t cÃ¢n báº±ng vá»›i tá»‰ lá»‡ 1:10 cho positive:negative, Ä‘áº·t bias cá»§a logits sao cho máº¡ng cá»§a báº¡n dá»± Ä‘oÃ¡n xÃ¡c suáº¥t 0.1 á»Ÿ bÆ°á»›c khá»Ÿi táº¡o. Äiá»u nÃ y giÃºp quÃ¡ trÃ¬nh há»™i tá»¥ nhanh hÆ¡n vÃ  trÃ¡nh hiá»‡n tÆ°á»£ng loss giáº£m cháº­m trong vÃ i epoch Ä‘áº§u tiÃªn.
- **human baseline:** Theo dÃµi cÃ¡c chá»‰ sá»‘ mÃ  con ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu vÃ  kiá»ƒm tra Ä‘Æ°á»£c, cháº³ng háº¡n nhÆ° accuracy. Náº¿u cÃ³ thá»ƒ, hÃ£y Ä‘Ã¡nh giÃ¡ káº¿t quáº£ cá»§a model báº±ng cÃ¡ch so sÃ¡nh vá»›i Ä‘á»™ chÃ­nh xÃ¡c cá»§a con ngÆ°á»i trÃªn cÃ¹ng má»™t táº­p dá»¯ liá»‡u.
- **input-independent baseline:** Huáº¥n luyá»‡n má»™t model mÃ  Ä‘áº§u vÃ o khÃ´ng liÃªn quan Ä‘áº¿n dá»¯ liá»‡u thá»±c táº¿ (vÃ­ dá»¥: Ä‘áº·t toÃ n bá»™ input vá» 0). Model nÃ y pháº£i cÃ³ hiá»‡u suáº¥t kÃ©m hÆ¡n so vá»›i model thá»±c táº¿. Náº¿u khÃ´ng, cÃ³ thá»ƒ model chÆ°a há»c Ä‘Æ°á»£c gÃ¬ tá»« dá»¯ liá»‡u cáº£.
- **overfit one batch:** Huáº¥n luyá»‡n má»™t batch nhá» (chá»‰ má»™t vÃ i máº«u, tháº­m chÃ­ chá»‰ 2 máº«u). ThÃªm cÃ¡c lá»›p hoáº·c bá»™ lá»c Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng model cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c loss ráº¥t nhá» (gáº§n 0). Náº¿u khÃ´ng thá»ƒ, cÃ³ thá»ƒ model cá»§a báº¡n Ä‘ang gáº·p váº¥n Ä‘á». HÃ£y váº½ cáº£ nhÃ£n tháº­t vÃ  dá»± Ä‘oÃ¡n trÃªn cÃ¹ng má»™t biá»ƒu Ä‘á»“ Ä‘á»ƒ kiá»ƒm tra xem model cÃ³ há»c Ä‘Ãºng hay khÃ´ng.
- **verify decreasing training loss:** á» giai Ä‘oáº¡n nÃ y, model cá»§a báº¡n cÃ³ thá»ƒ Ä‘ang underfitting vÃ¬ dá»¯ liá»‡u cÃ²n Ä‘Æ¡n giáº£n. HÃ£y thá»­ tÄƒng má»™t chÃºt Ä‘á»™ phá»©c táº¡p cá»§a model vÃ  kiá»ƒm tra xem training loss cÃ³ giáº£m nhÆ° mong Ä‘á»£i khÃ´ng.
- **visualize just before the net:** NÆ¡i tá»‘t nháº¥t Ä‘á»ƒ kiá»ƒm tra dá»¯ liá»‡u lÃ  ngay trÆ°á»›c khi nÃ³ Ä‘i vÃ o model, tá»©c lÃ  y_hat = model(x). Viá»‡c trá»±c quan hÃ³a dá»¯ liá»‡u á»Ÿ thá»i Ä‘iá»ƒm nÃ y giÃºp báº¡n kiá»ƒm tra chÃ­nh xÃ¡c nhá»¯ng gÃ¬ Ä‘ang Ä‘i vÃ o model, giÃºp phÃ¡t hiá»‡n lá»—i trong quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ hoáº·c augmentation.
- **visualize prediction dynamics:** HÃ£y theo dÃµi dá»± Ä‘oÃ¡n cá»§a model trÃªn má»™t batch cá»‘ Ä‘á»‹nh trong suá»‘t quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Viá»‡c quan sÃ¡t sá»± thay Ä‘á»•i cá»§a dá»± Ä‘oÃ¡n theo thá»i gian cÃ³ thá»ƒ giÃºp báº¡n phÃ¡t hiá»‡n báº¥t á»•n, cháº³ng háº¡n nhÆ° model â€œváº­t lá»™nâ€ Ä‘á»ƒ khá»›p vá»›i dá»¯ liá»‡u hoáº·c nháº¡y cáº£m quÃ¡ má»©c vá»›i noise. Learning rate quÃ¡ tháº¥p hoáº·c quÃ¡ cao cÅ©ng cÃ³ thá»ƒ nháº­n biáº¿t báº±ng cÃ¡ch nÃ y.
- **use backprop to chart dependencies:** Code deep learning thÆ°á»ng cÃ³ nhiá»u phÃ©p toÃ¡n vector hÃ³a vÃ  broadcast phá»©c táº¡p. Má»™t lá»—i phá»• biáº¿n lÃ  sá»­ dá»¥ng view thay vÃ¬ transpose/permute, khiáº¿n dá»¯ liá»‡u bá»‹ trá»™n láº«n giá»¯a cÃ¡c batch. ThÆ°á»ng thÃ¬ model váº«n cÃ³ thá»ƒ huáº¥n luyá»‡n Ä‘Æ°á»£c vÃ¬ nÃ³ há»c cÃ¡ch bá» qua dá»¯ liá»‡u sai, nhÆ°ng Ä‘á»ƒ kiá»ƒm tra lá»—i nÃ y, báº¡n cÃ³ thá»ƒ Ä‘áº·t loss thÃ nh má»™t hÃ m Ä‘Æ¡n giáº£n (vÃ­ dá»¥ tá»•ng cÃ¡c Ä‘áº§u ra), cháº¡y backprop vá» input vÃ  kiá»ƒm tra xem gradient cÃ³ giÃ¡ trá»‹ khÃ¡c 0 á»Ÿ vá»‹ trÃ­ cáº§n thiáº¿t khÃ´ng.
- **generalize a special case:** Má»™t máº¹o láº­p trÃ¬nh quan trá»ng lÃ  Ä‘á»«ng cá»‘ viáº¿t ngay má»™t hÃ m tá»•ng quÃ¡t phá»©c táº¡p. Thay vÃ o Ä‘Ã³, hÃ£y viáº¿t má»™t phiÃªn báº£n Ä‘Æ¡n giáº£n trÆ°á»›c, Ä‘áº£m báº£o nÃ³ hoáº¡t Ä‘á»™ng Ä‘Ãºng, rá»“i má»›i má»Ÿ rá»™ng nÃ³. Äiá»u nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch khi vectorizing code: tÃ´i thÆ°á»ng viáº¿t phiÃªn báº£n vÃ²ng láº·p Ä‘áº§y Ä‘á»§ trÆ°á»›c, kiá»ƒm tra káº¿t quáº£, sau Ä‘Ã³ má»›i tá»‘i Æ°u hÃ³a tá»«ng bÆ°á»›c má»™t

## 3. Overfit

- á» giai Ä‘oáº¡n nÃ y, chÃºng ta Ä‘Ã£ hiá»ƒu rÃµ vá» táº­p dá»¯ liá»‡u vÃ  cÃ³ quy trÃ¬nh huáº¥n luyá»‡n + Ä‘Ã¡nh giÃ¡ hoÃ n chá»‰nh. Vá»›i má»—i model, ta cÃ³ thá»ƒ tÃ­nh toÃ¡n láº¡i má»™t cÃ¡ch Ä‘Ã¡ng tin cáº­y cÃ¡c chá»‰ sá»‘ Ä‘o lÆ°á»ng hiá»‡u suáº¥t. Äá»“ng thá»i, ta cÅ©ng cÃ³ cÃ¡c káº¿t quáº£ benchmark Ä‘á»ƒ so sÃ¡nh, bao gá»“m cáº£ baseline Ä‘Æ¡n giáº£n láº«n hiá»‡u suáº¥t cá»§a con ngÆ°á»i (má»¥c tiÃªu cao nháº¥t). Giai Ä‘oáº¡n nÃ y lÃ  lÃºc Ä‘á»ƒ thá»­ nghiá»‡m vÃ  cáº£i thiá»‡n model.

- CÃ¡ch tiáº¿p cáº­n cá»§a tÃ´i Ä‘á»ƒ tÃ¬m model tá»‘t gá»“m 2 bÆ°á»›c:
1. Chá»n má»™t model Ä‘á»§ lá»›n Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ overfit (tá»©c lÃ  Ä‘áº¡t loss ráº¥t tháº¥p trÃªn táº­p huáº¥n luyá»‡n).
2. Äiá»u chá»‰nh láº¡i model Ä‘á»ƒ giáº£m overfit (tÄƒng loss trÃªn táº­p huáº¥n luyá»‡n má»™t chÃºt nhÆ°ng giáº£m loss trÃªn táº­p validation).

- LÃ½ do tÃ´i thÃ­ch phÆ°Æ¡ng phÃ¡p nÃ y lÃ  náº¿u ta khÃ´ng thá»ƒ Ä‘áº¡t lá»—i tháº¥p vá»›i báº¥t ká»³ model nÃ o, cÃ³ thá»ƒ dá»¯ liá»‡u hoáº·c code cá»§a ta cÃ³ váº¥n Ä‘á».

### Má»™t sá»‘ máº¹o quan trá»ng:

- **Chá»n model phÃ¹ há»£p:** Äá»ƒ giáº£m loss trÃªn táº­p huáº¥n luyá»‡n, báº¡n cáº§n má»™t kiáº¿n trÃºc phÃ¹ há»£p. Lá»i khuyÃªn cá»§a tÃ´i: **"Äá»«ng lÃ m anh hÃ¹ng!"** Nhiá»u ngÆ°á»i thÃ­ch sÃ¡ng táº¡o quÃ¡ má»©c, thá»­ nhá»¯ng model phá»©c táº¡p má»™t cÃ¡ch khÃ´ng cáº§n thiáº¿t. Äá»«ng máº¯c báº«y nÃ y! HÃ£y tÃ¬m má»™t bÃ i bÃ¡o uy tÃ­n, chá»n model Ä‘Æ¡n giáº£n nháº¥t cá»§a há» vÃ  sá»­ dá»¥ng trÆ°á»›c. VÃ­ dá»¥, náº¿u lÃ m bÃ i toÃ¡n phÃ¢n loáº¡i áº£nh, Ä‘á»«ng cá»‘ sÃ¡ng táº¡o ngay tá»« Ä‘áº§u, cá»© dÃ¹ng ResNet-50 trÆ°á»›c Ä‘Ã£. Sau nÃ y, náº¿u cáº§n, báº¡n cÃ³ thá»ƒ thá»­ nghiá»‡m vá»›i model phá»©c táº¡p hÆ¡n Ä‘á»ƒ cáº£i thiá»‡n.
- **Adam is safe:** á» giai Ä‘oáº¡n Ä‘áº§u, tÃ´i thÆ°á»ng dÃ¹ng Adam vá»›i learning rate 3e-4. Adam cÃ³ kháº£ nÄƒng thÃ­ch nghi tá»‘t vá»›i hyperparameter, ngay cáº£ khi báº¡n chá»n learning rate chÆ°a chuáº©n. Tuy nhiÃªn, náº¿u dÃ¹ng SGD Ä‘Æ°á»£c tá»‘i Æ°u tá»‘t thÃ¬ nÃ³ váº«n cÃ³ thá»ƒ vÆ°á»£t trá»™i hÆ¡n. (LÆ°u Ã½: náº¿u dÃ¹ng RNN hoáº·c model dáº¡ng sequence, Adam lÃ  lá»±a chá»n phá»• biáº¿n).
- **Chá»‰ thÃªm Ä‘á»™ phá»©c táº¡p má»™t chÃºt má»—i láº§n:** Náº¿u báº¡n muá»‘n thÃªm nhiá»u tÃ­n hiá»‡u Ä‘áº§u vÃ o (features) cho model, hÃ£y thÃªm tá»«ng cÃ¡i má»™t vÃ  kiá»ƒm tra xem cÃ³ cáº£i thiá»‡n hiá»‡u suáº¥t khÃ´ng. Äá»«ng thÃªm táº¥t cáº£ cÃ¹ng lÃºc mÃ  khÃ´ng kiá»ƒm tra! Má»™t cÃ¡ch khÃ¡c lÃ  thá»­ vá»›i táº­p dá»¯ liá»‡u nhá» trÆ°á»›c, rá»“i tÄƒng kÃ­ch thÆ°á»›c sau.
- **Cáº©n tháº­n vá»›i learning rate decay:** Náº¿u báº¡n dÃ¹ng code tá»« dá»± Ã¡n khÃ¡c, hÃ£y kiá»ƒm tra learning rate decay. Trong má»™t sá»‘ trÆ°á»ng há»£p, learning rate cÃ³ thá»ƒ giáº£m quÃ¡ nhanh khiáº¿n model khÃ´ng há»c Ä‘Æ°á»£c ná»¯a. VÃ­ dá»¥, vá»›i ImageNet, learning rate cÃ³ thá»ƒ giáº£m 10 láº§n sau má»—i 30 epochs, nhÆ°ng náº¿u báº¡n khÃ´ng train trÃªn ImageNet thÃ¬ cÃ³ thá»ƒ khÃ´ng cáº§n lÃ m váº­y. CÃ¡ nhÃ¢n tÃ´i thÆ°á»ng vÃ´ hiá»‡u hÃ³a learning rate decay á»Ÿ giai Ä‘oáº¡n Ä‘áº§u vÃ  tá»± Ä‘iá»u chá»‰nh thá»§ cÃ´ng.

## 4. Regularize

- BÃ¢y giá», chÃºng ta Ä‘Ã£ cÃ³ má»™t model lá»›n Ä‘á»§ tá»‘t trÃªn táº­p huáº¥n luyá»‡n. Äáº¿n lÃºc Ä‘iá»u chá»‰nh Ä‘á»ƒ giáº£m overfit vÃ  tÄƒng Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p validation, dÃ¹ pháº£i Ä‘Ã¡nh Ä‘á»•i má»™t chÃºt Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p huáº¥n luyá»‡n. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ máº¹o há»¯u Ã­ch:
- **ThÃªm dá»¯ liá»‡u thá»±c táº¿:** CÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ regularize model trong thá»±c táº¿ lÃ  thu tháº­p thÃªm dá»¯ liá»‡u huáº¥n luyá»‡n. Nhiá»u ngÆ°á»i máº¥t quÃ¡ nhiá»u thá»i gian tá»‘i Æ°u trÃªn táº­p dá»¯ liá»‡u nhá» thay vÃ¬ cá»‘ gáº¯ng má»Ÿ rá»™ng dá»¯ liá»‡u. Náº¿u cÃ³ thá»ƒ, hÃ£y luÃ´n Æ°u tiÃªn thu tháº­p thÃªm dá»¯ liá»‡u thá»±c, vÃ¬ Ä‘iá»u nÃ y gáº§n nhÆ° luÃ´n Ä‘áº£m báº£o cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a model. Má»™t cÃ¡ch khÃ¡c lÃ  sá»­ dá»¥ng ensemble models, nhÆ°ng chá»‰ khi báº¡n cÃ³ Ä‘á»§ tÃ i nguyÃªn (vÃ¬ chÃºng tá»‘n nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n).
- **Data augmentation:** Náº¿u khÃ´ng thá»ƒ cÃ³ dá»¯ liá»‡u thá»±c, hÃ£y thá»­ data augmentation (táº¡o dá»¯ liá»‡u nhÃ¢n táº¡o). HÃ£y sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p máº¡nh máº½ hÆ¡n nhÆ° xoay áº£nh, cáº¯t, thay Ä‘á»•i mÃ u sáº¯c, v.v.
- **Creative augmentation:**Náº¿u augmentation thÃ´ng thÆ°á»ng váº«n chÆ°a Ä‘á»§, cÃ³ thá»ƒ thá»­ cÃ¡c phÆ°Æ¡ng phÃ¡p sÃ¡ng táº¡o hÆ¡n nhÆ°:
  - [Domain randomization](https://openai.com/index/learning-dexterity/) (lÃ m dá»¯ liá»‡u Ä‘a dáº¡ng hÆ¡n báº±ng cÃ¡ch thay Ä‘á»•i ngáº«u nhiÃªn)
  - DÃ¹ng mÃ´ phá»ng ([simulation](https://vladlen.info/publications/playing-data-ground-truth-computer-games/)) Ä‘á»ƒ táº¡o dá»¯ liá»‡u giáº£ láº­p
  - Sá»­ dá»¥ng GANs hoáº·c cÃ¡c phÆ°Æ¡ng phÃ¡p AI khÃ¡c Ä‘á»ƒ táº¡o dá»¯ liá»‡u má»›i.

- **Pretrain náº¿u cÃ³ thá»ƒ:** Náº¿u báº¡n cÃ³ má»™t máº¡ng pretrained, hÃ£y táº­n dá»¥ng nÃ³ thay vÃ¬ huáº¥n luyá»‡n tá»« Ä‘áº§u. Äiá»u nÃ y thÆ°á»ng cÃ³ Ã­ch, Ä‘áº·c biá»‡t vá»›i bÃ i toÃ¡n cÃ³ Ã­t dá»¯ liá»‡u.

- **Chá»‰ dÃ¹ng supervised pretraining:** KhÃ´ng nÃªn quÃ¡ ká»³ vá»ng vÃ o unsupervised pretraining. Trong computer vision, unsupervised pretraining khÃ´ng mang láº¡i hiá»‡u quáº£ rÃµ rÃ ng. Tuy nhiÃªn, trong NLP (xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn), nÃ³ láº¡i ráº¥t há»¯u Ã­ch (vÃ­ dá»¥ nhÆ° BERT).
- **Giáº£m sá»‘ chiá»u cá»§a input:** Loáº¡i bá» cÃ¡c features khÃ´ng cáº§n thiáº¿t Ä‘á»ƒ trÃ¡nh overfitting. Náº¿u dá»¯ liá»‡u cá»§a báº¡n nhá», viá»‡c giá»¯ quÃ¡ nhiá»u features cÃ³ thá»ƒ gÃ¢y overfit.
- **DÃ¹ng model nhá» hÆ¡n** Náº¿u cÃ³ thá»ƒ, hÃ£y giáº£m kÃ­ch thÆ°á»›c model báº±ng cÃ¡ch thay tháº¿ cÃ¡c pháº§n dÆ° thá»«a. VÃ­ dá»¥, trÆ°á»›c Ä‘Ã¢y, cÃ¡c model cho ImageNet thÆ°á»ng dÃ¹ng Fully Connected Layers á»Ÿ pháº§n cuá»‘i, nhÆ°ng bÃ¢y giá», ta cÃ³ thá»ƒ thay báº±ng average pooling, giÃºp giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ tham sá»‘.
- **Giáº£m batch size:** Náº¿u dÃ¹ng batch normalization, batch nhá» cÃ³ thá»ƒ giÃºp regularize model. Batch lá»›n hÆ¡n cÃ³ thá»ƒ lÃ m giáº£m Ä‘á»™ ngáº«u nhiÃªn cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
- **DÃ¹ng dropout:** Náº¿u dÃ¹ng ConvNets, cÃ³ thá»ƒ thá»­ dropout2d (spatial dropout), nhÆ°ng cáº©n tháº­n vÃ¬ dropout cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng khÃ´ng tá»‘t Ä‘áº¿n batch normalization.
- **TÄƒng weight decay:** HÃ£y thá»­ tÄƒng weight decay Ä‘á»ƒ giáº£m overfitting.
- **DÃ¹ng early stopping:** Dá»«ng huáº¥n luyá»‡n dá»±a trÃªn validation loss Ä‘á»ƒ trÃ¡nh model overfit quÃ¡ má»©c.
- **Thá»­ model lá»›n hÆ¡n:** Cuá»‘i cÃ¹ng, náº¿u cÃ¡c cÃ¡ch trÃªn chÆ°a Ä‘á»§, hÃ£y thá»­ model lá»›n hÆ¡n. ÄÃ´i khi, model lá»›n hÆ¡n nhÆ°ng dá»«ng sá»›m (early stopping) láº¡i hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n so vá»›i model nhá» hÆ¡n.

- Cuá»‘i cÃ¹ng, Ä‘á»ƒ cÃ³ thÃªm sá»± tá»± tin ráº±ng máº¡ng cá»§a báº¡n lÃ  má»™t bá»™ phÃ¢n loáº¡i há»£p lÃ½, báº¡n nÃªn trá»±c quan hÃ³a the networkâ€™s first-layer weights vÃ  Ä‘áº£m báº£o báº¡n cÃ³ Ä‘Æ°á»£c cÃ¡c cáº¡nh Ä‘áº¹p cÃ³ Ã½ nghÄ©a. Náº¿u bá»™ lá»c lá»›p Ä‘áº§u tiÃªn cá»§a báº¡n trÃ´ng giá»‘ng nhÆ° nhiá»…u thÃ¬ cÃ³ thá»ƒ cÃ³ Ä‘iá»u gÃ¬ Ä‘Ã³ khÃ´ng á»•n. TÆ°Æ¡ng tá»± nhÆ° váº­y, activations bÃªn trong máº¡ng Ä‘Ã´i khi cÃ³ thá»ƒ hiá»ƒn thá»‹ cÃ¡c hiá»‡n váº­t ká»³ láº¡ vÃ  gá»£i Ã½ vá» cÃ¡c váº¥n Ä‘á».

## 5. Tune (Tá»‘i Æ°u)

- BÃ¢y giá», báº¡n Ä‘Ã£ quen thuá»™c vá»›i táº­p dá»¯ liá»‡u vÃ  cÃ³ thá»ƒ thá»­ nghiá»‡m nhiá»u kiáº¿n trÃºc model khÃ¡c nhau Ä‘á»ƒ Ä‘áº¡t validation loss tháº¥p. Má»™t sá»‘ máº¹o á»Ÿ giai Ä‘oáº¡n nÃ y:
- **DÃ¹ng random search thay vÃ¬ grid search:** Khi tinh chá»‰nh nhiá»u hyperparameter, nhiá»u ngÆ°á»i thÃ­ch dÃ¹ng grid search (thá»­ nghiá»‡m toÃ n bá»™ cÃ¡c giÃ¡ trá»‹), nhÆ°ng thá»±c táº¿, random search láº¡i hiá»‡u quáº£ hÆ¡n. LÃ½ do lÃ  má»™t sá»‘ tham sá»‘ quan trá»ng hÆ¡n nhá»¯ng tham sá»‘ khÃ¡c, vÃ¬ váº­y viá»‡c thá»­ ngáº«u nhiÃªn thÆ°á»ng mang láº¡i káº¿t quáº£ tá»‘t hÆ¡n.
- **DÃ¹ng ká»¹ thuáº­t tá»‘i Æ°u hyperparameter:** CÃ³ ráº¥t nhiá»u cÃ´ng cá»¥ tá»‘i Æ°u hyperparameter dá»±a trÃªn Bayesian optimization. Má»™t sá»‘ Ä‘á»“ng nghiá»‡p cá»§a tÃ´i Ä‘Ã£ thÃ nh cÃ´ng vá»›i chÃºng, nhÆ°ng theo kinh nghiá»‡m cÃ¡ nhÃ¢n, cÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ tÃ¬m model phÃ¹ há»£p váº«n lÃ â€¦ dÃ¹ng má»™t thá»±c táº­p sinh Ä‘á»ƒ thá»­ nghiá»‡m táº¥t cáº£ (Ä‘Ã¹a thÃ´i ğŸ˜†).

## 6. Squeeze out the juice (Táº­n dá»¥ng tá»‘i Ä‘a model)

- Khi Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c kiáº¿n trÃºc vÃ  hyperparameter tá»‘t nháº¥t, váº«n cÃ²n má»™t sá»‘ cÃ¡ch Ä‘á»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t:
- **DÃ¹ng ensemble models:** Ensemble (káº¿t há»£p nhiá»u model) gáº§n nhÆ° luÃ´n giÃºp tÄƒng thÃªm 2% Ä‘á»™ chÃ­nh xÃ¡c. Náº¿u khÃ´ng thá»ƒ dÃ¹ng ensemble khi cháº¡y thá»±c táº¿, hÃ£y thá»­ distillation (chuyá»ƒn kiáº¿n thá»©c tá»« model lá»›n sang model nhá» hÆ¡n).
- **Tiáº¿p tá»¥c huáº¥n luyá»‡n lÃ¢u hÆ¡n:** Nhiá»u ngÆ°á»i dá»«ng huáº¥n luyá»‡n khi validation loss khÃ´ng giáº£m ná»¯a, nhÆ°ng trong nhiá»u trÆ°á»ng há»£p, model váº«n cÃ³ thá»ƒ tiáº¿p tá»¥c cáº£i thiá»‡n sau má»™t thá»i gian dÃ i. TÃ´i tá»«ng Ä‘á»ƒ model train trong ká»³ nghá»‰ Ä‘Ã´ng, vÃ  khi quay láº¡i vÃ o thÃ¡ng 1, nÃ³ Ä‘Ã£ trá»Ÿ thÃ nh state-of-the-art (SOTA)! ğŸ˜†

## Conclusion (Tá»•ng káº¿t)


- Náº¿u Ä‘Ã£ Ä‘áº¿n giai Ä‘oáº¡n nÃ y, báº¡n cÃ³ táº¥t cáº£ yáº¿u tá»‘ Ä‘á»ƒ thÃ nh cÃ´ng:
- âœ… Hiá»ƒu rÃµ cÃ´ng nghá»‡, táº­p dá»¯ liá»‡u vÃ  bÃ i toÃ¡n
- âœ… XÃ¢y dá»±ng há»‡ thá»‘ng huáº¥n luyá»‡n/Ä‘Ã¡nh giÃ¡ bÃ i báº£n
- âœ… Äáº¡t Ä‘á»™ chÃ­nh xÃ¡c cao vÃ  thá»­ nghiá»‡m cÃ¡c model ngÃ y cÃ ng phá»©c táº¡p
- âœ… Cáº£i thiá»‡n hiá»‡u suáº¥t qua tá»«ng bÆ°á»›c

BÃ¢y giá», báº¡n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ Ä‘á»c nhiá»u bÃ i bÃ¡o hÆ¡n, thá»­ nghiá»‡m model lá»›n hÆ¡n, hoáº·câ€¦ Ä‘i nghá»‰ má»™t chÃºt sau khi model train xong! ğŸš€

