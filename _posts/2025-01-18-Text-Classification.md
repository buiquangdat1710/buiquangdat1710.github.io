---
title: "Text Classification"
date: 2025-01-18 00:00:00  + 0800
categories: [Natural Language Processing with Transformers Book]
tags: [transfomers]
---
---


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>


- PhÃ¢n loáº¡i vÄƒn báº£n lÃ  má»™t trong nhá»¯ng nhiá»‡m vá»¥ phá»• biáº¿n nháº¥t trong NLP; NÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cho nhiá»u á»©ng dá»¥ng, cháº³ng háº¡n nhÆ° gáº¯n tháº» pháº£n há»“i cá»§a khÃ¡ch hÃ ng vÃ o cÃ¡c danh má»¥c hoáº·c Ä‘á»‹nh tuyáº¿n phiáº¿u há»— trá»£ theo ngÃ´n ngá»¯ cá»§a há». Ráº¥t cÃ³ thá»ƒ bá»™ lá»c email cá»§a báº¡n Ä‘ang sá»­ dá»¥ng phÃ¢n loáº¡i vÄƒn báº£n Ä‘á»ƒ báº£o vá»‡ há»™p thÆ° Ä‘áº¿n cá»§a báº¡n khá»i má»™t Ä‘á»‘ng thÆ° rÃ¡c !
- Má»™t loáº¡i phÃ¢n loáº¡i vÄƒn báº£n phá»• biáº¿n khÃ¡c lÃ  phÃ¢n tÃ­ch cáº£m xÃºc (sentiment analysis), nháº±m xÃ¡c Ä‘á»‹nh cáº£m xÃºc cá»§a má»™t vÄƒn báº£n nháº¥t Ä‘á»‹nh. VÃ­ dá»¥, má»™t cÃ´ng ty nhÆ° Tesla cÃ³ thá»ƒ phÃ¢n tÃ­ch cÃ¡c bÃ i Ä‘Äƒng trÃªn Twitter nhÆ° trong HÃ¬nh 2-1 Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem má»i ngÆ°á»i cÃ³ thÃ­ch nÃ³c xe má»›i cá»§a há» hay khÃ´ng.

![anh](./image/298.png)

- BÃ¢y giá» hÃ£y tÆ°á»Ÿng tÆ°á»£ng ráº±ng báº¡n lÃ  má»™t nhÃ  khoa há»c dá»¯ liá»‡u cáº§n xÃ¢y dá»±ng má»™t há»‡ thá»‘ng cÃ³ thá»ƒ tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh cÃ¡c tráº¡ng thÃ¡i cáº£m xÃºc nhÆ° "tá»©c giáº­n" hoáº·c "niá»m vui" mÃ  má»i ngÆ°á»i thá»ƒ hiá»‡n vá» sáº£n pháº©m cá»§a cÃ´ng ty báº¡n trÃªn Twitter. Trong chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ giáº£i quyáº¿t nhiá»‡m vá»¥ nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng má»™t biáº¿n thá»ƒ cá»§a BERT Ä‘Æ°á»£c gá»i lÃ  [DistilBERT](https://arxiv.org/abs/1910.01108). Æ¯u Ä‘iá»ƒm chÃ­nh cá»§a mÃ´ hÃ¬nh nÃ y lÃ  nÃ³ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i BERT, Ä‘á»“ng thá»i nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ vÃ  hiá»‡u quáº£ hÆ¡n. Äiá»u nÃ y cho phÃ©p chÃºng ta train má»™t bá»™ phÃ¢n loáº¡i trong vÃ i phÃºt vÃ  náº¿u báº¡n muá»‘n train má»™t mÃ´ hÃ¬nh BERT lá»›n hÆ¡n, báº¡n cÃ³ thá»ƒ chá»‰ cáº§n thay Ä‘á»•i checkpoint cá»§a pretrained model. Má»™t checkpoints tÆ°Æ¡ng á»©ng vá»›i táº­p há»£p cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c táº£i vÃ o má»™t kiáº¿n trÃºc transformer nháº¥t Ä‘á»‹nh.
- ÄÃ¢y cÅ©ng sáº½ lÃ  cuá»™c gáº·p gá»¡ Ä‘áº§u tiÃªn cá»§a chÃºng ta vá»›i ba trong sá»‘ cÃ¡c thÆ° viá»‡n cá»‘t lÃµi tá»« há»‡ sinh thÃ¡i Hugging Face: ğŸ¤— Datasets, ğŸ¤— Tokenizers vÃ  ğŸ¤— Transformers. NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 2-2, cÃ¡c thÆ° viá»‡n nÃ y sáº½ cho phÃ©p chÃºng ta nhanh chÃ³ng chuyá»ƒn tá»« vÄƒn báº£n thÃ´ sang má»™t fine-tuned model cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ suy luáº­n trÃªn cÃ¡c bÃ i tweet má»›i.

![anh](./image/299.png)

## 1. The Dataset


- Äá»ƒ xÃ¢y dá»±ng cÃ´ng cá»¥ phÃ¢n loáº¡i cáº£m xÃºc, chÃºng ta sáº½ sá»­ dá»¥ng má»™t bá»™ data tuyá»‡t vá»i tá»« má»™t bÃ i bÃ¡o khÃ¡m phÃ¡ cÃ¡ch cáº£m xÃºc Ä‘Æ°á»£c thá»ƒ hiá»‡n trong cÃ¡c tin nháº¯n [Twitter báº±ng tiáº¿ng Anh](https://aclanthology.org/D18-1404/). Háº§u háº¿t cÃ¡c bá»™ data phÃ¢n loáº¡i cáº£m xÃºc khÃ¡c chá»‰ cÃ³ hai nhÃ£n lÃ  "postive" vÃ  "negative", bá»™ data nÃ y chá»©a sÃ¡u nhÃ£n cáº£m xÃºc cÆ¡ báº£n: anger, disgust, fear, joy, sadness, and surprise. Vá»›i má»™t tweet, nhiá»‡m vá»¥ cá»§a chÃºng ta sáº½ lÃ  train má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ phÃ¢n loáº¡i nÃ³ thÃ nh má»™t trong nhá»¯ng cáº£m xÃºc nÃ y.

### A First Look at Hugging Face Datasets

- ChÃºng ta sáº½ sá»­ dá»¥ng ğŸ¤— Datasets Ä‘á»ƒ táº£i bá»™ data tá»« [Hugging Face Hub](https://github.com/huggingface/datasets). ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m `list_datasets()` Ä‘á»ƒ xem nhá»¯ng bá»™ dá»¯ liá»‡u nÃ o cÃ³ sáºµn trÃªn Hub:

```python
from huggingface_hub import list_datasets

all_datasets = list(list_datasets())  
print(f"There are {len(all_datasets)} datasets currently available on the Hub")
print(f"The first 10 are: {all_datasets[:10]}")
```

- ChÃºng ta tháº¥y ráº±ng má»—i táº­p dataset Ä‘Æ°á»£c Ä‘áº·t má»™t tÃªn, vÃ¬ váº­y hÃ£y táº£i táº­p emotion dataset báº±ng hÃ m `load_dataset()`:

```python
from datasets import load_dataset

emotions = load_dataset("emotion")
```

- Náº¿u chÃºng ta in ra biáº¿n `emotions`:

```python
print(emotions)

# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label'],
#         num_rows: 16000
#     })
#     validation: Dataset({
#         features: ['text', 'label'],
#         num_rows: 2000
#     })
#     test: Dataset({
#         features: ['text', 'label'],
#         num_rows: 2000
#     })
# })
```

- ChÃºng ta sáº½ tháº¥y ráº¥t giá»‘ng vá»›i tá»« Ä‘iá»ƒn trong Python, vá»›i má»—i khÃ³a tÆ°Æ¡ng á»©ng vá»›i má»™t táº­p dá»¯ liá»‡u. ChÃºng ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c cÃº phÃ¡p cá»§a tá»« Ä‘iá»ƒn Ä‘á»ƒ truy cáº­p vÃ o táº­p dá»¯ liá»‡u cá»¥ thá»ƒ:

```python
train_ds = emotions["train"]
train_ds

# Dataset({
#     features: ['text', 'label'],
#     num_rows: 16000
# })
```

- Code trÃªn tráº£ vá» má»™t thá»±c thá»ƒ cá»§a lá»›p Dataset. Äá»‘i tÆ°á»£ng Dataset lÃ  má»™t trong nhá»¯ng cáº¥u trÃºc dá»¯ liá»‡u cá»‘t lÃµi trong ğŸ¤— Datasets vÃ  chÃºng ta sáº½ khÃ¡m phÃ¡ nhiá»u tÃ­nh nÄƒng cá»§a nÃ³ trong suá»‘t cuá»‘n sÃ¡ch nÃ y. Äá»‘i vá»›i ngÆ°á»i má»›i báº¯t Ä‘áº§u, nÃ³ hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° má»™t máº£ng hoáº·c danh sÃ¡ch Python thÃ´ng thÆ°á»ng, vÃ¬ váº­y chÃºng ta cÃ³ thá»ƒ truy váº¥n Ä‘á»™ dÃ i cá»§a nÃ³:

```python
len(train_ds) # 16000
```

- Hoáº·c truy cáº­p vÃ o má»™t pháº§n tá»­ cá»¥ thá»ƒ báº±ng chá»‰ sá»‘:

```python
train_ds[0]
# {'text': 'i didnt feel humiliated', 'label': 0}
```

- á» Ä‘Ã¢y chÃºng ta tháº¥y má»—i hÃ ng cá»¥ thá»ƒ tÆ°á»£ng trÆ°ng cho dáº¡ng tá»« Ä‘iá»ƒn, trong Ä‘Ã³ khÃ³a cá»§a nÃ³ lÃ  tÃªn cÃ¡c cá»™t:

```python
train_ds.column_names
# ['text', 'label']
```

- VÃ  giÃ¡ trá»‹ trong trÆ°á»ng há»£p nÃ y lÃ  vÄƒn báº£n tweet vÃ  cáº£m xÃºc. Äiá»u nÃ y pháº£n Ã¡nh sá»± tháº­t lÃ  ğŸ¤— Datasets dá»±a trÃªn Apache Arrow, thá»© mÃ  Ä‘á»‹nh dáº¡ng cá»™t Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› hiá»‡u quáº£ hÆ¡n Python bÃ¬nh thÆ°á»ng. ChÃºng ta cÃ³ thá»ƒ xem nhá»¯ng kiá»ƒu dá»¯ liá»‡u nÃ o Ä‘ang Ä‘Æ°á»£c báº±ng cÃ¡ch truy cáº­p thuá»™c tÃ­nh `features` cá»§a Ä‘á»‘i tÆ°á»£ng Dataset:

```python
train_ds.features

# {'text': Value(dtype='string', id=None),
#  'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}
```

- Trong trÆ°á»ng há»£p nÃ y, kiá»ƒu dá»¯ liá»‡u cá»§a cá»™t `text` lÃ  `string`, trong khi cá»™t `label` lÃ  má»™t Ä‘á»‘i tÆ°á»£ng `ClassLabel` Ä‘áº·c biá»‡t chá»©a thÃ´ng tin vá» tÃªn lá»›p vÃ  Ã¡nh xáº¡ cá»§a chÃºng vá»›i sá»‘ nguyÃªn. ChÃºng ta cÅ©ng cÃ³ thá»ƒ truy cáº­p má»™t sá»‘ hÃ ng vá»›i slice:

```python
train_ds[:5]

# {'text': ['i didnt feel humiliated',
#   'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',
#   'im grabbing a minute to post i feel greedy wrong',
#   'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',
#   'i am feeling grouchy'],
#  'label': [0, 0, 3, 2, 3]}
```

- ChÃºng ta cÅ©ng cÃ³ thá»ƒ láº¥y toÃ n bá»™ má»™t cá»™t theo tÃªn:

```python
train_ds["text"][:5]

# ['i didnt feel humiliated',
#  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',
#  'im grabbing a minute to post i feel greedy wrong',
#  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',
#  'i am feeling grouchy']
```

#### Náº¿u Dataset cá»§a báº¡n khÃ´ng cÃ³ trÃªn Hub thÃ¬ sao ?

- ChÃºng ta sáº½ sá»­ dá»¥ng Hugging Face Hub Ä‘á»ƒ táº£i xuá»‘ng bá»™ dá»¯ liá»‡u cho háº§u háº¿t cÃ¡c vÃ­ dá»¥ trong cuá»‘n sÃ¡ch nÃ y. NhÆ°ng trong nhiá»u trÆ°á»ng há»£p, báº¡n sáº½ tháº¥y mÃ¬nh lÃ m viá»‡c vá»›i dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n hoáº·c trÃªn remote serve trong tá»• chá»©c. ğŸ¤— Datasets cung cáº¥p má»™t sá»‘ lá»‡nh táº£i Ä‘á»ƒ xá»­ lÃ½ cÃ¡c táº­p dá»¯ liá»‡u cá»¥c bá»™ vÃ  remote. VÃ­ dá»¥ vá» cÃ¡c Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u phá»• biáº¿n nháº¥t Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2-1

![anh](./image/300.png)

- NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, Ä‘á»‘i vá»›i má»—i Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u, chÃºng ta chá»‰ cáº§n chuyá»ƒn táº­p lá»‡nh táº£i cÃ³ liÃªn quan Ä‘áº¿n hÃ m `load_dataset()`, cÃ¹ng vá»›i tham sá»‘ `data_files` Ä‘á»ƒ chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n hoáº·c URL Ä‘áº¿n má»™t hoáº·c nhiá»u file. VÃ­ dá»¥: cÃ¡c file vá» emotion dataset Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn Dropbox, vÃ¬ váº­y má»™t cÃ¡ch thay tháº¿ Ä‘á»ƒ táº£i data lÃ :

```python
dataset_url = "https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt"
!wget {dataset_url
```

- Xem kÄ© pháº§n nÃ y hÆ¡n táº¡i [ğŸ¤— Datasets Documentation](https://huggingface.co/docs/datasets/load_hub)


### From Datasets to DataFrames

- Máº·c dÃ¹ ğŸ¤— Datasets cung cáº¥p ráº¥t nhiá»u chá»©c nÄƒng cáº¥p tháº¥p Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u cá»§a chÃºng ta, nhÆ°ng viá»‡c chuyá»ƒn Ä‘á»•i ğŸ¤— Datasets thÃ nh Pandas DataFrame thÆ°á»ng ráº¥t thuáº­n tiá»‡n Ä‘á»ƒ chÃºng ta cÃ³ thá»ƒ truy cáº­p cÃ¡c API cáº¥p cao Ä‘á»ƒ trá»±c quan hÃ³a dá»¯ liá»‡u. Äá»ƒ kÃ­ch hoáº¡t chuyá»ƒn Ä‘á»•i, ğŸ¤— Datasets cung cáº¥p phÆ°Æ¡ng thá»©c `set_format()` cho phÃ©p chÃºng ta thay Ä‘á»•i Ä‘á»‹nh dáº¡ng Ä‘áº§u ra cá»§a Dataset:

```python
import pandas as pd
emotions.set_format(type = "pandas")
df = emotions["train"][:]
df.head()
``` 

![anh](./image/301.png)

- NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, cÃ¡c tiÃªu Ä‘á» cá»™t Ä‘Ã£ Ä‘Æ°á»£c giá»¯ nguyÃªn vÃ  má»™t vÃ i hÃ ng Ä‘áº§u tiÃªn khá»›p vá»›i cÃ¡c cháº¿ Ä‘á»™ xem dá»¯ liá»‡u trÆ°á»›c Ä‘Ã³ cá»§a chÃºng ta. Tuy nhiÃªn, cÃ¡c nhÃ£n Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng sá»‘ nguyÃªn, vÃ¬ váº­y hÃ£y sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `int2str()` Ä‘á»ƒ táº¡o má»™t cá»™t má»›i `label_name` trong DataFrame:

```python
def label_int2str(row):
  return emotions["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
df.head()
````

![anh](./image/302.png)

- TrÆ°á»›c khi Ä‘i sÃ¢u vÃ o viá»‡c xÃ¢y dá»±ng má»™t bá»™ phÃ¢n loáº¡i, chÃºng ta hÃ£y xem xÃ©t ká»¹ hÆ¡n táº­p dá»¯ liá»‡u. NhÆ° Andrej Karpathy lÆ°u Ã½ trong bÃ i Ä‘Äƒng trÃªn blog ná»•i tiáº¿ng cá»§a mÃ¬nh ["A Recipe for Training Neural Networks"](https://karpathy.github.io/2019/04/25/recipe/), "Become one with the data" lÃ  má»™t bÆ°á»›c cáº§n thiáº¿t Ä‘á»ƒ train cÃ¡c mÃ´ hÃ¬nh tuyá»‡t vá»i!


### Looking at the Class Distribution


- Báº¥t cá»© khi nÃ o báº¡n Ä‘ang lÃ m viá»‡c vá»›i cÃ¡c váº¥n Ä‘á» phÃ¢n loáº¡i vÄƒn báº£n, báº¡n nÃªn kiá»ƒm tra sá»± phÃ¢n bá»‘ cá»§a cÃ¡c vÃ­ dá»¥ giá»¯a cÃ¡c lá»›p. Má»™t táº­p dá»¯ liá»‡u cÃ³ phÃ¢n bá»‘ lá»›p sai lá»‡ch cÃ³ thá»ƒ yÃªu cáº§u má»™t cÃ¡ch xá»­ lÃ½ khÃ¡c vá» evaluation metrics vÃ  training loss so vá»›i má»™t bá»™ dá»¯ liá»‡u cÃ¢n báº±ng.

- Vá»›i `Pandas` vÃ  `Matplotlib`, chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng trá»±c quan hÃ³a phÃ¢n bá»‘ cá»§a cÃ¡c nhÃ£n nhÆ° sau:

```python
import matplotlib.pyplot as plt 
df["label_name"].value_counts(ascending=True).plot.barh()
plt.title("Frequency of Classes")
plt.show()
```

![anh](./image/303.png)

- Trong trÆ°á»ng há»£p nÃ y, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng táº­p dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng náº·ng; CÃ¡c lá»›p joy vÃ  sadness xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn, trong khi love vÃ  surprise xuáº¥t hiá»‡n hiáº¿m hÆ¡n khoáº£ng 5-10 láº§n. CÃ³ má»™t sá»‘ cÃ¡ch Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng, bao gá»“m:
1. Randomly oversample lá»›p thiá»ƒu sá»‘
2. Randomly undersample lá»›p Ä‘a sá»‘
3. Thu tháº­p thÃªm dá»¯ liá»‡u cÃ³ nhÃ£n tá»« cÃ¡c lá»›p thiá»ƒu sá»‘

- Äá»ƒ giá»¯ cho má»i thá»© Ä‘Æ¡n giáº£n trong chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ lÃ m viá»‡c vá»›i cÃ¡c táº§n sá»‘ lá»›p thÃ´, khÃ´ng cÃ¢n báº±ng. Náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu thÃªm vá» cÃ¡c ká»¹ thuáº­t láº¥y máº«u nÃ y, tÃ´i khuyÃªn báº¡n nÃªn xem [thÆ° viá»‡n Imbalanced-learn](https://imbalanced-learn.org/stable/). Chá»‰ cáº§n Ä‘áº£m báº£o ráº±ng báº¡n khÃ´ng Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p sampling trÆ°á»›c khi tÃ¡ch thÃ nh hai táº­p train vÃ  test, náº¿u khÃ´ng báº¡n sáº½ bá»‹ rÃ² rá»‰ dá»¯ liá»‡u giá»¯a chÃºng !

### How Long Are Our Tweets?

- Äá»™ dÃ i tá»‘i Ä‘a cá»§a input cho cÃ¡c mÃ´ hÃ¬nh transformers Ä‘Æ°á»£c gá»i lÃ  maximum context size. Äá»‘i vá»›i cÃ¡c á»©ng dá»¥ng sá»­ dá»¥ng DistilBERT, maximum context size lÃ  512 token, tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i má»™t vÃ i Ä‘oáº¡n vÄƒn báº£n. NhÆ° chÃºng ta sáº½ tháº¥y trong pháº§n tiáº¿p theo, token lÃ  thÃ nh pháº§n nhá» nháº¥t (thÃ nh pháº§n nguyÃªn tá»­); BÃ¢y giá», Ä‘á»ƒ Ä‘Æ¡n giáº£n, chÃºng ta sáº½ coi má»™t token nhÆ° má»™t tá»«. ChÃºng ta cÃ³ thá»ƒ Æ°á»›c tÃ­nh sÆ¡ bá»™ vá» Ä‘á»™ dÃ i tweet trÃªn má»—i cáº£m xÃºc báº±ng cÃ¡ch xem xÃ©t sá»± phÃ¢n phá»‘i cá»§a cÃ¡c tá»« trÃªn má»—i tweet:

```python
df["Words Per Tweet"] = df["text"].str.split().apply(len)
df.boxplot("Words Per Tweet", by="label_name", grid=False,
            showfliers=False, color="black")
plt.suptitle("")
plt.xlabel("")
plt.show()
```

![anh](./image/304.png)


- Tá»« Ä‘á»“ thá»‹, chÃºng ta tháº¥y ráº±ng Ä‘á»‘i vá»›i má»—i cáº£m xÃºc, háº§u háº¿t cÃ¡c tweet dÃ i khoáº£ng 15 tá»« vÃ  cÃ¡c tweet dÃ i nháº¥t tháº¥p hÆ¡n nhiá»u so vá»›i maximum context size cá»§a DistilBERT. CÃ¡c vÄƒn báº£n dÃ i hÆ¡n maximum context size cá»§a mÃ´ hÃ¬nh cáº§n Ä‘Æ°á»£c cáº¯t bá»›t, Ä‘iá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n giáº£m hiá»‡u suáº¥t náº¿u vÄƒn báº£n bá»‹ cáº¯t bá»›t chá»©a thÃ´ng tin quan trá»ng; Trong trÆ°á»ng há»£p cá»§a chÃºng ta, cÃ³ váº» nhÆ° Ä‘Ã³ sáº½ khÃ´ng pháº£i lÃ  váº¥n Ä‘á» vÃ¬ cÃ¡c vÄƒn báº£n Ä‘á»u cÃ³ Ä‘á»™ dÃ i nhá» hÆ¡n ngÆ°á»¡ng tá»‘i Ä‘a.
- BÃ¢y giá» chÃºng ta hÃ£y tÃ¬m hiá»ƒu cÃ¡ch chÃºng ta cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c vÄƒn báº£n thÃ´ nÃ y thÃ nh má»™t Ä‘á»‹nh dáº¡ng phÃ¹ há»£p vá»›i ğŸ¤— Transformers! TrÆ°á»›c khi lÃ m Ä‘iá»u Ä‘Ã³, chÃºng ta cÅ©ng hÃ£y Ä‘áº·t láº¡i Ä‘á»‹nh dáº¡ng Ä‘áº§u ra cá»§a táº­p dá»¯ liá»‡u vÃ¬ chÃºng ta khÃ´ng cáº§n Ä‘á»‹nh dáº¡ng DataFrame ná»¯a:

```python
emotions.reset_format()
```


## 2. From Text to Tokens

- CÃ¡c mÃ´ hÃ¬nh Transformer nhÆ° DistilBERT khÃ´ng thá»ƒ nháº­n vÄƒn báº£n thÃ´ lÃ m Ä‘áº§u vÃ o; thay vÃ o Ä‘Ã³, cÃ¡c mÃ´ hÃ¬nh chá»‰ nháº­n vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c tokenized vÃ  encoded dÆ°á»›i dáº¡ng sá»‘. Tokenization lÃ  bÆ°á»›c chia nhá» má»™t chuá»—i thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ nhá» (nguyÃªn tá»­) Ä‘Æ°á»£c sá»­ dá»¥ng trong mÃ´ hÃ¬nh. CÃ³ má»™t sá»‘ chiáº¿n lÆ°á»£c tokenization mÃ  ngÆ°á»i ta cÃ³ thá»ƒ Ã¡p dá»¥ng vÃ  viá»‡c phÃ¢n tÃ¡ch cÃ¡c tá»« thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ con tá»‘i Æ°u thÆ°á»ng Ä‘Æ°á»£c há»c tá»« kho dá»¯ liá»‡u. TrÆ°á»›c khi xem xÃ©t tokenizer Ä‘Æ°á»£c sá»­ dá»¥ng cho DistilBERT, chÃºng ta hÃ£y xem xÃ©t hai trÆ°á»ng há»£p Ä‘Æ¡n giáº£n: token kÃ½ tá»± vÃ  tá»«.

### Character Tokenization

- CÃ¡ch tokenization Ä‘Æ¡n giáº£n nháº¥t lÃ  chia thÃ nh tá»«ng kÃ½ tá»± rá»“i cho vÃ o mÃ´ hÃ¬nh. Trong Python, Ä‘á»‘i tÆ°á»£ng `str` thá»±c cháº¥t lÃ  má»™t máº£ng, cho phÃ©p chÃºng ta nhanh chÃ³ng thá»±c hiá»‡n token cáº¥p kÃ½ tá»± chá»‰ vá»›i má»™t dÃ²ng code:

```python
text = "Tokenizing text is a core task of NLP."
tokenized_text = list(text)
print(tokenized_text)

# ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ',
# 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o',
# 'f', ' ', 'N', 'L', 'P', '.']

```

- ÄÃ¢y lÃ  má»™t khá»Ÿi Ä‘áº§u tá»‘t, nhÆ°ng chÃºng ta chÆ°a lÃ m xong Ä‘Ã¢u. MÃ´ hÃ¬nh cáº§n cÃ¡c kÃ­ tá»± pháº£i Ä‘Æ°á»£c chuyá»ƒn vá» dáº¡ng sá»‘, quÃ¡ trÃ¬nh nÃ y Ä‘Ã´i khi Ä‘Æ°á»£c gá»i lÃ  numericalization. Má»™t cÃ¡ch Ä‘Æ¡n giáº£n lÃ  encoding tá»«ng token cá»¥ thá»ƒ (trong trÆ°á»ng há»£p nÃ y lÃ  kÃ½ tá»±) sang má»™t sá»‘ nguyÃªn cá»¥ thá»ƒ:

```python
token2idx = {ch: idx for idx, ch in enumerate(sorted(set(text)))}
print(token2idx)

# {' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9,
# 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18,
# 'z': 19}
```

- Code trÃªn cho chÃºng ta má»™t cÃ¡ch mapping má»—i kÃ­ tá»± trong tá»« Ä‘iá»ƒn sang má»™t sá»‘ nguyÃªn duy nháº¥t. Giá» chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng `token2idx` Ä‘á»ƒ chuyá»ƒn tokenized text sang má»™t list cÃ¡c sá»‘ nguyÃªn:

```python
input_ids = [token2idx[token] for token in tokenized_text]
print(input_ids)

# [5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]
```

- Giá» má»—i token Ä‘Ã£ Ä‘Æ°á»£c mapping sang má»™t sá»‘ Ä‘á»‹nh danh duy nháº¥t (lÆ°u trong biáº¿n `input_ids`). BÆ°á»›c cuá»‘i cÃ¹ng lÃ  chuyá»ƒn `input_ids` sang dáº¡ng 2D tensor cá»§a cÃ¡c one-hot vectors. One-hot vectors thÆ°á»ng ráº¥t hay Ä‘Æ°á»£c sá»­ dá»¥ng trong machine learning Ä‘á»ƒ encode cÃ¡c categorcial data, thÆ°á»ng lÃ  ordinal hoáº·c normal. VÃ­ dá»¥, giáº£ sá»­ chÃºng ta muá»‘n encoder tÃªn cá»§a cÃ¡c nhÃ¢n váº­t trong phim Transformers. Má»™t cÃ¡ch Ä‘á»ƒ lÃ m Ä‘Ã³ lÃ  map tÃªn sang má»™t ID duy nháº¥t:

```python
categorical_df = pd.DataFrame(
 {"Name": ["Bumblebee", "Optimus Prime", "Megatron"], "Label ID": [0,1,2]})
categorical_df
```

![anh](./image/305.png)


- Váº¥n Ä‘á» vá»›i cÃ¡ch tiáº¿p cáº­n nÃ y lÃ  nÃ³ táº¡o ra má»™t thá»© tá»± hÆ° cáº¥u giá»¯a cÃ¡c tÃªn vÃ  neural networks thá»±c sá»± tá»‘t trong viá»‡c há»c cÃ¡c loáº¡i má»‘i quan há»‡ nÃ y. VÃ¬ váº­y, thay vÃ o Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ táº¡o má»™t cá»™t má»›i cho má»—i loáº¡i vÃ  gÃ¡n 1 náº¿u loáº¡i Ã½ Ä‘Ãºng, 0 trong trÆ°á»ng há»£p ngÆ°á»£c láº¡i. Trong Pandas, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i hÃ m `get_dummies()` nhÆ° sau:

```python
pd.get_dummies(categorical_df["Name"])
```

![anh](./image/306.png)


- CÃ¡c hÃ ng cá»§a DataFrame nÃ y lÃ  vector one-hot, hiá»ƒu Ä‘Æ¡n giáº£n lÃ  chá»‰ cÃ³ má»™t sá»‘ 1 vÃ  cÃ²n láº¡i lÃ  sá»‘ 0. Giá» hÃ£y nhÃ¬n láº¡i biáº¿n `input_ids`, chÃºng ta cÃ³ váº¥n Ä‘á» tÆ°Æ¡ng tá»±: cÃ¡c pháº§n tá»­ táº¡o ra thá»© tá»± vÃ´ nghÄ©a. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cá»™ng hay trá»« hai ID lÃ  má»™t phÃ©p toÃ¡n vÃ´ nghiÄ©, vÃ¬ káº¿t quáº£ lÃ  má»™t ID tuá»£ng trÆ°ng cho má»™t token ngáº«u nhiÃªn khÃ¡c.

- Máº·t khÃ¡c, káº¿t quáº£ cá»§a viá»‡c cá»™ng hai vector one-hot cÃ³ thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c giáº£i thÃ­ch: Hai sá»‘ 1 tÆ°á»£ng trÆ°ng cho 2 token Ä‘á»“ng thá»i cÃ³. ChÃºng ta cÃ³ thá»ƒ táº¡o mÃ£ hÃ³a one-hot trong PyTorch báº±ng cÃ¡ch chuyá»ƒn Ä‘á»•i `input_ids` thÃ nh tensor vÃ  Ã¡p dá»¥ng hÃ m `one_hot()` nhÆ° sau:

```python
import torch
import torch.nn.functional as F

input_ids = torch.tensor(input_ids)
one_hot_encodings = F.one_hot(input_ids, num_classes = len(token2idx))
one_hot_encodings.shape
# torch.Size([38, 20])
```

- Má»—i trong sá»‘ 38 input tokens, cÃ³ má»™t vectÆ¡ ont-hot vá»›i 20 chiá»u, vÃ¬ tá»« vá»±ng cá»§a chÃºng ta bao gá»“m 20 kÃ½ tá»± duy nháº¥t.


> Äiá»u quan trá»ng lÃ  pháº£i luÃ´n Ä‘áº·t tham sá»‘ num_classes trong hÃ m one_hot() vÃ¬ náº¿u khÃ´ng cÃ¡c vectÆ¡ one-hot cÃ³ thá»ƒ ngáº¯n hÆ¡n Ä‘á»™ dÃ i cá»§a tá»« vá»±ng (vÃ  cáº§n Ä‘Æ°á»£c Ä‘á»‡m báº±ng sá»‘ 0 theo cÃ¡ch thá»§ cÃ´ng). Trong TensorFlow, hÃ m tÆ°Æ¡ng Ä‘Æ°Æ¡ng lÃ  tf.one_hot(), trong Ä‘Ã³ tham sá»‘ depth Ä‘Ã³ng vai trÃ² nhÆ° tham sá»‘ num_classes.

- Báº±ng cÃ¡ch kiá»ƒm tra vectÆ¡ Ä‘áº§u tiÃªn, chÃºng ta cÃ³ thá»ƒ xÃ¡c nháº­n ráº±ng sá»‘ 1 xuáº¥t hiá»‡n á»Ÿ vá»‹ trÃ­ Ä‘Æ°á»£c chá»‰ ra bá»Ÿi input_ids[0]:

```python
print(f"Token: {tokenized_text[0]}")
print(f"Tensor index: {input_ids[0]}")
print(f"One-hot: {one_hot_encodings[0]}")

# Token: T
# Tensor index: 5
# One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
```

- Tá»« vÃ­ dá»¥ Ä‘Æ¡n giáº£n trÃªn, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng tokenization cáº¥p kÃ½ tá»± bá» qua báº¥t ká»³ cáº¥u trÃºc nÃ o trong vÄƒn báº£n vÃ  coi toÃ n bá»™ chuá»—i nhÆ° má»™t luá»“ng kÃ½ tá»±. Máº·c dÃ¹ Ä‘iá»u nÃ y giÃºp Ä‘á»‘i phÃ³ vá»›i lá»—i chÃ­nh táº£ vÃ  cÃ¡c tá»« hiáº¿m, nhÆ°ng háº¡n cháº¿ chÃ­nh lÃ  cÃ¡c cáº¥u trÃºc ngÃ´n ngá»¯ nhÆ° tá»« cáº§n Ä‘Æ°á»£c há»c tá»« dá»¯ liá»‡u. Äiá»u nÃ y Ä‘Ã²i há»i tÃ­nh toÃ¡n, bá»™ nhá»› vÃ  dá»¯ liá»‡u Ä‘Ã¡ng ká»ƒ. VÃ¬ lÃ½ do nÃ y, tokenization kÃ½ tá»± hiáº¿m khi Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c táº¿. Thay vÃ o Ä‘Ã³, má»™t sá»‘ cáº¥u trÃºc cá»§a vÄƒn báº£n Ä‘Æ°á»£c giá»¯ nguyÃªn trong bÆ°á»›c tokenization. tokenization cáº¥p Ä‘á»™ tá»« lÃ  má»™t cÃ¡ch tiáº¿p cáº­n Ä‘Æ¡n giáº£n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, vÃ¬ váº­y chÃºng ta hÃ£y xem nÃ³ hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o.

### Word Tokenization

- Thay vÃ¬ chia vÄƒn báº£n thÃ nh cÃ¡c kÃ½ tá»±, chÃºng ta cÃ³ thá»ƒ chia nÃ³ thÃ nh cÃ¡c tá»« vÃ  Ã¡nh xáº¡ má»—i tá»« thÃ nh má»™t sá»‘ nguyÃªn. Sá»­ dá»¥ng tá»« ngay tá»« Ä‘áº§u cho phÃ©p mÃ´ hÃ¬nh bá» qua bÆ°á»›c há»c tá»« dá»±a trÃªn cÃ¡c kÃ½ tá»± vÃ  do Ä‘Ã³ giáº£m Ä‘á»™ phá»©c táº¡p cá»§a quÃ¡ trÃ¬nh training.

- Má»™t cÃ¡ch Ä‘Æ¡n giáº£n Ä‘á»ƒ chia tá»« Ä‘Ã³ lÃ  dá»±a vÃ o khoáº£ng tráº¯ng. ChÃºng ta cÃ³ thá»ƒ dÃ¹ng hÃ m `split()` trá»±c tiáº¿p trÃªn chuá»—i thÃ´ Ä‘á»ƒ chia tá»« dá»±a vÃ o khoáº£ng tráº¯ng:
  

```python
tokenized_text = text.split()
print(tokenized_text)
# ['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']
```

- Tá»« Ä‘Ã¢y, chÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c bÆ°á»›c tÆ°Æ¡ng tá»± nhÆ° chÃºng ta Ä‘Ã£ thá»±c hiá»‡n Ä‘á»‘i vá»›i token cáº¥p Ä‘Ã´ kÃ½ tá»± Ä‘á»ƒ Ã¡nh xáº¡ tá»«ng tá»« vá»›i má»™t ID duy nháº¥t. Tuy nhiÃªn, chÃºng ta Ä‘Ã£ cÃ³ thá»ƒ tháº¥y má»™t váº¥n Ä‘á» tiá»m áº©n vá»›i cÃ¡ch encode nÃ y: dáº¥u cÃ¢u khÃ´ng Ä‘Æ°á»£c tÃ­nh Ä‘áº¿n, vÃ¬ váº­y "NLP." Ä‘Æ°á»£c coi lÃ  má»™t token duy nháº¥t. Äiá»u nÃ y dáº«n Ä‘áº¿n vocab bao gá»“m cÃ¡c tá»« sai lá»‡ch, chia tá»« hoáº·c sai chÃ­nh táº£, kÃ­ch thÆ°á»›c cá»§a tá»« vá»±ng cÃ³ thá»ƒ dá»… dÃ ng tÄƒng lÃªn hÃ ng triá»‡u !

- Viá»‡c cÃ³ má»™t vocab lá»›n lÃ  má»™t váº¥n Ä‘á» vÃ¬ nÃ³ yÃªu cáº§u neural networks pháº£i cÃ³ má»™t sá»‘ lÆ°á»£ng tham sá»‘ khá»•ng lá»“. Äá»ƒ minh há»a, giáº£ sá»­ chÃºng ta cÃ³ 1 triá»‡u tá»« duy nháº¥t vÃ  muá»‘n nÃ©n cÃ¡c vector Ä‘áº§u vÃ o cÃ³ kÃ­ch thÆ°á»›c 1 triá»‡u chiá»u xuá»‘ng cÃ¡c vector 1 nghÃ¬n chiá»u trong lá»›p Ä‘áº§u tiÃªn cá»§a neural networks. ÄÃ¢y lÃ  má»™t bÆ°á»›c tiÃªu chuáº©n trong háº§u háº¿t cÃ¡c kiáº¿n trÃºc NLP, vÃ  ma tráº­n trá»ng sá»‘ káº¿t quáº£ cá»§a lá»›p Ä‘áº§u tiÃªn nÃ y sáº½ chá»©a 1 triá»‡u Ã— 1 nghÃ¬n = 1 tá»· trá»ng sá»‘. Äiá»u nÃ y Ä‘Ã£ gáº§n báº±ng sá»‘ tham sá»‘ cá»§a mÃ´ hÃ¬nh GPT-2 lá»›n nháº¥t, vá»›i khoáº£ng 1,5 tá»· tham sá»‘!

- Táº¥t nhiÃªn, chÃºng ta muá»‘n trÃ¡nh lÃ£ng phÃ­ tÃ i nguyÃªn mÃ´ hÃ¬nh nhÆ° váº­y bá»Ÿi vÃ¬ viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh ráº¥t tá»‘n kÃ©m, vÃ  cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n khÃ³ duy trÃ¬ hÆ¡n. Má»™t cÃ¡ch tiáº¿p cáº­n phá»• biáº¿n lÃ  giá»›i háº¡n kÃ­ch thÆ°á»›c tá»« vá»±ng vÃ  loáº¡i bá» cÃ¡c tá»« hiáº¿m báº±ng cÃ¡ch chá»‰ xÃ©t, vÃ­ dá»¥, 100.000 tá»« phá»• biáº¿n nháº¥t trong táº­p dá»¯ liá»‡u. CÃ¡c tá»« khÃ´ng thuá»™c tá»« vá»±ng nÃ y sáº½ Ä‘Æ°á»£c phÃ¢n loáº¡i lÃ  "khÃ´ng xÃ¡c Ä‘á»‹nh" vÃ  Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n má»™t token UNK chung. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng ta máº¥t má»™t sá»‘ thÃ´ng tin tiá»m nÄƒng quan trá»ng trong quÃ¡ trÃ¬nh token hÃ³a tá»«, vÃ¬ mÃ´ hÃ¬nh khÃ´ng cÃ³ thÃ´ng tin vá» cÃ¡c tá»« liÃªn quan Ä‘áº¿n token UNK.

- Liá»‡u sáº½ tháº­t tuyá»‡t náº¿u cÃ³ má»™t sá»± thá»a hiá»‡p giá»¯a token hÃ³a kÃ½ tá»± vÃ  token hÃ³a tá»«, cho phÃ©p giá»¯ láº¡i toÃ n bá»™ thÃ´ng tin Ä‘áº§u vÃ o vÃ  má»™t pháº§n cáº¥u trÃºc cá»§a Ä‘áº§u vÃ o? VÃ  cÃ¢u tráº£ lá»i lÃ : subwork tokenization.


### Subword Tokenization

- Ã tÆ°á»Ÿng cÆ¡ báº£n Ä‘áº±ng sau subword tokenization lÃ  káº¿t há»£p nhá»¯ng Ä‘iá»ƒm máº¡nh nháº¥t cá»§a token hÃ³a kÃ½ tá»± vÃ  token hÃ³a tá»«. Má»™t máº·t, chÃºng ta muá»‘n tÃ¡ch cÃ¡c tá»« hiáº¿m thÃ nh nhá»¯ng Ä‘Æ¡n vá»‹ nhá» hÆ¡n Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ xá»­ lÃ½ cÃ¡c tá»« phá»©c táº¡p vÃ  lá»—i chÃ­nh táº£. Máº·t khÃ¡c, chÃºng ta muá»‘n giá»¯ cÃ¡c tá»« phá»• biáº¿n nhÆ° cÃ¡c thá»±c thá»ƒ duy nháº¥t Ä‘á»ƒ cÃ³ thá»ƒ giá»¯ Ä‘á»™ dÃ i Ä‘áº§u vÃ o á»Ÿ má»©c há»£p lÃ½. Äiá»ƒm khÃ¡c biá»‡t chÃ­nh cá»§a subword tokenization lÃ  nÃ³ Ä‘Æ°á»£c há»c tá»« táº­p dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng sá»± káº¿t há»£p cá»§a cÃ¡c quy táº¯c thá»‘ng kÃª vÃ  thuáº­t toÃ¡n.

- CÃ³ má»™t sá»‘ thuáº­t toÃ¡n subword tokenization thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong NLP, nhÆ°ng hÃ£y báº¯t Ä‘áº§u vá»›i [WordPiece](https://ieeexplore.ieee.org/document/6289079), thuáº­t toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi bá»™ token hÃ³a cá»§a BERT vÃ  DistilBERT. CÃ¡ch dá»… nháº¥t Ä‘á»ƒ hiá»ƒu WordPiece hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o lÃ  xem nÃ³ trong thá»±c táº¿. ğŸ¤— Transformers cung cáº¥p má»™t lá»›p tiá»‡n lá»£i lÃ  `AutoTokenizer`, cho phÃ©p báº¡n nhanh chÃ³ng táº£i bá»™ tokenizer liÃªn káº¿t vá»›i má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. Chá»‰ cáº§n gá»i phÆ°Æ¡ng thá»©c `from_pretrained()`, cung cáº¥p ID cá»§a mÃ´ hÃ¬nh trÃªn Hub hoáº·c Ä‘Æ°á»ng dáº«n file cá»¥c bá»™. HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch táº£i bá»™ token hÃ³a cho DistilBERT:

```python
from transformers import AutoTokenizer

model_ckpt = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

```

- Lá»›p AutoTokenizer thuá»™c vá» má»™t nhÃ³m lá»›n hÆ¡n cÃ¡c lá»›p "auto" cÃ³ nhiá»‡m vá»¥ tá»± Ä‘á»™ng láº¥y cáº¥u hÃ¬nh cá»§a mÃ´ hÃ¬nh, trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n, hoáº·c tá»« vá»±ng tá»« tÃªn checkpoint. Äiá»u nÃ y cho phÃ©p báº¡n nhanh chÃ³ng chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c mÃ´ hÃ¬nh, nhÆ°ng náº¿u muá»‘n táº£i má»™t lá»›p cá»¥ thá»ƒ theo cÃ¡ch thá»§ cÃ´ng, báº¡n cÅ©ng cÃ³ thá»ƒ nhÆ° sau. VÃ­ dá»¥, chÃºng ta cÃ³ thá»ƒ táº£i DistilBERT tokenizer nhÆ° sau:

```python
from transformers import DistilBertTokenizer

tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)
```

> Khi báº¡n cháº¡y phÆ°Æ¡ng thá»©c `AutoTokenizer.from_pretrained()` láº§n Ä‘áº§u tiÃªn, báº¡n sáº½ tháº¥y má»™t thanh tiáº¿n trÃ¬nh hiá»ƒn thá»‹ cÃ¡c tham sá»‘ cá»§a bá»™ token hÃ³a Ä‘Æ°á»£c táº£i tá»« Hugging Face Hub. Khi cháº¡y láº¡i mÃ£ láº§n thá»© hai, token hÃ³a sáº½ Ä‘Æ°á»£c táº£i tá»« bá»™ nhá»› Ä‘á»‡m (thÆ°á»ng lÃ  `~/.cache/huggingface`).

- HÃ£y kiá»ƒm tra cÃ¡ch bá»™ tokenizer nÃ y hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch cung cáº¥p má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n: â€œTokenizing text is a core task of NLP.â€

```python
encoded_text = tokenizer(text)
print(encoded_text)

# {'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953,
# 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

- CÅ©ng giá»‘ng nhÆ° vá»›i viá»‡c token kÃ½ tá»±, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c tá»« Ä‘Ã£ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh cÃ¡c sá»‘ nguyÃªn duy nháº¥t trong trÆ°á»ng input_ids. ChÃºng ta sáº½ tháº£o luáº­n vai trÃ² cá»§a trÆ°á»ng `attention_mask` trong pháº§n tiáº¿p theo. BÃ¢y giá», khi Ä‘Ã£ cÃ³ `input_ids`, chÃºng ta cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i chÃºng thÃ nh cÃ¡c token báº±ng cÃ¡ch sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `convert_ids_to_tokens()` cá»§a `tokenizer`:

```python
tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)
print(tokens)
# ['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl',
# '##p', '.', '[SEP]']
```

- ChÃºng ta cÃ³ thá»ƒ quan sÃ¡t ba Ä‘iá»u á»Ÿ Ä‘Ã¢y. Thá»© nháº¥t, má»™t sá»‘ token Ä‘áº·c biá»‡t [CLS] vÃ  [SEP] Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ Ä‘Ã¡nh dáº¥u báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a chuá»—i. CÃ¡c token nÃ y cÃ³ thá»ƒ  khÃ¡c nhau tÃ¹y thuá»™c vÃ o mÃ´ hÃ¬nh, nhÆ°ng chá»©c nÄƒng chÃ­nh cá»§a chÃºng lÃ  chá»‰ ra Ä‘iá»ƒm báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a chuá»—i. Thá»© hai, cÃ¡c token Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh chá»¯ thÆ°á»ng, Ä‘Ã¢y lÃ  má»™t tÃ­nh nÄƒng cá»§a checkpoint cá»¥ thá»ƒ nÃ y. Cuá»‘i cÃ¹ng, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng â€œtokenizingâ€ vÃ  â€œNLPâ€ Ä‘Ã£ Ä‘Æ°á»£c chia thÃ nh hai token, Ä‘iá»u nÃ y cÃ³ Ã½ nghÄ©a vÃ¬ chÃºng khÃ´ng pháº£i lÃ  cÃ¡c tá»« phá»• biáº¿n. Tiá»n tá»‘ ## trong ##izing vÃ  ##p cho biáº¿t ráº±ng chuá»—i trÆ°á»›c Ä‘Ã³ khÃ´ng cÃ³ khoáº£ng tráº¯ng; báº¥t ká»³ token nÃ o vá»›i tiá»n tá»‘ nÃ y nÃªn Ä‘Æ°á»£c ná»‘i vá»›i token trÆ°á»›c Ä‘Ã³ khi báº¡n chuyá»ƒn Ä‘á»•i chÃºng thÃ nh má»™t chuá»—i. Lá»›p `AutoTokenizer` cÃ³ phÆ°Æ¡ng thá»©c `convert_tokens_to_string()` Ä‘á»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y, vÃ¬ váº­y hÃ£y Ã¡p dá»¥ng nÃ³ vá»›i cÃ¡c token cá»§a chÃºng ta:

```python
print(tokenizer.convert_tokens_to_string(tokens))
# [CLS] tokenizing text is a core task of nlp. [SEP]
```
- Lá»›p `AutoTokenizer` cÅ©ng cÃ³ má»™t sá»‘ thuá»™c tÃ­nh khÃ¡c cung cáº¥p thÃ´ng tin vá» tokenizer. VÃ­ dá»¥, chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra kÃ­ch thÆ°á»›c tá»« vá»±ng:

```python
tokenizer.vocab_size # 30522
```

- Maximum context size cá»§a mÃ´ hÃ¬nh:

```python
tokenizer.model_max_length # 512
```

- Má»™t thuá»™c tÃ­nh thÃº vá»‹ khÃ¡c cáº§n biáº¿t lÃ  tÃªn cá»§a cÃ¡c trÆ°á»ng mÃ  mÃ´ hÃ¬nh mong Ä‘á»£i trong quÃ¡ trÃ¬nh truyá»n dá»¯ liá»‡u Ä‘áº§u vÃ o:

```python
tokenizer.model_input_names
# ['input_ids', 'attention_mask']
```

- BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ hiá»ƒu biáº¿t cÆ¡ báº£n vá» quÃ¡ trÃ¬nh tokenization cho má»™t cÃ¢u vÄƒn báº£n, hÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ tokenization toÃ n bá»™ táº­p dá»¯ liá»‡u!

> Khi sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, Ä‘iá»u cá»±c ká»³ quan trá»ng lÃ  báº¡n pháº£i sá»­ dá»¥ng chÃ­nh tokenizer mÃ  mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ¹ng. Tá»« gÃ³c nhÃ¬n cá»§a mÃ´ hÃ¬nh, viá»‡c thay Ä‘á»•i tokenizer giá»‘ng nhÆ° thay Ä‘á»•i hoÃ n toÃ n tá»« vá»±ng. Náº¿u má»i ngÆ°á»i xung quanh báº¡n báº¯t Ä‘áº§u trÃ¡o Ä‘á»•i ngáº«u nhiÃªn cÃ¡c tá»« nhÆ° â€œnhÃ â€ vá»›i â€œmÃ¨oâ€, báº¡n cÅ©ng sáº½ gáº·p khÃ³ khÄƒn trong viá»‡c hiá»ƒu chuyá»‡n gÃ¬ Ä‘ang diá»…n ra!

### Tokenizing the Whole Dataset

- Äá»ƒ tokenize toÃ n bá»™ táº­p dá»¯ liá»‡u, chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `map()` cá»§a Ä‘á»‘i tÆ°á»£ng DatasetDict. ChÃºng ta sáº½ gáº·p phÆ°Æ¡ng thá»©c nÃ y nhiá»u láº§n trong sÃ¡ch nÃ y, vÃ¬ nÃ³ cung cáº¥p má»™t cÃ¡ch thuáº­n tiá»‡n Ä‘á»ƒ Ã¡p dá»¥ng má»™t hÃ m xá»­ lÃ½ cho tá»«ng pháº§n tá»­ trong táº­p dá»¯ liá»‡u. NhÆ° báº¡n sáº½ sá»›m tháº¥y, phÆ°Æ¡ng thá»©c `map()` cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o cÃ¡c hÃ ng vÃ  cá»™t má»›i.

- Äá»ƒ báº¯t Ä‘áº§u, viá»‡c Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ  má»™t hÃ m xá»­ lÃ½ Ä‘á»ƒ tokenize cÃ¡c vÃ­ dá»¥ cá»§a chÃºng ta, nhÆ° sau:

```python
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)
```

- HÃ m nÃ y Ã¡p dá»¥ng tokenizer lÃªn má»™t batch; `padding=True` sáº½ Ä‘iá»n thÃªm cÃ¡c sá»‘ 0 Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c chuá»—i cÃ³ cÃ¹ng Ä‘á»™ dÃ i vá»›i chuá»—i dÃ i nháº¥t trong batch, vÃ  `truncation=True` sáº½ cáº¯t ngáº¯n cÃ¡c chuá»—i vÆ°á»£t quÃ¡ Ä‘á»™ maximum context size cá»§a mÃ´ hÃ¬nh. Äá»ƒ tháº¥y `tokenize()` hoáº¡t Ä‘á»™ng, hÃ£y thá»­ Ã¡p dá»¥ng nÃ³ vá»›i hai vÃ­ dá»¥ tá»« táº­p huáº¥n luyá»‡n:

```python
print(tokenize(emotions["train"][:2]))

# {'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0,
# 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000,
# 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300,
# 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
# 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
# 1, 1]]}
```

- á» Ä‘Ã¢y, chÃºng ta cÃ³ thá»ƒ tháº¥y káº¿t quáº£ cá»§a viá»‡c padding: pháº§n tá»­ Ä‘áº§u tiÃªn cá»§a input_ids ngáº¯n hÆ¡n pháº§n tá»­ thá»© hai, vÃ¬ váº­y cÃ¡c sá»‘ 0 Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ lÃ m cho chÃºng cÃ³ cÃ¹ng Ä‘á»™ dÃ i. CÃ¡c sá»‘ 0 nÃ y tÆ°Æ¡ng á»©ng vá»›i token Ä‘áº·c biá»‡t [PAD] trong tá»« vá»±ng, vÃ  bá»™ cÃ¡c token Ä‘áº·c biá»‡t nÃ y cÅ©ng bao gá»“m cÃ¡c token [CLS] vÃ  [SEP] mÃ  chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³:

![anh](./image/307.png)

- NgoÃ i ra, lÆ°u Ã½ ráº±ng bÃªn cáº¡nh viá»‡c tráº£ vá» vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a dÆ°á»›i dáº¡ng `input_ids`, `tokenizer` cÃ²n tráº£ vá» má»™t danh sÃ¡ch cÃ¡c máº£ng `attention_mask`. Äiá»u nÃ y nháº±m ngÄƒn mÃ´ hÃ¬nh bá»‹ nháº§m láº«n bá»Ÿi cÃ¡c token padding (cÃ¡c sá»‘ 0 trong `input_ids`); `attention_mask` cho phÃ©p mÃ´ hÃ¬nh bá» qua cÃ¡c token padding nÃ y. HÃ¬nh 2-3 cung cáº¥p hÃ¬nh áº£nh minh há»a vá» cÃ¡ch cÃ¡c `input_ids` vÃ  `attention_mask` Ä‘Æ°á»£c Ã¡p dá»¥ng khi token padding:

![anh](./image/308.png)

- Khi chÃºng ta Ä‘Ã£ Ä‘á»‹nh nghÄ©a má»™t hÃ m xá»­ lÃ½ xong, chuÃºn ta cÃ³ thá»ƒ Ã¡p dá»¥ng nÃ³ cho táº¥t cáº£ cÃ¡c pháº§n trong táº­p dá»¯ liá»‡u chá»‰ vá»›i má»™t dÃ²ng lá»‡nh:

```python
emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)
```

- Theo máº·c Ä‘á»‹nh, phÆ°Æ¡ng thá»©c `map()` hoáº¡t Ä‘á»™ng trÃªn tá»«ng vÃ­ dá»¥ riÃªng láº» trong táº­p dá»¯ liá»‡u. Do Ä‘Ã³, khi Ä‘áº·t `batched=True`, phÆ°Æ¡ng thá»©c nÃ y sáº½ encode theo tá»«ng batch. VÃ¬ chÃºng ta Ä‘Ã£ Ä‘áº·t `batch_size=None`, hÃ m `tokenize()` cá»§a chÃºng ta sáº½ Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u dÆ°á»›i dáº¡ng má»™t batch duy nháº¥t. Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng cÃ¡c tensor Ä‘áº§u vÃ o (input_ids) vÃ  máº·t náº¡ chÃº Ã½ (attention_mask) cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u.

- ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng thao tÃ¡c nÃ y Ä‘Ã£ thÃªm cÃ¡c cá»™t má»›i lÃ  `input_ids` vÃ  `attention_mask` vÃ o táº­p dá»¯ liá»‡u:

```python
print(emotions_encoded["train"].column_names)
# ['text', 'label', 'input_ids', 'attention_mask']
```


## 3. Training a Text Classifier

- NhÆ° Ä‘Ã£ tháº£o luáº­n trong ChÆ°Æ¡ng 1, cÃ¡c mÃ´ hÃ¬nh nhÆ° DistilBERT Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c tá»« Ä‘Æ°á»£c che giáº¥u trong má»™t chuá»—i vÄƒn báº£n. Tuy nhiÃªn, chÃºng ta khÃ´ng thá»ƒ sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ nÃ y trá»±c tiáº¿p Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n; chÃºng ta cáº§n sá»­a Ä‘á»•i chÃºng má»™t chÃºt. Äá»ƒ hiá»ƒu nhá»¯ng sá»­a Ä‘á»•i nÃ o lÃ  cáº§n thiáº¿t, chÃºng ta hÃ£y xem kiáº¿n trÃºc cá»§a má»™t mÃ´ hÃ¬nh dá»±a trÃªn encoder-based nhÆ° DistilBERT, Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 2-4.

![anh](./image/309.png)


- Äáº§u tiÃªn, vÄƒn báº£n Ä‘Æ°á»£c tokenized vÃ  biá»ƒu diá»…n dÆ°á»›i dáº¡ng one-hot vectors Ä‘Æ°á»£c gá»i lÃ  token encodings. KÃ­ch thÆ°á»›c cá»§a tokenizer vocabulary xÃ¡c Ä‘á»‹nh sá»‘ chiá»u cá»§a token encodings vÃ  nÃ³ thÆ°á»ng bao gá»“m 20kâ€“200k token duy nháº¥t. Tiáº¿p theo, cÃ¡c token encodings nÃ y Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh token embeddings, lÃ  cÃ¡c vectÆ¡ cÃ³ sá»‘ chiá»u tháº¥p hÆ¡n. Sau Ä‘Ã³, cÃ¡c token embeddings Ä‘Æ°á»£c chuyá»ƒn qua cÃ¡c lá»›p encoder block Ä‘á»ƒ táº¡o ra hidden state cho má»—i input token. Äá»‘i vá»›i má»¥c tiÃªu pretrained cá»§a language modeling (Äá»‘i vá»›i mÃ´ hÃ¬nh DistilBERT, nÃ³ lÃ  Ä‘oÃ¡n cÃ¡c token mask), má»—i hidden state Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t lá»›p dá»± Ä‘oÃ¡n cÃ¡c mask input token. Äá»‘i vá»›i nhiá»‡m vá»¥ phÃ¢n loáº¡i, chÃºng ta thay tháº¿ lá»›p language moedeling báº±ng lá»›p phÃ¢n loáº¡i.

> Trong thá»±c táº¿, PyTorch bá» qua bÆ°á»›c táº¡o one-hot vectÆ¡ cho token encodings vÃ¬ nhÃ¢n má»™t ma tráº­n vá»›i one-hot vector cÅ©ng giá»‘ng nhÆ° chá»n má»™t cá»™t tá»« ma tráº­n. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n trá»±c tiáº¿p báº±ng cÃ¡ch láº¥y cá»™t cÃ³ token ID tá»« ma tráº­n. ChÃºng ta sáº½ tháº¥y Ä‘iá»u nÃ y trong ChÆ°Æ¡ng 3 khi chÃºng ta sá»­ dá»¥ng nn.Embedding.

- ChÃºng ta cÃ³ hai lá»±a chá»n Ä‘á»ƒ train má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i trÃªn bá»™ dá»¯ liá»‡u Twitter:

1. **Feature extraction:** ChÃºng ta sá»­ dá»¥ng cÃ¡c hidden state nhÆ° feature vÃ  chá»‰ train bá»™ phÃ¢n loáº¡i mÃ  khÃ´ng train trá»ng sá»‘ cá»§a pretrained model (trá»ng sá»‘ cá»§a pretrained model Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng).
2. **Fine-tuning:** ChÃºng ta train toÃ n bá»™ mÃ´ hÃ¬nh, cÃ³ cáº­p nháº­t cáº£ trá»ng sá»‘ cá»§a pretrained model.

- Trong cÃ¡c pháº§n sau, chÃºng ta khÃ¡m phÃ¡ cáº£ hai lá»±a chá»n cho DistilBERT vÃ  xem xÃ©t sá»± Ä‘Ã¡nh Ä‘á»•i cá»§a chÃºng.

### Transformers as Feature Extractors

- Sá»­ dá»¥ng Transformers lÃ m Feature Extractors khÃ¡ Ä‘Æ¡n giáº£n. NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 2-5, chÃºng ta Ä‘Ã³ng bÄƒng trá»ng sá»‘ cá»§a pretrained model trong quÃ¡ trÃ¬nh train vÃ  sá»­ dá»¥ng cÃ¡c hidden state lÃ m feature cho bá»™ phÃ¢n loáº¡i. Æ¯u Ä‘iá»ƒm cá»§a cÃ¡ch tiáº¿p cáº­n nÃ y lÃ  chÃºng ta cÃ³ thá»ƒ nhanh chÃ³ng train má»™t mÃ´ hÃ¬nh nhá» hoáº·c mÃ´ hÃ¬nh nÃ´ng (shallow model). Má»™t mÃ´ hÃ¬nh nhÆ° váº­y cÃ³ thá»ƒ lÃ  má»™t neural classification layer hoáº·c má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng dÃ¹ng Ä‘áº¿n gradient, vÃ­ dá»¥ nhÆ° random forest. PhÆ°Æ¡ng phÃ¡p nhÆ° random forest Ä‘áº·c biá»‡t thuáº­n tiá»‡n náº¿u GPU khÃ´ng kháº£ dá»¥ng, vÃ¬ cÃ¡c hidden state chá»‰ cáº§n Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c má»™t láº§n.

![anh](./image/310.png)

##### Using pretrained models


- ChÃºng ta sáº½ sá»­ dá»¥ng má»™t auto class tiá»‡n lá»£i khÃ¡c tá»« ğŸ¤— Transformers cÃ³ tÃªn lÃ  `AutoModel`. TÆ°Æ¡ng tá»± nhÆ° lá»›p `AutoTokenizer`, `AutoModel` cÃ³ má»™t phÆ°Æ¡ng thá»©c `from_pretrained()` Ä‘á»ƒ táº£i trá»ng sá»‘ cá»§a má»™t pretrained model. HÃ£y sá»­ dá»¥ng phÆ°Æ¡ng thá»©c nÃ y Ä‘á»ƒ táº£i DistilBERT checkpoint:

```python
from transformers import AutoModel

model_ckpt = "distilbert-base-uncased"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModel.from_pretrained(model_ckpt).to(device)
```

- á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ sá»­ dá»¥ng PyTorch Ä‘á»ƒ kiá»ƒm tra xem GPU cÃ³ sáºµn hay khÃ´ng. Äiá»u nÃ y Ä‘áº£m báº£o mÃ´ hÃ¬nh sáº½ cháº¡y trÃªn GPU náº¿u chÃºng ta cÃ³. Náº¿u khÃ´ng, mÃ´ hÃ¬nh sáº½ cháº¡y trÃªn CPU, cÃ³ thá»ƒ cháº­m hÆ¡n Ä‘Ã¡ng ká»ƒ.
- Lá»›p `AutoModel` chuyá»ƒn Ä‘á»•i token encoding thÃ nh embedding vÃ  sau Ä‘Ã³ cho chÃºng qua encoder stack Ä‘á»ƒ tráº£ vá» cÃ¡c hidden state. HÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ trÃ­ch xuáº¥t cÃ¡c tráº¡ng thÃ¡i nÃ y tá»« kho dá»¯ liá»‡u cá»§a mÃ¬nh.


##### Extracting the last hidden states


- Äá»ƒ khá»Ÿi Ä‘á»™ng, hÃ£y truy xuáº¥t last hidden states cho má»™t chuá»—i duy nháº¥t. Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  encode chuá»—i vÃ  chuyá»ƒn Ä‘á»•i cÃ¡c token thÃ nh tensor PyTorch. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch cung cáº¥p tham sá»‘ `return_tensors="pt"` cho `tokenizer` nhÆ° sau:

```python
text = "this is a test"
inputs = tokenizer(text, return_tensors = "pt")
print(f"Input tensor shape: {inputs['input_ids'].size()}")
# Input tensor shape: torch.Size([1, 6])
```

- NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, tensor káº¿t quáº£ cÃ³ kÃ­ch thÆ°á»›c lÃ  [batch_size, n_tokens]. BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ cÃ¡c encoding dÆ°á»›i dáº¡ng tensor, bÆ°á»›c cuá»‘i cÃ¹ng lÃ  Ä‘áº·t chÃºng trÃªn cÃ¹ng má»™t `device` vá»›i mÃ´ hÃ¬nh vÃ  chuyá»ƒn cÃ¡c Ä‘áº§u vÃ o nhÆ° sau:

```python
inputs = {k:v.to(device) for k,v in inputs.items()}
with torch.no_grad():
  outputs = model(**inputs)
print(outputs)
# BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862, 0.0528, ...,
# -0.1188, 0.0662, 0.5470],
#  [-0.3575, -0.6484, -0.0618, ..., -0.3040, 0.3508, 0.5221],
#  [-0.2772, -0.4459, 0.1818, ..., -0.0948, -0.0076, 0.9958],
#  [-0.2841, -0.3917, 0.3753, ..., -0.2151, -0.1173, 1.0526],
#  [ 0.2661, -0.5094, -0.3180, ..., -0.4203, 0.0144, -0.2149],
#  [ 0.9441, 0.0112, -0.4714, ..., 0.1439, -0.7288, -0.1619]]],
#  device='cuda:0'), hidden_states=None, attentions=None)
```

- á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ sá»­ dá»¥ng trÃ¬nh quáº£n lÃ½ ngá»¯ cáº£nh `torch.no_grad()` Ä‘á»ƒ vÃ´ hiá»‡u hÃ³a tÃ­nh toÃ¡n tá»± Ä‘á»™ng cá»§a gradient. Äiá»u nÃ y ráº¥t há»¯u Ã­ch cho suy luáº­n vÃ¬ nÃ³ lÃ m giáº£m lÆ°á»£ng bá»™ nhá»› cá»§a cÃ¡c tÃ­nh toÃ¡n. TÃ¹y thuá»™c vÃ o cáº¥u hÃ¬nh mÃ´ hÃ¬nh, Ä‘áº§u ra cÃ³ thá»ƒ chá»©a má»™t sá»‘ Ä‘á»‘i tÆ°á»£ng, cháº³ng háº¡n nhÆ° hidden state, losses hoáº·c attention, Ä‘Æ°á»£c sáº¯p xáº¿p trong má»™t lá»›p tÆ°Æ¡ng tá»± nhÆ° má»™t bá»™ cÃ³ tÃªn trong Python. Trong vÃ­ dá»¥ cá»§a chÃºng ta, Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh lÃ  má»™t thá»±c thá»ƒ cá»§a `BaseModelOutput v`Ã  chÃºng ta cÃ³ thá»ƒ chá»‰ cáº§n truy cáº­p cÃ¡c thuá»™c tÃ­nh cá»§a nÃ³ theo tÃªn. MÃ´ hÃ¬nh hiá»‡n táº¡i chá»‰ tráº£ vá» má»™t thuá»™c tÃ­nh, Ä‘Ã³ lÃ  last hidden state, vÃ¬ váº­y hÃ£y kiá»ƒm tra kÃ­ch thÆ°á»›c cá»§a nÃ³:

```python
outputs.last_hidden_state.size()
# torch.Size([1, 6, 768])
```

- NhÃ¬n vÃ o hidden state tensor, chÃºng ta tháº¥y ráº±ng nÃ³ cÃ³ kÃ­ch thÆ°á»›c lÃ  [batch_size, n_tokens, hidden_dim]. NÃ³i cÃ¡ch khÃ¡c, má»™t vectÆ¡ 768 chiá»u Ä‘Æ°á»£c tráº£ vá» cho má»—i trong sá»‘ 6 input token. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ phÃ¢n loáº¡i, thÃ´ng thÆ°á»ng chÃºng ta chá»‰ sá»­ dá»¥ng hidden state Ä‘Æ°á»£c liÃªn káº¿t vá»›i token [CLS] lÃ m feature Ä‘áº§u vÃ o. VÃ¬ token nÃ y xuáº¥t hiá»‡n á»Ÿ Ä‘áº§u má»—i chuá»—i, chÃºng ta cÃ³ thá»ƒ trÃ­ch xuáº¥t nÃ³ báº±ng cÃ¡ch chá»‰ cáº§n gá»i chá»‰ sá»‘ cá»§a outputs.last_hidden_state nhÆ° sau:

```python
output.last_hidden_state[:,0].size()
# torch.Size([1, 768])
```

- BÃ¢y giá» chÃºng ta Ä‘Ã£ biáº¿t cÃ¡ch láº¥y last hidden state cho má»™t chuá»—i duy nháº¥t; HÃ£y lÃ m tÆ°Æ¡ng tá»± cho toÃ n bá»™ táº­p dá»¯ liá»‡u báº±ng cÃ¡ch táº¡o ra má»™t cá»™t hidden_state má»›i lÆ°u trá»¯ táº¥t cáº£ cÃ¡c vector nÃ y. NhÆ° chÃºng ta Ä‘Ã£ lÃ m vá»›i tokenizer, chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `map()` cá»§a `DatasetDict` Ä‘á»ƒ trÃ­ch xuáº¥t táº¥t cáº£ cÃ¡c hidden state trong má»™t láº§n. Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  bao bá»c cÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã³ trong má»™t hÃ m xá»­ lÃ½:

```python
def extract_hidden_states(batch):
  inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}
  with torch.no_grad():
    last_hidden_state = model(**inputs).last_hidden_state
  return {"hidden_state": last_hidden_state[:,0].cpu().numpy()}
```

- Sá»± khÃ¡c biá»‡t duy nháº¥t giá»¯a hÃ m nÃ y vÃ  logic trÆ°á»›c Ä‘Ã³ cá»§a chÃºng ta lÃ  bÆ°á»›c cuá»‘i cÃ¹ng khi chÃºng ta Ä‘áº·t last hidden state trá»Ÿ láº¡i CPU dÆ°á»›i dáº¡ng máº£ng NumPy. PhÆ°Æ¡ng thá»©c `map()` yÃªu cáº§u hÃ m xá»­ lÃ½ tráº£ vá» cÃ¡c Ä‘á»‘i tÆ°á»£ng Python hoáº·c NumPy khi chÃºng ta Ä‘ang sá»­ dá»¥ng Ä‘áº§u vÃ o lÃ  batch.
- VÃ¬ mÃ´ hÃ¬nh cá»§a chÃºng ta mong Ä‘á»£i tensor lÃ m Ä‘áº§u vÃ o, Ä‘iá»u tiáº¿p theo cáº§n lÃ m lÃ  chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t `input_ids` vÃ  `attention_mask` sang Ä‘á»‹nh dáº¡ng "torch", nhÆ° sau:

```python
emotions_encoded.set_format("torch", columns = ["input_ids", "attention_mask", "label"])
```

- Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ trÃ­ch xuáº¥t cÃ¡c hidden state trong má»™t láº§n:

```python
emotions_hidden = emotions_encoded.map(extract_hidden_states, batched = True)
```

- LÆ°u Ã½ ráº±ng chÃºng ta Ä‘Ã£ khÃ´ng Ä‘áº·t `batch_size=None` trong trÆ°á»ng há»£p nÃ y, cÃ³ nghÄ©a lÃ  máº·c Ä‘á»‹nh `batch_size=1000` Ä‘Æ°á»£c sá»­ dá»¥ng thay tháº¿. ÄÃºng nhÆ° dá»± Ä‘oÃ¡n, viá»‡c Ã¡p dá»¥ng hÃ m `extract_ hidden_ states()` Ä‘Ã£ thÃªm má»™t cá»™t `hidden_state` má»›i vÃ o táº­p dá»¯ liá»‡u cá»§a chÃºng ta:

```python
emotions_hidden["train"].column_names
# ['attention_mask', 'hidden_state', 'input_ids', 'label', 'text']
```

- BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ cÃ¡c hidden states Ä‘Æ°á»£c liÃªn káº¿t vá»›i má»—i tweet, bÆ°á»›c tiáº¿p theo lÃ  train má»™t bá»™ phÃ¢n loáº¡i trÃªn chÃºng. Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, chÃºng ta sáº½ cáº§n má»™t ma tráº­n tÃ­nh feature â€” chÃºng ta hÃ£y xem xÃ©t á»Ÿ pháº§n tiáº¿p theo.

##### Creating a feature matrix

- Táº­p dá»¯ liá»‡u giá» chá»©a táº¥t cáº£ thÃ´ng tin chÃºng ta cáº§n Ä‘á»ƒ train má»™t bá»™ phÃ¢n loáº¡i. ChÃºng ta sáº½ sá»­ dá»¥ng cÃ¡c hidden states lÃ m input features vÃ  label lÃ m target. ChÃºng ta cÃ³ thá»ƒ dá»… dÃ ng táº¡o cÃ¡c máº£ng tÆ°Æ¡ng á»©ng á»Ÿ Ä‘á»‹nh dáº¡ng Scikit-learn ná»•i tiáº¿ng nhÆ° sau:

```python
import numpy as np
X_train = np.array(emotions_hidden["train"]["hidden_state"])
X_valid = np.array(emotions_hidden["validation"]["hidden_state"])
y_train = np.array(emotions_hidden["train"]["label"])
y_valid = np.array(emotions_hidden["validation"]["label"])
X_train.shape, X_valid.shape
# ((16000, 768), (2000, 768)
```
- TrÆ°á»›c khi chÃºng ta train má»™t mÃ´ hÃ¬nh vá» cÃ¡c hidden states, báº¡n nÃªn thá»±c hiá»‡n kiá»ƒm tra nhanh Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng hidden states cung cáº¥p má»™t biá»ƒu diá»…n há»¯u Ã­ch vá» cáº£m xÃºc mÃ  chÃºng ta muá»‘n phÃ¢n loáº¡i. Trong pháº§n tiáº¿p theo, chÃºng ta sáº½ tháº¥y cÃ¡ch trá»±c quan hÃ³a cÃ¡c features má»™t cÃ¡ch nhanh chÃ³ng.

##### Visualizing the training set

- VÃ¬ viá»‡c trá»±c quan hÃ³a cÃ¡c hidden state trong 768 chiá»u lÃ  ráº¥t khÃ³, chÃºng ta sáº½ sá»­ dá»¥ng thuáº­t toÃ¡n [UMAP](https://arxiv.org/abs/1802.03426) Ä‘á»ƒ chiáº¿u cÃ¡c vectÆ¡ xuá»‘ng 2D. VÃ¬ UMAP hoáº¡t Ä‘á»™ng tá»‘t nháº¥t khi cÃ¡c feature Ä‘Æ°á»£c scaled Ä‘á»ƒ náº±m trong khoáº£ng [0,1], trÆ°á»›c tiÃªn chÃºng ta sáº½ Ã¡p dá»¥ng `MinMaxScaler` vÃ  sau Ä‘Ã³ sá»­ dá»¥ng triá»ƒn khai UMAP tá»« thÆ° viá»‡n `umap-learn` Ä‘á»ƒ giáº£m sá»‘ chiá»u cÃ¡c hidden states:

```python
from umap import UMAP # !pip install umap-learn 
from sklearn.preprocessing import MinMaxScaler
X_scaled = MinMaxScaler().fit_transform(X_train)
mappper = UMAP(n_components=2, metric = "cosine").fit(X_scaled)
df_emb = pd.DataFrame(mapper.embedding_, columns = ["X","Y"])
df_emb["label"] = y_train
df_emb.head()
```

![anh](./image/311.png)

- Káº¿t quáº£ lÃ  má»™t máº£ng cÃ³ cÃ¹ng sá»‘ lÆ°á»£ng máº«u Ä‘Ã o táº¡o, nhÆ°ng chá»‰ cÃ³ 2 chiá»u thay vÃ¬ 768 mÃ  chÃºng ta Ä‘Ã£ báº¯t Ä‘áº§u ! HÃ£y nghiÃªn cá»©u dá»¯ liá»‡u nÃ©n thÃªm má»™t chÃºt vÃ  váº½ máº­t Ä‘á»™ cá»§a cÃ¡c Ä‘iá»ƒm cho tá»«ng danh má»¥c riÃªng biá»‡t:


```python
fig, axes = plt.subplots(2, 3, figsize=(7,5))
axes = axes.flatten()
cmaps = ["Greys", "Blues", "Oranges", "Reds", "Purples", "Greens"]
labels = emotions["train"].features["label"].names
for i, (label, cmap) in enumerate(zip(labels, cmaps)):
  df_emb_sub = df_emb.query(f"label == {i}")
  axes[i].hexbin(df_emb_sub["X"], df_emb_sub["Y"], cmap=cmap,gridsize=20, linewidths=(0,))
  axes[i].set_title(label)
  axes[i].set_xticks([]), axes[i].set_yticks([])
plt.tight_layout()
plt.show()

```

![anh](./image/312.png)


- Tá»« Ä‘á»“ thá»‹ nÃ y, chÃºng ta cÃ³ thá»ƒ tháº¥y má»™t sá»‘ mÃ´ hÃ¬nh rÃµ rÃ ng: nhá»¯ng cáº£m xÃºc tiÃªu cá»±c nhÆ° sadness, anger vÃ  fear Ä‘á»u chiáº¿m cÃ¡c vÃ¹ng tÆ°Æ¡ng tá»± vá»›i sá»± phÃ¢n bá»‘ hÆ¡i khÃ¡c nhau. Máº·t khÃ¡c, joy vÃ  love Ä‘Æ°á»£c tÃ¡ch biá»‡t tá»‘t vá»›i nhá»¯ng cáº£m xÃºc tiÃªu cá»±c vÃ  cÅ©ng chia sáº» má»™t khÃ´ng gian tÆ°Æ¡ng tá»±. Cuá»‘i cÃ¹ng, surprise náº±m ráº£i rÃ¡c kháº¯p nÆ¡i. Máº·c dÃ¹ chÃºng ta cÃ³ thá»ƒ hy vá»ng vÃ o má»™t sá»‘ sá»± tÃ¡ch biá»‡t, nhÆ°ng Ä‘iá»u nÃ y khÃ´ng Ä‘Æ°á»£c Ä‘áº£m báº£o vÃ¬ mÃ´ hÃ¬nh khÃ´ng Ä‘Æ°á»£c train Ä‘á»ƒ biáº¿t sá»± khÃ¡c biá»‡t giá»¯a nhá»¯ng cáº£m xÃºc nÃ y. NÃ³ chá»‰ há»c chÃºng má»™t cÃ¡ch ngáº§m Ä‘á»‹nh báº±ng cÃ¡ch Ä‘oÃ¡n cÃ¡c tá»« Ä‘Æ°á»£c che giáº¥u trong vÄƒn báº£n.
- BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ Ä‘Æ°á»£c má»™t sá»‘ thÃ´ng tin chi tiáº¿t vá» cÃ¡c features cá»§a bá»™ dá»¯ liá»‡u, giá» hÃ£y train má»™t mÃ´ hÃ¬nh trÃªn Ä‘Ã³.

##### Training a simple classifier

- ChÃºng ta Ä‘Ã£ tháº¥y ráº±ng cÃ¡c hidden state cÃ³ pháº§n khÃ¡c nhau giá»¯a cÃ¡c cáº£m xÃºc, máº·c dÃ¹ Ä‘á»‘i vá»›i má»™t sá»‘ trong sá»‘ chÃºng khÃ´ng cÃ³ ranh giá»›i rÃµ rÃ ng. HÃ£y sá»­ dá»¥ng cÃ¡c hidden state nÃ y Ä‘á»ƒ train má»™t mÃ´ hÃ¬nh há»“i quy logistic vá»›i Scikit-learn. Train má»™t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n nhÆ° váº­y ráº¥t nhanh vÃ  khÃ´ng yÃªu cáº§u GPU:

```python
from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(max_iter = 3000)
lr_clf.fit(X_train, y_train)
lr_clf.score(X_valid, y_valid)
# 0.633
```

- NhÃ¬n vÃ o Ä‘á»™ chÃ­nh xÃ¡c, cÃ³ váº» nhÆ° mÃ´ hÃ¬nh cá»§a chÃºng ta chá»‰ tá»‘t hÆ¡n má»™t chÃºt so vá»›i dá»± Ä‘oÃ¡n ngáº«u nhiÃªn - nhÆ°ng vÃ¬ chÃºng ta Ä‘ang xá»­ lÃ½ má»™t bá»™ dá»¯ liá»‡u Ä‘a lá»›p khÃ´ng cÃ¢n báº±ng, nÃ³ thá»±c sá»± tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ. ChÃºng ta cÃ³ thá»ƒ kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ tá»‘t hay khÃ´ng báº±ng cÃ¡ch so sÃ¡nh nÃ³ vá»›i má»™t baseline Ä‘Æ¡n giáº£n. Trong Scikit-learn cÃ³ `DummyClassifier` cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng má»™t bá»™ phÃ¢n loáº¡i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p phá»ng Ä‘oÃ¡n Ä‘Æ¡n giáº£n nhÆ° luÃ´n chá»n lá»›p Ä‘a sá»‘ hoáº·c luÃ´n chá»n má»™t lá»›p ngáº«u nhiÃªn. Trong trÆ°á»ng há»£p nÃ y, heuristic hoáº¡t Ä‘á»™ng tá»‘t nháº¥t lÃ  luÃ´n chá»n lá»›p thÆ°á»ng xuyÃªn nháº¥t, mang láº¡i Ä‘á»™ chÃ­nh xÃ¡c khoáº£ng $35\%$:

```python
from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy = "most_frequent")
dummy_clf.fit(X_train, y_train)
dummy_clf.score(X_valid, y_valid)
# 0.352
```

- VÃ¬ váº­y, bá»™ phÃ¢n loáº¡i Ä‘Æ¡n giáº£n cá»§a chÃºng ta vá»› DistilBERT embedding tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i baseline. ChÃºng ta cÃ³ thá»ƒ Ä‘iá»u tra thÃªm hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch xem confusion matrix cá»§a bá»™ phÃ¢n loáº¡i, cho chÃºng ta biáº¿t má»‘i quan há»‡ giá»¯a nhÃ£n thá»±c táº¿ vÃ  nhÃ£n dá»± Ä‘oÃ¡n:

```python
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

def plot_confusion_matrix(y_preds, y_true, labels):
  cm = confusion_matrix(y_true, y_preds, normalize = "true")
  fig, ax = plt.subplots(figsize = (6,6))
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = labels)
  disp.plot(cmap = "Blues", values_format = ".2f", ax = ax, colorbar = False)
  plt.title("Normalized confusion matrix")
  plt.show()

y_preds = lr_clf.predict(X_valid)
plot_confusion_matrix(y_preds,y_valid, labels)
```

![anh](./image/313.png)


- ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng anger vÃ  fear thÆ°á»ng bá»‹ nháº§m láº«n vá»›i sadness, Ä‘iá»u nÃ y phÃ¹ há»£p vá»›i quan sÃ¡t mÃ  chÃºng ta Ä‘Ã£ trá»±c quan hÃ³a embeddings. NgoÃ i ra, love vÃ  surprise thÆ°á»ng bá»‹ nháº§m láº«n vá»›i joy.
- Trong pháº§n tiáº¿p theo, chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡ch tiáº¿p cáº­n fine-tuning, dáº«n Ä‘áº¿n hiá»‡u suáº¥t phÃ¢n loáº¡i vÆ°á»£t trá»™i. Tuy nhiÃªn, Ä‘iá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  lÃ m Ä‘iá»u nÃ y Ä‘Ã²i há»i nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n hÆ¡n, cháº³ng háº¡n nhÆ° GPU, cÃ³ thá»ƒ khÃ´ng cÃ³ sáºµn trong mÃ¡y tÃ­nh cá»§a báº¡n. Trong nhá»¯ng trÆ°á»ng há»£p nhÆ° tháº¿ nÃ y, cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn features cÃ³ thá»ƒ lÃ  má»™t sá»± thá»a hiá»‡p tá»‘t giá»¯a viá»‡c thá»±c hiá»‡n há»c mÃ¡y truyá»n thá»‘ng vÃ  há»c sÃ¢u.

### Fine-Tuning Transformers

- BÃ¢y giá» chÃºng ta hÃ£y khÃ¡m phÃ¡ nhá»¯ng gÃ¬ cáº§n thiáº¿t Ä‘á»ƒ  fine-tune transformer tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i. Vá»›i cÃ¡ch tiáº¿p cáº­n fine-tune, chÃºng ta khÃ´ng sá»­ dá»¥ng cÃ¡c hidden state nhÆ° cÃ¡c feature cá»‘ Ä‘á»‹nh, mÃ  thay vÃ o Ä‘Ã³ train chÃºng nhÆ° trong HÃ¬nh 2-6. Äiá»u nÃ y Ä‘Ã²i há»i Ä‘áº§u phÃ¢n loáº¡i pháº£i cÃ³ thá»ƒ phÃ¢n biá»‡t, Ä‘Ã³ lÃ  lÃ½ do táº¡i sao phÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng sá»­ dá»¥ng neural network Ä‘á»ƒ phÃ¢n loáº¡i.

![anh](./image/314.png)


- Training cÃ¡c hidden states Ä‘Ã³ng vai trÃ² lÃ  Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh phÃ¢n loáº¡i sáº½ giÃºp chÃºng ta trÃ¡nh Ä‘Æ°á»£c váº¥n Ä‘á» lÃ m viá»‡c vá»›i dá»¯ liá»‡u cÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p vá»›i nhiá»‡m vá»¥ phÃ¢n loáº¡i. Thay vÃ o Ä‘Ã³, cÃ¡c hidden state ban Ä‘áº§u thÃ­ch á»©ng trong quÃ¡ trÃ¬nh train Ä‘á»ƒ giáº£m loss cá»§a mÃ´ hÃ¬nh vÃ  do Ä‘Ã³ tÄƒng hiá»‡u suáº¥t cá»§a nÃ³.
- ChÃºng ta sáº½ sá»­ dá»¥ng API Trainer tá»« ğŸ¤— Transformers Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a training loop. HÃ£y xem cÃ¡c thÃ nh pháº§n chÃºng ta cáº§n thiáº¿t láº­p !

##### Loading a pretrained model

- Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ  má»™t mÃ´ hÃ¬nh pretrained DistilBERT giá»‘ng nhÆ° mÃ´ hÃ¬nh chÃºng ta Ä‘Ã£ sá»­ dá»¥ng trong cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn features. Sá»­a Ä‘á»•i nhá» duy nháº¥t lÃ  chÃºng ta sá»­ dá»¥ng mÃ´ hÃ¬nh `AutoModelForSequenceClassification` thay vÃ¬ `AutoModel`. Sá»± khÃ¡c biá»‡t lÃ  mÃ´ hÃ¬nh `AutoModelForSequenceClassification` cÃ³ classification head á»Ÿ Ä‘áº§u ra cá»§a pretrained model, cÃ³ thá»ƒ dá»… dÃ ng train vá»›i mÃ´ hÃ¬nh baseline. ChÃºng ta chá»‰ cáº§n chá»‰ Ä‘á»‹nh cÃ³ bao nhiÃªu label mÃ  mÃ´ hÃ¬nh pháº£i dá»± Ä‘oÃ¡n (6 trong trÆ°á»ng há»£p cá»§a chÃºng ta), vÃ¬ Ä‘iá»u nÃ y quyáº¿t Ä‘á»‹nh sá»‘ lÆ°á»£ng Ä‘áº§u ra mÃ  Ä‘áº§u phÃ¢n loáº¡i cÃ³:

```python
from transformers import AutoModelForSequenceClassification

num_labels = 6
model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels = num_labels).to(device))
```

- Báº¡n sáº½ tháº¥y cáº£nh bÃ¡o ráº±ng má»™t sá»‘ pháº§n cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. Äiá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng vÃ¬ classification head váº«n chÆ°a Ä‘Æ°á»£c train. BÆ°á»›c tiáº¿p theo lÃ  xÃ¡c Ä‘á»‹nh cÃ¡c chá»‰ sá»‘ mÃ  chÃºng ta sáº½ sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh fine-tuning.

##### Defining the performance metrics

- Äá»ƒ theo dÃµi cÃ¡c metrics trong quÃ¡ trÃ¬nh train, chÃºng ta cáº§n xÃ¡c Ä‘á»‹nh má»™t hÃ m `compute_metrics()` cho `Trainer`. HÃ m nÃ y nháº­n má»™t Ä‘á»‘i tÆ°á»£ng `EvalPrediction` (lÃ  má»™t tuple vá»›i cÃ¡c thuá»™c tÃ­nh `predictions` vÃ  `label_ids`) vÃ  cáº§n tráº£ vá» má»™t tá»« Ä‘iá»ƒn Ã¡nh xáº¡ tÃªn cá»§a tá»«ng metrics vá»›i giÃ¡ trá»‹ cá»§a nÃ³. Äá»‘i vá»›i á»©ng dá»¥ng cá»§a chÃºng ta, chÃºng ta sáº½ tÃ­nh $F_1$ scores vÃ  accuracy, recall, precision cá»§a mÃ´ hÃ¬nh:

```python
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    # TÃ­nh cÃ¡c metrics
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    precision = precision_score(labels, preds, average="weighted")
    recall = recall_score(labels, preds, average="weighted")
    
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

```

- Vá»›i dataset vÃ  metrics Ä‘Ã£ sáºµn sÃ ng, chÃºng ta chá»‰ cÃ³ hai Ä‘iá»u cuá»‘i cÃ¹ng cáº§n quan tÃ¢m trÆ°á»›c khi xÃ¡c Ä‘á»‹nh lá»›p `Trainer`:

1. ÄÄƒng nháº­p tÃ i khoáº£n trÃªn Hugging Face Hub. Äiá»u nÃ y sáº½ cho phÃ©p báº¡n Ä‘áº©y mÃ´ hÃ¬nh fine-tuned cá»§a mÃ¬nh vÃ o tÃ i khoáº£n cá»§a báº¡n trÃªn Hub vÃ  chia sáº» nÃ³ vá»›i cá»™ng Ä‘á»“ng.
2. XÃ¡c Ä‘á»‹nh táº¥t cáº£ cÃ¡c siÃªu tham sá»‘ cho quÃ¡ trÃ¬nh cháº¡y.

- ChÃºng ta sáº½ xem xÃ©t cÃ¡c bÆ°á»›c nÃ y trong pháº§n tiáº¿p theo.

##### Training the model

- Náº¿u báº¡n Ä‘ang cháº¡y code trong Jupyter notebook, báº¡n cÃ³ thá»ƒ Ä‘Äƒng nháº­p vÃ o Hub báº±ng hÃ m trá»£ giÃºp sau:

```python
from huggingface_hub import notebook_login
notebook_login()
```

- Code nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch trong Ä‘Ã³ báº¡n cÃ³ thá»ƒ nháº­p tÃªn ngÆ°á»i dÃ¹ng vÃ  máº­t kháº©u cá»§a mÃ¬nh hoáº·c access token cÃ³ Ä‘áº·c quyá»n write. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y chi tiáº¿t vá» cÃ¡ch táº¡o access token trong [tÃ i liá»‡u Hub](https://oreil.ly/IRkN1). Náº¿u báº¡n Ä‘ang lÃ m viá»‡c trÃªn ternimal, báº¡n cÃ³ thá»ƒ Ä‘Äƒng nháº­p báº±ng cÃ¡ch cháº¡y lá»‡nh sau:

```bash
huggingface-cli login
```

- Äá»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c training parameters, chÃºng ta sá»­ dá»¥ng lá»›p `TrainingArguments`. Lá»›p nÃ y lÆ°u trá»¯ ráº¥t nhiá»u thÃ´ng tin vÃ  cho phÃ©p báº¡n kiá»ƒm soÃ¡t chi tiáº¿t quÃ¡ trÃ¬nh training vÃ  evaluation. Tham sá»‘ quan trá»ng nháº¥t cáº§n chá»‰ Ä‘á»‹nh lÃ  `output_dir`, Ä‘Ã³ lÃ  nÆ¡i lÆ°u trá»¯ táº¥t cáº£ cÃ¡c táº¡o tÃ¡c tá»« Ä‘Ã o táº¡o. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ vá» `TrainingArguments:

```python
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(emotions_encoded["train"]) // batch_size # 250
model_name = f"{model_ckpt}-finetuned-emotion"
training_args = TrainingArguments(output_dir = model_name,
                                  num_train_epochs = 20,
                                  learning_rate = 2e-5,
                                  per_device_train_batch_size = batch_size,
                                  per_device_eval_batch_size = batch_size,
                                  weight_decay = 0.01,
                                  eval_strategy = "epoch",
                                  disable_tqdm = False,
                                  logging_steps = logging_steps,
                                  push_to_hub = True,
                                  log_level = "error")

```

- á» Ä‘Ã¢y chÃºng ta cÅ©ng Ä‘áº·t tham sá»‘ batch size, learning rate vÃ  sá»‘ epoch, Ä‘á»“ng thá»i chá»‰ Ä‘á»‹nh táº£i best model vÃ o cuá»‘i quÃ¡ trÃ¬nh training. Vá»›i thÃ nh pháº§n cuá»‘i cÃ¹ng nÃ y, chÃºng ta cÃ³ thá»ƒ khá»Ÿi táº¡o vÃ  fine-tune mÃ´ hÃ¬nh cá»§a mÃ¬nh vá»›i `Trainer`:

```python
from transformers import Trainer

trainer = Trainer(model = model, args = training_args,
                  compute_metrics = compute_metrics,
                  train_dataset = emotions_encoded["train"],
                  eval_dataset = emotions_encoded["validation"],
                  tokenizer = tokenizer)
trainer.train()
```

![anh](./image/315.png)

- NhÃ¬n vÃ o logs, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng mÃ´ hÃ¬nh Ä‘áº¡t cÃ¡c metrics xáº¥p xá»‰ $92\%$ â€” Ä‘Ã¢y lÃ  má»™t cáº£i tiáº¿n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn feature !
- ChÃºng ta cÃ³ thá»ƒ xem xÃ©t chi tiáº¿t hÆ¡n cÃ¡c training metrics báº±ng cÃ¡ch tÃ­nh toÃ¡n confusion matrix. Äá»ƒ trá»±c quan hÃ³a confusion matrix, trÆ°á»›c tiÃªn chÃºng ta cáº§n láº¥y cÃ¡c dá»± Ä‘oÃ¡n trÃªn táº­p valdidation. PhÆ°Æ¡ng thá»©c `predict()` cá»§a lá»›p `Trainer` tráº£ vá» má»™t sá»‘ Ä‘á»‘i tÆ°á»£ng há»¯u Ã­ch mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:

```python
preds_output = trainer.predict(emotions_encoded["validation"])
```

- Äáº§u ra cá»§a phÆ°Æ¡ng thá»©c `predict()` lÃ  má»™t Ä‘á»‘i tÆ°á»£ng `PredictionOutput` chá»©a cÃ¡c máº£ng `predictions` vÃ  `label_ids`, cÃ¹ng vá»›i cÃ¡c metrics mÃ  chÃºng tÃ´i Ä‘Ã£ pass cho `trainer`. VÃ­ dá»¥: metrics trÃªn táº­p validation cÃ³ thá»ƒ Ä‘Æ°á»£c truy cáº­p nhÆ° sau:

```python
preds_output.metrics
# {'test_loss': 0.22047173976898193,
#  'test_accuracy': 0.9225,
#  'test_f1': 0.9225500751072866,
#  'test_runtime': 1.6357,
#  'test_samples_per_second': 1222.725,
#  'test_steps_per_second': 19.564}
```


- NÃ³ cÅ©ng chá»©a cÃ¡c dá»± Ä‘oÃ¡n thÃ´ cho má»—i lá»›p. ChÃºng ta cÃ³ thá»ƒ decode cÃ¡c dá»± Ä‘oÃ¡n má»™t cÃ¡ch tham lam báº±ng cÃ¡ch sá»­ dá»¥ng `np.argmax()`. Äiá»u nÃ y mang láº¡i cÃ¡c nhÃ£n dá»± Ä‘oÃ¡n vÃ  cÃ³ cÃ¹ng Ä‘á»‹nh dáº¡ng vá»›i cÃ¡c nhÃ£n Ä‘Æ°á»£c tráº£ vá» bá»Ÿi cÃ¡c mÃ´ hÃ¬nh Scikit-learn trong cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn feature:

```python
y_preds = np.argmax(preds_output.predictions, axis = 1)
```

- Vá»›i cÃ¡c dá»± Ä‘oÃ¡n, chÃºng ta cÃ³ thá»ƒ váº½ láº¡i confusion matrix:

```python
plot_confusion_matrix(y_preds, y_valid, labels)
```

![anh](./image/316.png)

- Äiá»u nÃ y gáº§n vá»›i confusion matrix cÃ³ Ä‘Æ°á»ng chÃ©o lÃ½ tÆ°á»Ÿng. Pháº¡m trÃ¹ love váº«n thÆ°á»ng bá»‹ nháº§m láº«n vá»›i joy, Ä‘iá»u nÃ y dÆ°á»ng nhÆ° tá»± nhiÃªn. Surprise cÅ©ng thÆ°á»ng bá»‹ nháº§m láº«n vá»›i joy, hoáº·c nháº§m láº«n vá»›i fear. NhÃ¬n chung, hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh cÃ³ váº» khÃ¡ tá»‘t, chÃºng ta hÃ£y tÃ¬m hiá»ƒu sÃ¢u hÆ¡n má»™t chÃºt vá» cÃ¡c loáº¡i lá»—i mÃ  mÃ´ hÃ¬nh cá»§a chÃºng ta cÃ³ thá»ƒ máº¯c pháº£i.

##### Error analysis

- TrÆ°á»›c khi tiáº¿p tá»¥c, chÃºng ta nÃªn Ä‘iá»u tra cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh thÃªm má»™t chÃºt. Má»™t ká»¹ thuáº­t Ä‘Æ¡n giáº£n nhÆ°ng máº¡nh máº½ lÃ  sáº¯p xáº¿p validation samples theo model loss. Khi chÃºng ta chuyá»ƒn label trong quÃ¡ trÃ¬nh forawrd pass, loss sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c tÃ­nh toÃ¡n vÃ  tráº£ vá». DÆ°á»›i Ä‘Ã¢y lÃ  má»™t hÃ m tráº£ vá» loss cÃ¹ng vá»›i label dá»± Ä‘oÃ¡n:

```python
from torch.nn.functional import cross_entropy

def forward_pass_with_label(batch):
  inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}
  with torch.no_grad():
    output = model(**inputs)
    pred_label = torch.argmax(output.logits, axis = -1)
    loss = cross_entropy(output.logits, batch["label"].to(device), reduction = "none")
  return {"loss": loss.cpu().numpy(),
          "predicted_label": pred_label.cpu().numpy()}
```

- Sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `map()` má»™t láº§n ná»¯a, chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng hÃ m nÃ y Ä‘á»ƒ láº¥y loss cho táº¥t cáº£ cÃ¡c samples:

```python
# Convert our dataset back to PyTorch tensors
emotions_encoded.set_format("torch",columns=["input_ids", "attention_mask", "label"])
# Compute loss values
emotions_encoded["validation"] = emotions_encoded["validation"].map(forward_pass_with_label, batched=True, batch_size=16)
```

- Cuá»‘i cÃ¹ng, chÃºng ta táº¡o má»™t `DataFrame` vá»›i cÃ¡c text, losses vÃ  predicted/true label:

```python
emotions_encoded.set_format("pandas")
cols = ["text", "label", "predicted_label", "loss"]
df_test = emotions_encoded["validation"][:][cols]
df_test["label"] = df_test["label"].apply(label_int2str)
df_test["predicted_label"] = (df_test["predicted_label"].apply(label_int2str))
```

- BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng sáº¯p xáº¿p `emotions_encoded` theo loss vá»›i thá»© tá»± tÄƒng dáº§n hoáº·c giáº£m dáº§n. Má»¥c tiÃªu cá»§a bÃ i táº­p nÃ y lÃ  phÃ¡t hiá»‡n má»™t trong nhá»¯ng Ä‘iá»u sau:

1. **Wrong labels:** Má»i quy trÃ¬nh thÃªm nhÃ£n vÃ o dá»¯ liá»‡u Ä‘á»u cÃ³ thá»ƒ cÃ³ sai sÃ³t. NgÆ°á»i chÃº thÃ­ch cÃ³ thá»ƒ máº¯c lá»—i hoáº·c khÃ´ng Ä‘á»“ng Ã½, trong khi cÃ¡c nhÃ£n Ä‘Æ°á»£c suy ra tá»« cÃ¡c tÃ­nh nÄƒng khÃ¡c cÃ³ thá»ƒ sai. Náº¿u dá»… dÃ ng tá»± Ä‘á»™ng chÃº thÃ­ch dá»¯ liá»‡u, thÃ¬ chÃºng ta sáº½ khÃ´ng cáº§n má»™t mÃ´ hÃ¬nh Ä‘á»ƒ lÃ m Ä‘iá»u Ä‘Ã³. VÃ¬ váº­y, viá»‡c cÃ³ má»™t sá»‘ vÃ­ dá»¥ Ä‘Æ°á»£c dÃ¡n nhÃ£n sai lÃ  Ä‘iá»u bÃ¬nh thÆ°á»ng. Vá»›i cÃ¡ch tiáº¿p cáº­n nÃ y, chÃºng ta cÃ³ thá»ƒ nhanh chÃ³ng tÃ¬m vÃ  sá»­a chÃºng.
2. **Quirks of the dataset:** Data trong thá»±c táº¿ luÃ´n cÃ³ má»™t chÃºt lá»™n xá»™n. Khi lÃ m viá»‡c vá»›i vÄƒn báº£n, cÃ¡c kÃ½ tá»± hoáº·c chuá»—i Ä‘áº·c biá»‡t trong Ä‘áº§u vÃ o cÃ³ thá»ƒ cÃ³ tÃ¡c Ä‘á»™ng lá»›n Ä‘áº¿n cÃ¡c Ä‘á»‹nh vá»‹ cá»§a mÃ´ hÃ¬nh. Kiá»ƒm tra cÃ¡c dá»± Ä‘oÃ¡n yáº¿u nháº¥t cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ giÃºp xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÆ° váº­y vÃ  lÃ m sáº¡ch dá»¯ liá»‡u hoáº·c chÃ¨n cÃ¡c vÃ­ dá»¥ tÆ°Æ¡ng tá»± cÃ³ thá»ƒ lÃ m cho mÃ´ hÃ¬nh máº¡nh máº½ hÆ¡n.

- TrÆ°á»›c tiÃªn, chÃºng ta hÃ£y xem xÃ©t cÃ¡c samples cÃ³ loss cao nháº¥t:

```python
df_test.sort_values("loss", ascending=False).head(10)
```

![anh](./image/317.png)

- ChÃºng ta cÃ³ thá»ƒ tháº¥y rÃµ ráº±ng mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n má»™t sá»‘ nhÃ£n khÃ´ng chÃ­nh xÃ¡c. Máº·t khÃ¡c, cÃ³ váº» nhÆ° cÃ³ khÃ¡ nhiá»u vÃ­ dá»¥ khÃ´ng cÃ³ lá»›p rÃµ rÃ ng, cÃ³ thá»ƒ bá»‹ dÃ¡n nhÃ£n sai hoáº·c yÃªu cáº§u má»™t lá»›p má»›i hoÃ n toÃ n. Äáº·c biá»‡t, joy dÆ°á»ng nhÆ° bá»‹ dÃ¡n nhÃ£n sai nhiá»u láº§n. Vá»›i thÃ´ng tin nÃ y, chÃºng ta cÃ³ thá»ƒ tinh chá»‰nh bá»™ dá»¯ liá»‡u, Ä‘iá»u nÃ y thÆ°á»ng cÃ³ thá»ƒ dáº«n Ä‘áº¿n tÄƒng hiá»‡u suáº¥t lá»›n (hoáº·c nhiá»u hÆ¡n) nhÆ° cÃ³ nhiá»u dá»¯ liá»‡u hÆ¡n hoáº·c mÃ´ hÃ¬nh lá»›n hÆ¡n !
- Khi nhÃ¬n vÃ o cÃ¡c samples cÃ³ loss tháº¥p nháº¥t, chÃºng ta quan sÃ¡t tháº¥y ráº±ng mÃ´ hÃ¬nh dÆ°á»ng nhÆ° tá»± tin nháº¥t khi dá»± Ä‘oÃ¡n lá»›p sadness. CÃ¡c mÃ´ hÃ¬nh deep learning Ä‘áº·c biá»‡t giá»i trong viá»‡c tÃ¬m vÃ  khai thÃ¡c cÃ¡c phÃ­m táº¯t Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n. VÃ¬ lÃ½ do nÃ y, cÅ©ng nÃªn Ä‘áº§u tÆ° thá»i gian vÃ o viá»‡c xem xÃ©t cÃ¡c vÃ­ dá»¥ mÃ  mÃ´ hÃ¬nh tá»± tin nháº¥t, Ä‘á»ƒ chÃºng ta cÃ³ thá»ƒ tá»± tin ráº±ng mÃ´ hÃ¬nh khÃ´ng khai thÃ¡c khÃ´ng Ä‘Ãºng cÃ¡ch má»™t sá»‘ feature nháº¥t Ä‘á»‹nh cá»§a vÄƒn báº£n. VÃ¬ váº­y, chÃºng ta cÅ©ng hÃ£y xem xÃ©t cÃ¡c dá»± Ä‘oÃ¡n cÃ³ má»©c loss nhá» nháº¥t:

```python
df_test.sort_values("loss", ascending=True).head(10)
```

![anh](./image/318.png)

- BÃ¢y giá» chÃºng ta biáº¿t ráº±ng joy Ä‘Ã´i khi bá»‹ dÃ¡n nhÃ£n sai vÃ  mÃ´ hÃ¬nh tá»± tin nháº¥t trong viá»‡c dá»± Ä‘oÃ¡n sadness cá»§a nhÃ£n hiá»‡u. Vá»›i thÃ´ng tin nÃ y, chÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c cáº£i tiáº¿n cÃ³ má»¥c tiÃªu Ä‘á»‘i vá»›i bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh vÃ  cÅ©ng theo dÃµi lá»›p mÃ  mÃ´ hÃ¬nh dÆ°á»ng nhÆ° ráº¥t tá»± tin.
- BÆ°á»›c cuá»‘i cÃ¹ng trÆ°á»›c khi phá»¥c vá»¥ mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Ã o táº¡o lÃ  lÆ°u nÃ³ Ä‘á»ƒ sá»­ dá»¥ng sau nÃ y. ğŸ¤— Transformers cho phÃ©p chÃºng ta thá»±c hiá»‡n Ä‘iá»u nÃ y trong má»™t vÃ i bÆ°á»›c, hÃ£y cÃ¹ng xem trong pháº§n tiáº¿p theo.

##### Saving and sharing the model

-Cá»™ng Ä‘á»“ng NLP Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i ráº¥t nhiá»u tá»« viá»‡c chia sáº» cÃ¡c pretrained model vÃ  fine-tuned model, Ä‘á»“ng thá»i má»i ngÆ°á»i cÃ³ thá»ƒ chia sáº» mÃ´ hÃ¬nh cá»§a há» vá»›i nhá»¯ng ngÆ°á»i khÃ¡c thÃ´ng qua Hugging Face Hub. Báº¥t ká»³ mÃ´ hÃ¬nh do cá»™ng Ä‘á»“ng táº¡o ra nÃ o cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c táº£i xuá»‘ng tá»« Hub giá»‘ng nhÆ° chÃºng ta Ä‘Ã£ sá»­ dá»¥ng mÃ´ hÃ¬nh DistilBERT. Vá»›i `Trainer API`, viá»‡c lÆ°u vÃ  chia sáº» mÃ´ hÃ¬nh ráº¥t Ä‘Æ¡n giáº£n:


```python
trainer.push_to_hub(commit_message = "Training completed!")
```

- ChÃºng ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng fine-tuned model cá»§a mÃ¬nh Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n vá» cÃ¡c tweet má»›i. VÃ¬ chÃºng ta Ä‘Ã£ Ä‘áº©y mÃ´ hÃ¬nh cá»§a mÃ¬nh lÃªn Hub, bÃ¢y giá» chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ vá»›i hÃ m `pipeline()`, giá»‘ng nhÆ° chÃºng ta Ä‘Ã£ lÃ m trong ChÆ°Æ¡ng 1:

```python
from transformers import pipeline
# Change `Dat1710` to your Hub username
model_id = "Dat1710/distilbert-base-uncased-finetuned-emotion"
classifier = pipeline("text-classification", model=model_id)
```

- Giá» hÃ£y test thá»­ mÃ´ hÃ¬nh vá»›i má»™t cÃ¢u tweet má»›i:

```python
custom_tweet = "I saw a movie today and it was really good."
preds = classifier(custom_tweet, return_all_scores=True)
```

- Cuá»‘i cÃ¹ng, chÃºng ta cÃ³ thá»ƒ váº½ xÃ¡c suáº¥t cho má»—i lá»›p thÃ nh biá»ƒu Ä‘á»“ cá»™t. RÃµ rÃ ng, mÃ´ hÃ¬nh Æ°á»›c tÃ­nh ráº±ng cáº£m xÃºc cÃ³ kháº£ nÄƒng nháº¥t lÃ  joy, Ä‘iá»u nÃ y cÃ³ váº» há»£p lÃ½ vá»›i dÃ²ng tweet:

```python
preds_df = pd.DataFrame(preds[0])
plt.bar(labels, 100 * preds_df["score"], color='C0')
plt.title(f'"{custom_tweet}"')
plt.ylabel("Class probability (%)")
plt.show()
```

![anh](./image/319.png)


## 4. Káº¿t Luáº­n

- Xin chÃºc má»«ng, bÃ¢y giá» báº¡n Ä‘Ã£ biáº¿t cÃ¡ch train mÃ´ hÃ¬nh transformer Ä‘á»ƒ phÃ¢n loáº¡i cáº£m xÃºc trong tweet! ChÃºng ta Ä‘Ã£ tháº¥y hai cÃ¡ch tiáº¿p cáº­n lÃ  dá»±a trÃªn cÃ¡c features vÃ  fine-tuning, Ä‘á»“ng thá»i Ä‘iá»u tra Ä‘iá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u cá»§a chÃºng.
- Tuy nhiÃªn, Ä‘Ã¢y chá»‰ lÃ  bÆ°á»›c Ä‘áº§u tiÃªn trong viá»‡c xÃ¢y dá»±ng má»™t á»©ng dá»¥ng trong tháº¿ giá»›i thá»±c vá»›i cÃ¡c mÃ´ hÃ¬nh transformer vÃ  chÃºng ta cÃ³ nhiá»u ná»n táº£ng hÆ¡n Ä‘á»ƒ Ä‘á» cáº­p. DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch nhá»¯ng thÃ¡ch thá»©c mÃ  báº¡n cÃ³ thá»ƒ gáº·p pháº£i trong hÃ nh trÃ¬nh NLP cá»§a mÃ¬nh:
1. My boss wants my model in production yesterday! : Trong háº§u háº¿t cÃ¡c á»©ng dá»¥ng, mÃ´ hÃ¬nh cá»§a báº¡n khÃ´ng chá»‰ náº±m á»Ÿ Ä‘Ã¢u Ä‘Ã³ bÃ¡m bá»¥i â€” báº¡n muá»‘n Ä‘áº£m báº£o ráº±ng nÃ³ Ä‘ang phá»¥c vá»¥ cÃ¡c dá»± Ä‘oÃ¡n! Khi má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘áº©y Ä‘áº¿n Hub, checkpoint suy luáº­n sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c táº¡o cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i báº±ng cÃ¡c yÃªu cáº§u HTTP. TÃ´i khuyÃªn báº¡n nÃªn xem [tÃ i liá»‡u vá» Inference API](https://huggingface.co/docs/api-inference/index) náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu thÃªm.
2. My users want faster predictions! : ChÃºng ta Ä‘Ã£ tháº¥y má»™t cÃ¡ch tiáº¿p cáº­n cho váº¥n Ä‘á» nÃ y: sá»­ dá»¥ng DistilBERT. Trong ChÆ°Æ¡ng 8, chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o khowledge distillation (quÃ¡ trÃ¬nh mÃ  DistilBERT Ä‘Æ°á»£c táº¡o ra), cÃ¹ng vá»›i cÃ¡c thá»§ thuáº­t khÃ¡c Ä‘á»ƒ tÄƒng tá»‘c cÃ¡c mÃ´ hÃ¬nh transformer cá»§a báº¡n.
3. Can your model also do X ? : NhÆ° chÃºng ta Ä‘Ã£ Ã¡m chá»‰ trong chÆ°Æ¡ng nÃ y, transformer cá»±c ká»³ linh hoáº¡t. Trong pháº§n cÃ²n láº¡i cá»§a cuá»‘n sÃ¡ch, chÃºng ta sáº½ khÃ¡m phÃ¡ má»™t loáº¡t cÃ¡c nhiá»‡m vá»¥, nhÆ° tráº£ lá»i cÃ¢u há»i vÃ  nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn, táº¥t cáº£ Ä‘á»u sá»­ dá»¥ng cÃ¹ng má»™t kiáº¿n trÃºc cÆ¡ báº£n.
4. None of my texts are in English !: HÃ³a ra cÃ¡c mÃ´ hÃ¬nh transformer cÅ©ng cÃ³ nhiá»u ngÃ´n ngá»¯ khÃ¡c nhau vÃ  chÃºng ta sáº½ sá»­ dá»¥ng chÃºng trong ChÆ°Æ¡ng 4 Ä‘á»ƒ giáº£i quyáº¿t nhiá»u ngÃ´n ngá»¯ cÃ¹ng má»™t lÃºc.
5. I donâ€™t have any labels ! : Náº¿u cÃ³ ráº¥t Ã­t dá»¯ liá»‡u Ä‘Æ°á»£c dÃ¡n nhÃ£n, fine-tuning cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  má»™t lá»±a chá»n. Trong ChÆ°Æ¡ng 9, chÃºng ta sáº½ khÃ¡m phÃ¡ má»™t sá»‘ ká»¹ thuáº­t Ä‘á»ƒ Ä‘á»‘i phÃ³ vá»›i tÃ¬nh huá»‘ng nÃ y.

- BÃ¢y giá» chÃºng ta Ä‘Ã£ tháº¥y nhá»¯ng gÃ¬ liÃªn quan Ä‘áº¿n viá»‡c training vÃ  sharing transformer, trong chÆ°Æ¡ng tiáº¿p theo, chÃºng ta sáº½ khÃ¡m phÃ¡ viá»‡c triá»ƒn khai mÃ´ hÃ¬nh transformer cá»§a riÃªng mÃ¬nh tá»« Ä‘áº§u.