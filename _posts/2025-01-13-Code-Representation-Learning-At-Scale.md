---
title: "Code Representation Learning At Scale"
date: 2025-01-13 00:00:00  + 0800
categories: [AI Research]
tags: [codesage]
---
---


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>


Blog nÃ y lÃ  phiÃªn báº£n tiáº¿ng viá»‡t Ä‘Æ°á»£c tÃ´i dá»‹ch tá»« [bÃ i bÃ¡o gá»‘c](https://arxiv.org/pdf/2402.01935).


## Abstract

- CÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ chá»‰ ra ráº±ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ code trÃªn quy mÃ´ lá»›n chá»©ng minh hiá»‡u suáº¥t tÄƒng Ä‘Ã¡ng ká»ƒ Ä‘á»‘i vá»›i cÃ¡c downstream task, vÃ­ dá»¥ nhÆ° lÃ  sinh code. Tuy nhiÃªn, háº§u háº¿t cÃ¡c cÃ´ng trÃ¬nh hiá»‡n liÃªn quan Ä‘áº¿n biá»ƒu diá»…n mÃ£ á»Ÿ quy mÃ´ hÃ ng trÄƒm triá»‡u tham sá»‘ sá»­ dá»¥ng táº­p dá»¯ liá»‡u tiá»n huáº¥n luyá»‡n ráº¥t háº¡n cháº¿. Trong bÃ¡o bÃ¡o nÃ y, chÃºng tÃ´i nÃ¢ng cáº¥p viá»‡c há»c biá»ƒu diá»…n mÃ£ báº±ng má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u mÃ£ thÃ´ng qua gá»“m hai giai Ä‘oáº¡n. Giai Ä‘oáº¡n má»™t, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c bá»™ mÃ£ thÃ´ng qua sá»± káº¿t há»£p táº­n dá»¥ng cá»§a randomness in masking language modeling (RiMLM) vÃ  khÃ­a cáº¡nh cáº¥u trÃºc cá»§a ngÃ´n ngá»¯ láº­p trÃ¬nh. Sau Ä‘Ã³, chung tÃ´i nÃ¢ng cao cÃ¡c biá»ƒu diá»…n thÃ´ng qua contrastive learning (há»c tÆ°Æ¡ng pháº£n) vá»›i hard negative vÃ  hard positive Ä‘Æ°á»£c xÃ¢y dá»±ng theo há»c khÃ´ng giÃ¡m sÃ¡t. ChÃºng tÃ´i xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh bá»™ mÃ£ hÃ³a cÃ³ sáºµn hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n cÃ¡c mÃ´ hÃ¬nh hiá»‡n cÃ³ trong nhiá»u downstream task vá»›i large margins. Äá»ƒ hiá»ƒu rÃµ cÃ¡c yáº¿u tá»‘ gÃ³p pháº§n vÃ o sá»± thÃ nh cÃ´ng trong viá»‡c biá»ƒu diá»…n mÃ£, chÃºng tÃ´i sáº½ loáº¡i bá» cÃ¡c chi tiáº¿t vÃ  chia sáº» sá»± phÃ¡t hiá»‡n cá»§a 
chÃºng tÃ´i vá» (i) sÆ¡ Ä‘á»“ khá»­ nhiá»…u má»©c mÃ£ thÃ´ng bÃ¡o tÃ¹y chá»‰nh vÃ  hiá»‡u quáº£ cho mÃ£ nguá»“n; (ii) táº§m quan trá»ng cá»§a hard negative vÃ  hard positive; (iii) bimodal contrastive learning trong viá»‡c thÃºc Ä‘áº©y hiá»‡u suáº¥t tÃ¬m kiáº¿m ngá»¯ nghÄ©a Ä‘a ngÃ´n ngá»¯; (iv) cÃ¡ch cÃ¡c pretraining quyáº¿t Ä‘á»‹nh cÃ¡c hiá»‡u suáº¥t 
cá»§a downstream task theo kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh.


## 1. Giá»›i thiá»‡u

- Large language models (LLMs) Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn má»™t lÆ°á»£ng lá»›n mÃ£ nguá»“n Ä‘Ã£ thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ lÄ©nh vá»±c táº¡o mÃ£. VÃ­ dá»¥, viá»‡c phÃ¡t hÃ nh gáº§n Ä‘Ã¢y má»™t táº­p dá»¯ liá»‡u 6TB bao gá»“m mÃ£ nguá»“n cÃ³ giáº¥y phÃ©p cho phÃ©p Ä‘Ã£ Ä‘Ã³ng vai trÃ² then chá»‘t trong viá»‡c thÃºc Ä‘áº©y sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ mÃ£ trong thá»i Ä‘iá»ƒm hiá»‡n táº¡i. Tuy nhiÃªn, nhá»¯ng táº­p dá»¯ liá»‡u lá»›n nÃ y chÆ°a Ä‘Æ°á»£c táº­n dá»¥ng háº¿t Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c Programming Language (PL) embedding models nhiá»u má»¥c Ä‘Ã­ch. Äáº¿n nay, háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh PL cÃ³ khÃ´ng quÃ¡ 125 triá»‡u tham sá»‘ vÃ  chá»§ yáº¿u Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t vÃ i triá»‡u vÃ­ dá»¥ huáº¥n luyá»‡n, vÃ­ dá»¥ nhÆ° CodeSearchNet.
- Máº·c dÃ¹ khÃ´ng thá»ƒ phá»§ nháº­n táº§m quan trá»ng cá»§a dá»¯ liá»‡u quy mÃ´ lá»›n, nhÆ°ng cáº§n pháº£i thá»«a nháº­n vai trÃ² quan trá»ng cá»§a cÃ¡c má»¥c tiÃªu huáº¥n luyá»‡n trÆ°á»›c. CÃ¡ch tiáº¿p cáº­n phá»• biáº¿n hiá»‡n nay Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c má»™t bá»™ mÃ£ hÃ³a Transformer hai chiá»u nháº±m há»c cÃ¡c biá»ƒu diá»…n lÃ  thÃ´ng qua viá»‡c tá»‘i Æ°u hÃ³a má»¥c tiÃªu Mask Language Model (MLM), nhÆ° Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Devlin vÃ  cá»™ng sá»± (2019b). Quy trÃ¬nh masking trong má»¥c tiÃªu MLM tiÃªu chuáº©n tuÃ¢n theo quy táº¯c 80-10-10. Tuy 
nhiÃªn, chÃºng tÃ´i nháº­n tháº¥y ráº±ng quy trÃ¬nh che nhÆ° váº­y dáº«n Ä‘áº¿n sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c mÃ´ hÃ¬nh nhÃºng mÃ£ khÃ´ng tá»‘i Æ°u. VÃ¬ cÃ¡c Ä‘oáº¡n mÃ£ chá»©a cáº£ cÃ¡c cÃ¢u lá»‡nh natural language (NL) (tá»©c lÃ  docstring, comment) vÃ  mÃ£ thuáº§n tÃºy, viá»‡c thay tháº¿ cÃ¡c token bá»‹ che báº±ng má»™t token ngáº«u nhiÃªn theo quy táº¯c 80-10-10 cÃ³ thá»ƒ dáº«n Ä‘áº¿n viá»‡c thay tháº¿ má»™t token NL báº±ng má»™t token PL, vÃ  ngÆ°á»£c láº¡i (xem thá»‘ng kÃª á»Ÿ Phá»¥ lá»¥c A.3). ChÃºng tÃ´i suy Ä‘oÃ¡n ráº±ng sá»± Ä‘á»“ng xuáº¥t hiá»‡n cá»§a PL vÃ  NL cÃ¹ng vá»›i báº£n cháº¥t cÃº phÃ¡p cá»§a mÃ£ nguá»“n lÃ m cho nÃ³ dá»… bá»‹ phÃ¡ vá»¡ cáº£ ngá»¯ nghÄ©a vÃ  cáº¥u trÃºc cá»§a mÃ£ bá»‹ che, dáº«n Ä‘áº¿n viá»‡c há»c ngÃ´n ngá»¯ mÃ´ hÃ¬nh khÃ´ng tá»‘i Æ°u.


- Trong khi MLM Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n mang láº¡i cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh cá»§a token, háº§u háº¿t cÃ¡c downstream sau nÃ y chá»§ yáº¿u hoáº¡t Ä‘á»™ng á»Ÿ má»©c Ä‘á»™ chuá»—i. Khi má»¥c tiÃªu lÃ  tÄƒng cÆ°á»ng kháº£ nÄƒng phÃ¢n biá»‡t biá»ƒu diá»…n Ä‘á»ƒ á»©ng dá»¥ng ngay láº­p tá»©c trong cÃ¡c nhiá»‡m vá»¥ á»Ÿ má»©c Ä‘á»™ chuá»—i, Contrastive Learning (CL) trá»Ÿ thÃ nh phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c lá»±a chá»n. CÃ¡c cÃ´ng trÃ¬nh hiá»‡n táº¡i Ä‘Ã£ sá»­ dá»¥ng CL Ä‘Æ¡n
(sá»­ dá»¥ng cÃ¡c cáº·p Code-Code) (Guo et al., 2022; Jain et al., 2021) hoáº·c CL Ä‘Ã´i (sá»­ dá»¥ng cÃ¡c cáº·p Text-Code) (Li et al., 2022) cho viá»‡c há»c biá»ƒu diá»…n. Trong CL Ä‘Æ¡n, má»™t lá»±a chá»n phá»• biáº¿n lÃ  sá»­ dá»¥ng bá»• sung dropout (Gao et al., 2021) Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c cáº·p mÃ£ tÃ­ch cá»±c. Tuy nhiÃªn, chÃºng tÃ´i nháº­n tháº¥y ráº±ng bá»• sung dropout gáº·p khÃ³ khÄƒn trong viá»‡c há»— trá»£ quÃ¡ trÃ¬nh huáº¥n luyá»‡n dÃ i, Ä‘iá»u nÃ y cÅ©ng Ä‘Æ°á»£c bÃ¡o cÃ¡o bá»Ÿi Zhou et al. (2022). NgÆ°á»£c láº¡i, CL Ä‘Ã´i trá»Ÿ thÃ nh má»™t lá»±a chá»n háº¥p dáº«n, chá»§ yáº¿u lÃ  do sá»± sáºµn cÃ³ cá»§a cÃ¡c cáº·p tá»± nhiÃªn. CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y sá»­ dá»¥ng cÃ¡c hÃ m vÃ  docstring tÆ°Æ¡ng á»©ng cá»§a chÃºng Ä‘á»ƒ thiáº¿t láº­p cÃ¡c cáº·p huáº¥n luyá»‡n Ä‘a mÃ´. Tuy nhiÃªn, cÃ¡c thÃ­ nghiá»‡m sÆ¡ bá»™ cá»§a chÃºng tÃ´i chá»‰ ra ráº±ng sá»± trÃ¹ng láº·p Ä‘Ã¡ng ká»ƒ giá»¯a docstring vÃ  chá»¯ kÃ½ hÃ m lÃ m Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh há»c Ä‘á»‘i láº­p (xem thá»‘ng kÃª á»Ÿ Phá»¥ lá»¥c A.6).

![anh](./image/262.png)

- Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu nÃ y, chÃºng tÃ´i giá»›i thiá»‡u CODESAGE, má»™t mÃ´ hÃ¬nh biá»ƒu diá»…n mÃ£ hÃ³a hai chiá»u cho mÃ£ nguá»“n. ChÃºng tÃ´i huáº¥n luyá»‡n trÆ°á»›c CODESAGE báº±ng cÃ¡ch sá»­ dá»¥ng má»™t lÆ°á»£c Ä‘á»“ huáº¥n luyá»‡n hai giai Ä‘oáº¡n vá»›i má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c tÃ¹y chá»‰nh (Kocetkov et al., 
2022). ChÃºng tÃ´i mÃ´ táº£ cÃ¡c thÃ nh pháº§n chÃ­nh cá»§a CODESAGE trong HÃ¬nh 1. Äáº§u tiÃªn, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c bá»™ mÃ£ hÃ³a hai chiá»u thÃ´ng qua sá»± káº¿t há»£p cá»§a hai má»¥c tiÃªu bá»• sung cho nhau: Identifier Deobfuscation (DOBF) vÃ  MLM mÃ  khÃ´ng theo quy táº¯c 80-10-10. TÆ°Æ¡ng tá»± nhÆ° má»™t láº­p trÃ¬nh viÃªn con ngÆ°á»i, viá»‡c tÃ¬m cÃ¡c tÃªn cÃ³ Ã½ nghÄ©a cho cÃ¡c Ä‘á»‹nh danh bá»‹ lÃ m xÃ¡o trá»™n yÃªu cáº§u 
mÃ´ hÃ¬nh pháº£i cÃ³ sá»± hiá»ƒu biáº¿t sÃ¢u sáº¯c vá» ngá»¯ nghÄ©a vÃ  cáº¥u trÃºc cá»§a mÃ£. Äá»“ng thá»i, nhÆ° má»™t má»¥c tiÃªu chung hÆ¡n, MLM bao phá»§ cÃ¡c khÃ­a cáº¡nh khÃ¡c ngoÃ i cÃ¡c Ä‘á»‹nh danh cá»§a mÃ£ â€“ Ä‘iá»u nÃ y quan trá»ng Ä‘á»ƒ lÃ m phong phÃº cÃ¡c tÃ­n hiá»‡u huáº¥n luyá»‡n, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c vÃ­ dá»¥ dá»¯ liá»‡u cÃ³ tÃªn Ä‘á»‹nh 
danh khÃ´ng mang tÃ­nh thÃ´ng tin. Trong giai Ä‘oáº¡n thá»© hai, chÃºng tÃ´i táº­n dá»¥ng cÃ¡c cáº·p (vÄƒn báº£n, mÃ£) báº±ng bimodal contrastive learning (CL). KhÃ¡c vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ chá»§ yáº¿u dá»±a vÃ o cÃ¡c cáº·p vÄƒn báº£n vÃ  mÃ£ tá»± nhiÃªn, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t chiáº¿n lÆ°á»£c Ä‘á»ƒ giáº£m kháº£ nÄƒng mÃ´ hÃ¬nh há»c cÃ¡c lá»‘i táº¯t. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i bao gá»“m chá»‰ sá»­ dá»¥ng pháº§n thÃ¢n hÃ m trong khi bá» qua chá»¯ 
kÃ½ vÃ  cÃ¡c cÃ¢u lá»‡nh tráº£ vá». ChÃºng tÃ´i cÅ©ng khai thÃ¡c CL dá»±a trÃªn cÃ¡c hard negative trong khÃ´ng gian nhÃºng. ChÃºng tÃ´i cho tháº¥y ráº±ng chiáº¿n lÆ°á»£c xÃ¢y dá»±ng cÃ¡c cáº·p hard postive vÃ  hard negative nhÆ° váº­y Ä‘Æ¡n giáº£n nhÆ°ng ráº¥t cáº§n thiáº¿t cho viá»‡c bimodal contrastive learning hiá»‡u quáº£.

- ChÃºng tÃ´i huáº¥n luyá»‡n ba mÃ´ hÃ¬nh biá»ƒu diá»…n mÃ£ hÃ³a hai chiá»u, cá»¥ thá»ƒ lÃ  CODESAGE-SMALL (130M), CODESAGE-BASE (356M) vÃ  CODESAGE-LARGE (1.3B). ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh trÃªn nhiá»u nhiá»‡m vá»¥ phÃ¢n biá»‡t khÃ¡c nhau, trong Ä‘Ã³ CODESAGE vÆ°á»£t trá»™i hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c mÃ´ hÃ¬nh tiÃªn tiáº¿n trÆ°á»›c Ä‘Ã³ cÃ³ kÃ­ch thÆ°á»›c tÆ°Æ¡ng tá»± trong háº§u háº¿t 
cÃ¡c nhiá»‡m vá»¥. Äá»ƒ hiá»ƒu rÃµ cÃ¡c yáº¿u tá»‘ gÃ³p pháº§n vÃ o viá»‡c há»c biá»ƒu diá»…n mÃ£ thÃ nh cÃ´ng, chÃºng tÃ´i phÃ¢n tÃ­ch tá»‰ má»‰ cÃ¡c thÃ nh pháº§n chÃ­nh cá»§a khung lÃ m viá»‡c cá»§a mÃ¬nh vÃ  trÃ¬nh bÃ y nhá»¯ng phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho cÃ¡c nghiÃªn cá»©u trong tÆ°Æ¡ng lai.

## 2. CÃ¡c nghiÃªn cá»©u liÃªn quan

- **Embedding for Programming Languages:** gáº§n Ä‘Ã¢y Ä‘Ã£ chá»©ng kiáº¿n sá»± gia tÄƒng Ä‘Ã¡ng ká»ƒ trong viá»‡c há»c cÃ¡c biá»ƒu diá»…n Ä‘a má»¥c Ä‘Ã­ch Ä‘á»ƒ há»— trá»£ nhiá»u nhiá»‡m vá»¥ tiáº¿p theo trong ngÃ´n ngá»¯ láº­p trÃ¬nh. Feng et al. (2020a); Kanade et al. (2020); Li et al. (2023) láº¥y cáº£m há»©ng tá»« thÃ nh cÃ´ng trong xá»­ lÃ½ vÄƒn báº£n vÃ  tá»‘i Æ°u hÃ³a má»¥c tiÃªu Masking Language Modeling (MLM) trÃªn dá»¯ liá»‡u mÃ£ hÃ³a thÃ nh dáº¡ng tuyáº¿n tÃ­nh. TÆ°Æ¡ng tá»± nhÆ° vÄƒn báº£n, há» cÅ©ng tá»‘i Æ°u hÃ³a vá»›i má»¥c tiÃªu phÃ¡t hiá»‡n token Ä‘Æ°á»£c thay tháº¿ (Clark et al., 2020) hoáº·c má»¥c tiÃªu dá»± Ä‘oÃ¡n cÃ¢u tiáº¿p theo (Devlin et al., 2019b) cho mÃ£ 
nguá»“n. Má»™t dÃ²ng nghiÃªn cá»©u khÃ¡c táº­n dá»¥ng máº·t cáº¥u trÃºc cá»§a mÃ£ Ä‘á»ƒ cung cáº¥p tÃ­n hiá»‡u huáº¥n luyá»‡n bá»• sung. Trong sá»‘ Ä‘Ã³, Guo et al. (2021) táº­n dá»¥ng luá»“ng dá»¯ liá»‡u Ä‘á»ƒ mÃ£ hÃ³a má»‘i quan há»‡ "nÆ¡i-giÃ¡ trá»‹-xuáº¥t-phÃ¡t" giá»¯a cÃ¡c biáº¿n. Wang et al. (2021a); Jiang et al. (2021) chÃ¨n cáº¥u trÃºc cÃº phÃ¡p tá»« abstract syntax tree (AST) thÃ´ng qua cÃ¡c má»¥c tiÃªu phá»¥ trá»£ biáº¿n thá»ƒ. Má»™t cÃ´ng trÃ¬nh má»›i Ä‘Ã¢y hÆ¡n (Guo et al., 2022) lÃ m pháº³ng cáº¥u trÃºc AST trá»±c tiáº¿p thÃ nh má»™t chuá»—i vÃ  mÃ£ hÃ³a thÃ´ng tin cÃº phÃ¡p qua cÃ¡c má»¥c tiÃªu mÃ´ hÃ¬nh ngÃ´n ngá»¯. Wang et al. (2021b); Lachaux et al. (2021) huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»« chuá»—i Ä‘áº¿n chuá»—i Ä‘á»ƒ tÃ¡i táº¡o mÃ£ gá»‘c tá»« má»™t mÃ£ bá»‹ lÃ m rá»‘i cÃ¡c tÃªn lá»›p, hÃ m vÃ  
biáº¿n báº±ng cÃ¡c token Ä‘áº·c biá»‡t. Giáº£i mÃ£ mÃ  khÃ´ng chá»‰ rÃµ nguá»“n dá»¯ liá»‡u vÃ  AST ngáº§m Ä‘á»‹nh mÃ£ hÃ³a mÃ  khÃ´ng liÃªn quan Ä‘áº¿n cÃ¡c má»¥c tiÃªu phá»¥ trá»£ hoáº·c Ä‘áº§u vÃ o phá»©c táº¡p vá»›i sá»± phÃ¢n cáº¥p sÃ¢u, vÃ¬ mÃ´ hÃ¬nh cáº§n hiá»ƒu rÃµ sá»± phá»¥ thuá»™c giá»¯a cÃ¡c biáº¿n cÅ©ng nhÆ° cáº¥u trÃºc mÃ£ Ä‘á»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c tÃªn cho cÃ¡c Ä‘á»‹nh danh.

- **Contrastive Learning:** Ká»ƒ tá»« nhá»¯ng thÃ nh cÃ´ng ban Ä‘áº§u cá»§a máº¡ng Siamese (Hadsell et al., 2006), há»c Ä‘á»‘i láº­p Ä‘Ã£ Ä‘Æ°á»£c rá»™ng rÃ£i Ã¡p dá»¥ng trong há»c biá»ƒu diá»…n báº±ng máº¡ng nÆ¡-ron sÃ¢u. Song et al. (2016) má»Ÿ rá»™ng máº¥t mÃ¡t ba cá»¥m nguyÃªn báº£n báº±ng cÃ¡ch so sÃ¡nh má»—i vÃ­ dá»¥ tÃ­ch cá»±c vá»›i táº¥t cáº£ cÃ¡c Ã¢m tÃ­ch cá»±c trong cÃ¹ng má»™t lÃ´, Ä‘iá»u nÃ y Ä‘Ã£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ há»c vÃ  Ä‘Æ°á»£c phá»• biáº¿n 
hÆ¡n ná»¯a bá»Ÿi SimCLR (Chen et al., 2020). Tuy nhiÃªn, khÃ¡c vá»›i lÄ©nh vá»±c tÃ­nh toÃ¡n nÆ¡i mÃ  cÃ¡c dÆ°Æ¡ng tÃ­ch cá»±c hiá»‡u quáº£ cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡c biáº¿n Ä‘á»•i ngáº«u nhiÃªn cá»§a hÃ¬nh áº£nh trong khÃ´ng gian Ä‘áº§u vÃ o, viá»‡c tÄƒng cÆ°á»ng dá»¯ liá»‡u hiá»‡u quáº£ Ä‘Ã£ lÃ¢u lÃ  má»™t thÃ¡ch thá»©c trong NLP do tÃ­nh 
rá»i ráº¡c cá»§a Ä‘áº§u vÃ o. ThÃ¡ch thá»©c nÃ y Ä‘Æ°á»£c kháº³ng Ä‘á»‹nh thÃªm trong cÃ´ng trÃ¬nh cá»§a Gao et al. (2021) chá»‰ ra ráº±ng dropout (Srivastava et al., 2014) nhÆ° lÃ  biáº¿n cá»‘ dá»¯ liá»‡u tá»‘i thiá»ƒu thÆ°á»ng hiá»‡u quáº£ hÆ¡n so vá»›i nhá»¯ng biáº¿n cá»‘ Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch hoáº¡t Ä‘á»™ng trong khÃ´ng gian Ä‘áº§u vÃ o rá»i ráº¡c, vÃ­ dá»¥ nhÆ° xÃ³a vÃ  thay tháº¿ tá»«.

- Má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c lÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ táº­n dá»¥ng cÃ¡c cáº·p tá»± nhiÃªn lÃ m dÆ°Æ¡ng tÃ­ch cá»±c. Zhou et al. (2022) xem xÃ©t cÃ¡c lá»i nÃ³i liÃªn tiáº¿p tá»« dá»¯ liá»‡u há»™i thoáº¡i nhÆ° lÃ  dÆ°Æ¡ng tÃ­ch cá»±c, trong khi Neelakantan et al. (2022) xem xÃ©t cÃ¡c vÄƒn báº£n lÃ¡ng giá»ng Ä‘Æ°á»£c khai thÃ¡c tá»« 
internet. Má»™t cÃ´ng trÃ¬nh ráº¥t gáº§n Ä‘Ã¢y (Wang et al., 2022) táº­n dá»¥ng cÃ¡c cáº·p cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i hoáº·c nháº­n xÃ©t tá»« StackExchange vÃ  Reddit. TrÃªn cÃ¹ng má»™t hÆ°á»›ng Ä‘i vá»›i ngÃ´n ngá»¯ láº­p trÃ¬nh, Guo et al. (2022); Wang et al. (2021a); Neelakantan et al. (2022) táº­n dá»¥ng cÃ¡c cáº·p (vÄƒn báº£n, mÃ£) vá»›i vÄƒn báº£n Ä‘Æ°á»£c khai thÃ¡c tá»« cÃ¡c docstring. ChÃºng tÃ´i Ä‘i má»™t bÆ°á»›c xa hÆ¡n báº±ng viá»‡c táº­p trung vÃ o xÃ¢y dá»±ng tÃ­ch cá»±c khÃ³ vÃ  Ã¢m tÃ­ch cá»±c khÃ³, Ä‘Ã¢y lÃ  thÃ nh pháº§n chÃ­nh cho há»c biá»ƒu diá»…n vÃ  cho phÃ©p chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c mÃ´ hÃ¬nh nhÃºng sáºµn sÃ ng sá»­ dá»¥ng.

## 3. PhÆ°Æ¡ng phÃ¡p

### 3.1 Mask Language Modeling vÃ  Deobfuscation Pre-Tranining.

- Cho má»™t cÃ¢u Ä‘áº§u vÃ o vá»›i $N$ tokens, vÃ­ dá»¥ nhÆ° $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N]$. Mask Language Model (MLM) sáº½ cá»‘ gáº¯ng tá»‘i Æ°u hÃ m Loss sau:

$$
\mathcal{L}_{\text{MLM}}(\mathbf{x}) = - \sum_{i \in \mathcal{M}} \log \mathbb{P}(x_i \mid \mathbf{x}^{\mathcal{M}})
$$

- á»ž Ä‘Ã¢y $\mathcal{M}$ Ä‘áº¡i diá»‡n cho mask Ã¡p dá»¥ng trÃªn chuá»—i ban Ä‘áº§u. PhÆ°Æ¡ng trÃ¬nh trÃªn cÆ¡ báº£n lÃ  dá»± bÃ¡o cÃ¡c tá»« bá»‹ che (mask) vá»›i Ä‘iá»u kiá»‡n cÃ¢u Ä‘áº§u vÃ o bá»‹ che. 
- VÃ­ dá»¥ nhÆ° ta cÃ³ cÃ¢u Ä‘áº§u vÃ o nhÆ° sau: â€œCon mÃ¨o trÃ¨o lÃªn cÃ¢yâ€. Ta cÃ³ thá»ƒ mask tá»« â€œtrÃ¨oâ€ vÃ  tá»« â€œcÃ¢yâ€ vá»›i token [MASK] nhÆ° sau: â€œCon mÃ¨o [MASK] lÃªn [MASK]â€. Sau Ä‘Ã³ chÃºng ta sáº½ encoder cÃ¡c tá»« trong cÃ¢u Ä‘Ã³ sang vector ð‘‘ chiá»u. GiÃ¡ sá»­ chÃºng ta ra Ä‘Æ°á»£c má»™t táº­p cÃ¡c vector biá»ƒu diá»…n cÃ¢u trÃªn nhÆ° sau $\mathbf{D} = [\mathbf{d}_1, \mathbf{d}_2, \mathbf{d}_3, \mathbf{d}_4, \mathbf{d}_5]$  ($\mathbf{d}_3$ vÃ  $\mathbf{d}_5$ chÃ­nh lÃ  biá»ƒu diá»…n vector cá»§a token [MASK]). Khi Ä‘Ã³ hÃ m Loss sáº½ Ä‘Æ°á»£c tÃ­nh nhÆ° sau: 


$$
\mathcal{L}_{\text{MLM}}(\mathbf{D})  = -\log \mathbb{P}(d_3 \mid d_1 d_2 d_4) - \log \mathbb{P}(d_5 \mid d_1 d_2 d_4)
$$


- **Giáº£i mÃ£ (deobfuscation):** ChÃºng ta Ä‘áº§u tiÃªn xem xÃ©t viá»‡c giáº£i mÃ£ Ä‘á»‹nh danh (DOBF), trong Ä‘Ã³ tiá»n huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n tÃªn cÃ¡c Ä‘á»‹nh danh Ä‘Ã£ bá»‹ che giáº¥u. TÆ°Æ¡ng tá»± nhÆ° cÃ¡c láº­p trÃ¬nh viÃªn con ngÆ°á»i, Ä‘á»ƒ giáº£i mÃ£ mÃ£ nguá»“n (dá»± Ä‘oÃ¡n cÃ¡c Ä‘á»‹nh danh), mÃ´ hÃ¬nh cáº§n pháº£i hiá»ƒu cáº£ ngá»¯ nghÄ©a vÃ  cáº¥u trÃºc cá»§a mÃ£ nguá»“n. CÅ©ng cáº§n lÆ°u Ã½ ráº±ng cÃ¡c token ngÃ´n ngá»¯ tá»± nhiÃªn (NL), tá»©c lÃ  cÃ¡c docstring vÃ  comment, Ä‘Æ°á»£c loáº¡i trá»« khá»i viá»‡c che giáº¥u mÃ£ nguá»“n. Khi mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n tÃªn Ä‘á»‹nh danh, nÃ³ cÃ³ thá»ƒ hÆ°á»Ÿng lá»£i tá»« viá»‡c nhÃ¬n vÃ o vÃ  liÃªn káº¿t vá»›i cÃ¡c token NL trong comment hoáº·c docstring vÃ¬ chÃºng thÆ°á»ng mang Ã½ nghÄ©a phong phÃº vá» mÃ£ nguá»“n. Káº¿t quáº£ lÃ , mÃ´ hÃ¬nh Ä‘Æ°á»£c khuyáº¿n khÃ­ch há»c cÃ¡c biá»ƒu diá»…n chung giá»¯a ngÃ´n ngá»¯ láº­p trÃ¬nh vÃ  ngÃ´n ngá»¯
tá»± nhiÃªn tá»‘t hÆ¡n, Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n qua hiá»‡u suáº¥t tÃ¬m kiáº¿m NL2Code tá»‘t hÆ¡n mÃ  DOBF Ä‘áº¡t Ä‘Æ°á»£c so vá»›i chiáº¿n lÆ°á»£c che giáº¥u ngáº«u nhiÃªn.

- DOBF ban Ä‘áº§u Ä‘Æ°á»£c Ä‘á» xuáº¥t cho cÃ¡c mÃ´ hÃ¬nh Seq2Seq (anne Lachaux et al., 2021; Wang et al., 2021b). Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn nÃ³ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng bá»™ mÃ£ hÃ³a. ThÃ¡ch thá»©c chÃ­nh khi Ã¡p dá»¥ng DOBF cho cÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng bá»™ mÃ£ hÃ³a lÃ  viá»‡c xÃ¢y dá»±ng báº£n Ä‘á»“ má»™t-má»™t giá»¯a cÃ¡c token máº·t náº¡ (Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ - LM) vÃ  cÃ¡c token Ä‘á»‹nh danh (nhÃ£n Ä‘áº§u ra) do sá»± khÃ¡c biá»‡t trong viá»‡c mÃ£ hÃ³a mÃ£ nguá»“n (vÃ­ dá»¥, sá»­ dá»¥ng tree-sitter) vÃ  mÃ£ hÃ³a cá»¥ thá»ƒ theo mÃ´ hÃ¬nh (vÃ­ dá»¥, sá»­ dá»¥ng bá»™ mÃ£ hÃ³a sentencepiece). ChÃºng tÃ´i tháº£o luáº­n ngáº¯n gá»n vá» thÃ¡ch thá»©c nÃ y trong Phá»¥ lá»¥c A.5.

- **Máº·t náº¡ ngáº«u nhiÃªn (Random Masking):** NgoÃ i ra, chÃºng tÃ´i cÅ©ng Ã¡p dá»¥ng chiáº¿n lÆ°á»£c máº·t náº¡ token ngáº«u nhiÃªn trong BERT (Devlin et al., 2019b) vÃ¬ hai lÃ½ do chÃ­nh. Thá»© nháº¥t, Ä‘á»ƒ thÃºc Ä‘áº©y cÃ¡c biá»ƒu diá»…n tá»‘t hÆ¡n báº±ng cÃ¡ch giÃºp mÃ´ hÃ¬nh há»c vÆ°á»£t ra ngoÃ i cÃ¡c Ä‘á»‹nh danh. Láº¥y Python lÃ m vÃ­ dá»¥, khoáº£ng 30% cÃ¡c token mÃ£ liÃªn quan Ä‘áº¿n Ä‘á»‹nh danh, do Ä‘Ã³ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c biá»ƒu diá»…n tá»‘t hÆ¡n báº±ng cÃ¡ch mÃ£ hÃ³a thÃ´ng tin Ä‘Æ°á»£c mang bá»Ÿi 70% token cÃ²n láº¡i. Thá»© hai, khÃ´ng pháº£i láº­p trÃ¬nh viÃªn nÃ o cÅ©ng tuÃ¢n theo quy Æ°á»›c Ä‘áº·t tÃªn, vÃ­ dá»¥: cÃ¡c tÃªn biáº¿n vÃ´ nghÄ©a nhÆ° v1, v2, v3 cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng. Viá»‡c dá»± Ä‘oÃ¡n cÃ¡c token nÃ y lÃ  khÃ´ng cáº§n thiáº¿t vÃ  cung cáº¥p tÃ­n hiá»‡u huáº¥n luyá»‡n ráº¥t háº¡n cháº¿.

- ChÃºng tÃ´i khÃ´ng tuÃ¢n theo quy Æ°á»›c máº·t náº¡ 80-10-10 Ä‘Æ°á»£c Ä‘á» xuáº¥t trong chuáº©n MLM cho vÄƒn báº£n (Devlin et al., 2019b). VÃ¬ mÃ£ nguá»“n bao gá»“m cÃ¡c thÃ nh pháº§n ngÃ´n ngá»¯ tá»± nhiÃªn (NL) vÃ  mÃ£ (tá»©c lÃ  Ä‘á»‹nh danh, tá»« khÃ³a, toÃ¡n tá»­), viá»‡c thay tháº¿ ngáº«u nhiÃªn cÃ¡c token cÃ³ thá»ƒ lÃ m tá»•n háº¡i cáº£ cáº¥u trÃºc vÃ  Ã½ nghÄ©a cá»§a mÃ£, dáº«n Ä‘áº¿n sá»± suy giáº£m trong há»c biá»ƒu diá»…n.Â³ ChÃºng tÃ´i cho tháº¥y trong Má»¥c 4.2.1 ráº±ng quy Æ°á»›c 80-10-10 nÃ y dáº«n Ä‘áº¿n káº¿t quáº£ kÃ©m hÆ¡n má»™t cÃ¡ch nháº¥t quÃ¡n trong cÃ¡c tÃ¡c vá»¥ háº¡ nguá»“n. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i cÅ©ng Ä‘áº·t tá»· lá»‡ máº·t náº¡ ngáº«u nhiÃªn lÃ  15%, mÃ  chÃºng tÃ´i tháº¥y lÃ  tá»‘i Æ°u thÃ´ng qua nghiÃªn cá»©u cáº¯t lá»›p cá»§a mÃ¬nh (trÃ¬nh bÃ y trong Phá»¥ lá»¥c A.4). Vá»›i má»—i táº­p huáº¥n luyá»‡n, chÃºng tÃ´i chá»n ngáº«u nhiÃªn giá»¯a DOBF hoáº·c máº·t náº¡ ngáº«u nhiÃªn vá»›i xÃ¡c suáº¥t báº±ng nhau.

### 3.2 Bimodal Contrastive Learning vá»›i Hard Negative vÃ  Hard Posotive

- Giáº£ sá»­ $x_i, x_{i+}$ kÃ­ hiá»‡u cho má»™t cáº·p positive input, cáº·p nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra bá»Ÿi data augmentation tá»« anchor ban Ä‘áº§u (nhÆ°ng khÃ´ng Ä‘áº£m báº£o viá»‡c cÃ¡c pháº§n tá»­ Ä‘Æ°á»£c táº¡o ra tá»« achor lÃ  má»™t cáº·p positive input), $h_i, h_{i+}$ lÃ  representations cá»§a $x_i, x_{i+}$ qua layer cuá»‘i cÃ¹ng cá»§a encoder.
- Äáº·t $\mathcal{B} = [h_1, h_{1+}, h_2, h_{2+}, \dots, h_N, h_{N+}]$ kÃ­ hiá»‡u cho má»™t táº­p $h_i, h_{i+}$ Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn tá»« má»™t batch cÃ³ $N$ cáº·p. ChÃºng ta pháº£i tá»‘i Æ°u hÃ³a hÃ m Loss Ä‘á»‘i xá»©ng sau:

$$
\mathcal{L}_{CL}(h_i, h_{i+}) = - \left( 
\log \frac{\exp(h_i \cdot h_{i+} / \tau)}{\exp(h_i \cdot h_{i+} / \tau) + \sum_{k \in \mathcal{B} \setminus \{i, i+\}} \gamma_i^k \cdot \exp(h_i \cdot h_k / \tau)}
+ 
\log \frac{\exp(h_{i+} \cdot h_i / \tau)}{\exp(h_{i+} \cdot h_i / \tau) + \sum_{k \in \mathcal{B} \setminus \{i, i+\}} \gamma_{i+}^k \cdot \exp(h_{i+} \cdot h_k / \tau)}
\right).
$$

- á»ž Ä‘Ã¢y, $\tau$ lÃ  siÃªu tham sá»‘ nhiá»‡t Ä‘á»™ mÃ  chÃºng tÃ´i Ä‘áº·t giÃ¡ trá»‹ lÃ  
0.05 trong nghiÃªn cá»©u nÃ y. KÃ½ hiá»‡u $\cdot$ biá»ƒu thá»‹ sá»± tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a hai vector biá»ƒu diá»…n. $\gamma_i^k$ lÃ  tham sá»‘ trá»ng sá»‘, sáº½ Ä‘Æ°á»£c giáº£i thÃ­ch chi tiáº¿t sau.

- Hard Negative: VÃ¬ Ä‘Ã¢y khÃ´ng pháº£i lÃ  bÃ i toÃ¡n cÃ³ giÃ¡m sÃ¡t nÃªn khÃ¡ khÃ³ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh hard negative. ChÃºng tÃ´i sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng giÃ¡m sÃ¡t dá»±a trÃªn khoáº£ng cÃ¡ch, Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Zhang et al. (2021). Vá»›i má»™t anchor $h_i$, cÃ¡c hard negative lÃ  nhá»¯ng vÃ­ dá»¥ khÃ¡c nhau vá» máº·t ngá»¯ nghÄ©a nhÆ°ng Ä‘Æ°á»£c Ã¡nh xáº¡ gáº§n $h_i$ trong khÃ´ng gian biá»ƒu diá»…n. Do Ä‘Ã³, má»™t hard negative cÃ ng gáº§n vá»›i anchor $h_i$ trong khÃ´ng gian biá»ƒu diá»…n thÃ¬ giÃ¡ trá»‹ $\gamma$ mong muá»‘n cÃ ng lá»›n, cá»¥ thá»ƒ Ä‘Æ°á»£c tÃ­nh bá»Ÿi cÃ´ng thá»©c sau:

$$
\gamma_i^k = \frac{\exp(\mathbf{h}_i \cdot \mathbf{h}_k / \tau)}{\exp(\mathbf{h}_i \cdot \mathbf{h}_k / \tau) + \sum_{j \in \mathcal{B} \setminus (i, i^+, k)} \exp(\mathbf{h}_i \cdot \mathbf{h}_j / \tau)}.

$$

- Äiá»u nÃ y cÃ³ nghÄ©a lÃ  $\gamma_i^k$ Æ°á»›c lÆ°á»£ng táº§m quan trá»ng tÆ°Æ¡ng Ä‘á»‘i cá»§a $h_k$ Ä‘á»‘i vá»›i Ä‘iá»ƒm neo $h_i$ trong sá»‘ táº¥t cáº£ $2N-2$ Ä‘iá»ƒm Ã¢m trong batch. Máº·c dÃ¹ cÃ¡c vÃ­ dá»¥ huáº¥n luyá»‡n cÃ³ tÃ­nh tÆ°Æ¡ng Ä‘Æ°Æ¡ng ngá»¯ nghÄ©a ngoáº¡i trá»« cÃ¡c cáº·p tÃ­ch cá»±c Ä‘Ã£ cho khÃ´ng cÃ³ sáºµn trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, sá»± Æ°á»›c lÆ°á»£ng á»Ÿ trÃªn Ä‘á»‘i vá»›i cÃ¡c Ä‘iá»ƒm Ã¢m khÃ³ (hard negatives) váº«n há»£p lÃ½. Äá»ƒ tháº¥y Ä‘Æ°á»£c Ä‘iá»u nÃ y, lÆ°u Ã½ ráº±ng má»—i batch huáº¥n luyá»‡n Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn vá»›i kÃ­ch thÆ°á»›c nhá» hÆ¡n nhiá»u so vá»›i kÃ­ch thÆ°á»›c cá»§a toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n. Do Ä‘Ã³, sá»± hiá»‡n diá»‡n cá»§a cÃ¡c Ä‘iá»ƒm Ã¢m sai trong má»—i batch lÃ  ráº¥t nhá» miá»…n lÃ  dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»§ lá»›n vÃ  Ä‘a dáº¡ng. Trong bÃ i viáº¿t nÃ y, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c batch (N) lÃ  8K, dÆ°á»›i Ä‘iá»u kiá»‡n nÃ y, chÃºng tÃ´i quan sÃ¡t tháº¥y hiá»‡u suáº¥t tÄƒng Ä‘Æ¡n Ä‘iá»‡u Ä‘Æ°á»£c bÃ¡o cÃ¡o trÃªn cÃ¡c tÃ¡c vá»¥ háº¡ nguá»“n.

- **Hard Positive**: ChÃºng tÃ´i xem xÃ©t cÃ¡c cáº·p (vÄƒn báº£n, hÃ m) xuáº¥t hiá»‡n tá»± nhiÃªn nhÆ° lÃ  cÃ¡c cáº·p tÃ­ch cá»±c, trong Ä‘Ã³ vÄƒn báº£n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« chuá»—i tÃ i liá»‡u (docstring) cá»§a hÃ m (Husain vÃ  cá»™ng sá»±, 2019). VÄƒn báº£n trÃ­ch xuáº¥t tá»« docstring thÆ°á»ng tÃ³m táº¯t tá»‘t cÃ¡c ngá»¯ nghÄ©a cáº¥p cao cá»§a mÃ£. Do Ä‘Ã³, viá»‡c há»c Ä‘á»‘i khÃ¡ng vá»›i dá»¯ liá»‡u hai chiá»u, tá»©c lÃ  cÃ¡c cáº·p vÄƒn báº£n vÃ  hÃ m, nÃ¢ng cao Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t tÃ¬m kiáº¿m ngá»¯ nghÄ©a NL2Code. Trong Má»¥c 4.2.2, thÃªm vÃ o Ä‘Ã³, vÄƒn báº£n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« mÃ£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng ngá»¯ nghÄ©a, báº¥t ká»ƒ tá»« cÃ¹ng má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh hay cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau, thÆ°á»ng kÃ©m Ä‘a dáº¡ng hÆ¡n so vá»›i chÃ­nh mÃ£ Ä‘Ã³. Do Ä‘Ã³, cÃ¡c mÃ£ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a cÃ³ thá»ƒ Ä‘Æ°á»£c nhÃ³m láº¡i vá»›i nhau má»™t cÃ¡ch ngáº§m thÃ´ng qua cÃ¡c vÄƒn báº£n tÃ³m táº¯t giá»‘ng nhau hoáº·c ráº¥t tÆ°Æ¡ng tá»±. Káº¿t luáº­n cá»§a chÃºng tÃ´i Ä‘Æ°á»£c xÃ¡c thá»±c bá»Ÿi sá»± cáº£i thiá»‡n hiá»‡u suáº¥t lá»›n Ä‘áº¡t Ä‘Æ°á»£c khi há»c Ä‘á»‘i khÃ¡ng trong cáº£ tÃ¬m kiáº¿m Code2Code cÃ¹ng ngÃ´n ngá»¯ vÃ  khÃ¡c ngÃ´n ngá»¯ trong Má»¥c 4.2.2.
- CÅ©ng dá»… tháº¥y ráº±ng cÃ¡c tÃªn hÃ m vÃ  tÃªn biáº¿n Ä‘áº§u vÃ o thÆ°á»ng chia sáº» sá»± tÆ°Æ¡ng Ä‘á»“ng Ä‘Ã¡ng ká»ƒ, Ä‘áº·c biá»‡t lÃ  vá» má»©c Ä‘á»™ trÃ¹ng láº·p tá»« vá»±ng vá»›i vÄƒn báº£n tÃ³m táº¯t. ChÃºng tÃ´i tiáº¿p tá»¥c Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ trÃ¹ng láº·p nÃ y vá»›i cÃ¡c thá»‘ng kÃª chi tiáº¿t trong Phá»¥ lá»¥c A.6. Tá»« Ä‘Ã³, chÃºng tÃ´i táº¡o cÃ¡c hard positives báº±ng cÃ¡ch loáº¡i bá» cáº£ chá»¯ kÃ½ hÃ m vÃ  cÃ¡c cÃ¢u lá»‡nh tráº£ vá». NhÆ° Ä‘Æ°á»£c minh há»a trong Má»¥c 4.2.2, cÃ¡c hard positives Ä‘Æ°á»£c hÃ¬nh thÃ nh theo cÃ¡ch nÃ y cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a viá»‡c há»c Ä‘á»‘i khÃ¡ng.

> Coming soon