---
title: "Tá»•ng quan vá» Contrastive Learning"
date: 2025-01-09 00:00:00  + 0800
categories: [AI Research]
tags: [contrastive learning]
---
---


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>

Contrastive Learning lÃ  ká»¹ thuáº­t há»c biá»ƒu diá»…n báº±ng cÃ¡ch Ä‘Æ°a cÃ¡c máº«u tÆ°Æ¡ng Ä‘á»“ng láº¡i gáº§n vÃ  Ä‘áº©y cÃ¡c máº«u khÃ¡c biá»‡t ra xa trong khÃ´ng gian vector. PhÆ°Æ¡ng phÃ¡p nÃ y, qua cÃ¡c mÃ´ hÃ¬nh nhÆ° `SimCLR` hay `MoCo`, ná»•i báº­t trong há»c khÃ´ng giÃ¡m sÃ¡t vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn nhiá»u bÃ i toÃ¡n AI.

## 1. Contrastive learning lÃ  gÃ¬ ?

- Ã tÆ°á»Ÿng chÃ­nh cá»§a `Contrastive learning` lÃ  tÃ¬m ra cÃ¡c cáº·p Ä‘áº·c trÆ°ng cá»§a dá»¯ liá»‡u cÃ³ tÃ­nh tÆ°Æ¡ng Ä‘á»“ng - tÆ°Æ¡ng pháº£n nhau trong bá»™ dataset. Tá»« Ä‘Ã³, vá»›i nhá»¯ng cáº·p dá»¯ liá»‡u mang tÃ­nh tÆ°Æ¡ng Ä‘á»“ng ta cÃ³ thá»ƒ "kÃ©o" chÃºng láº¡i gáº§n Ä‘á»ƒ há»c Ä‘Æ°á»£c nhá»¯ng Ä‘áº·c trÆ°ng cáº¥p cao hÆ¡n cá»§a nhau, vÃ  ngÆ°á»£c láº¡i vá»›i nhá»¯ng cáº·p nhá»¯ng liá»‡u tÆ°Æ¡ng pháº£n sáº½ bá»‹ "Ä‘áº©y" ra xa. Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u nÃ y, ta sáº½ cáº§n sá»­ dá»¥ng cÃ¡c `similarity metric` Ä‘á»ƒ tÃ­nh toÃ¡n khoáº£ng cÃ¡ch giá»¯a cÃ¡c `embedding vector` biá»ƒu diá»…n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vá»›i nhau. VÃ­ dá»¥, ta Ä‘Ã£ cÃ³ 1 Ä‘iá»ƒm dá»¯ liá»‡u gá»‘c gá»i lÃ  `anchor`, sau Ä‘Ã³ cÃ³ thá»ƒ dÃ¹ng thÃªm cÃ¡c ká»¹ thuáº­t `augmentation` khÃ¡c nhau Ä‘á»ƒ cÃ³ thÃªm 1 biáº¿n thá»ƒ tá»« anchor gá»‘c gá»i lÃ  `positive sample`, vÃ  pháº§n cÃ²n láº¡i cá»§a batch / dataset sáº½ Ä‘Æ°á»£c coi lÃ  `negative sample`. Sau Ä‘Ã³ model sáº½ Ä‘Æ°á»£c train Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c `positive sample` vá»›i `negative sample` tá»« 1 cá»¥m dá»¯ liá»‡u.

![anh](./image/247.png)

- Nghe cÃ³ váº» hÆ¡i trá»«u tÆ°á»£ng nhá»‰ ? Láº¥y má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n nhÆ° tháº¿ nÃ y nhÃ©. Con ngÆ°á»i chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c váº­t thá»ƒ khÃ¡c nhau tá»« khi cÃ²n bÃ© ( tháº­m chÃ­ lÃ  sÆ¡ sinh), vÃ  nhá»¯ng váº­t thá»ƒ Ä‘áº¥y sau nÃ y sáº½ Ä‘Æ°á»£c ngÆ°á»i lá»›n "gÃ¡n nhÃ£n". Váº­y táº¡i sao chÃºng ta láº¡i cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c váº­t thá»ƒ Ä‘áº¥y tá»« láº§n nhÃ¬n Ä‘áº§u tiÃªn ? Bá»Ÿi vÃ¬ nÃ£o bá»™ cá»§a chÃºng ta cÃ³ thá»ƒ nháº­n biáº¿t Ä‘Æ°á»£c nhá»¯ng Ä‘áº·c Ä‘iá»ƒm, Ä‘áº·c trÆ°ng khÃ¡c biá»‡t nháº¥t cá»§a má»™t váº­t thá»ƒ hay cÃ²n gá»i lÃ  Ä‘áº·c trÆ°ng báº­c cao, vÃ  Ä‘em so sÃ¡nh nhá»¯ng Ä‘áº·c trÆ°ng Ä‘áº¥y vá»›i "dá»¯ liá»‡u" cÃ³ sáºµn trong nÃ£o bá»™ Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c nhá»¯ng váº­t thá»ƒ giá»‘ng - khÃ¡c loáº¡i ( loÃ i ). Chá»‰ cáº§n nháº­n ra nhá»¯ng Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng vÃ  khÃ¡c biá»‡t giá»¯a váº­t khÃ¡c nhau, bá»™ nÃ£o cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm cáº¥p cao cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng trong tháº¿ giá»›i cá»§a chÃºng ta. VÃ­ dá»¥, chÃºng ta cÃ³ thá»ƒ nháº­n ra má»™t cÃ¡ch vÃ´ thá»©c ráº±ng hai con mÃ¨o á»Ÿ áº£nh trÃªn cÃ³ Ä‘Ã´i tai nhá»n, trong khi con chÃ³ cÃ³ Ä‘Ã´i tai cá»¥p xuá»‘ng. Hoáº·c chÃºng ta cÃ³ thá»ƒ Ä‘á»‘i chiáº¿u chiáº¿c mÅ©i nhÃ´ ra cá»§a chÃ³ vá»›i máº·t pháº³ng cá»§a mÃ¨o. Vá» cÆ¡ báº£n thÃ¬ `contrastive learning` cÅ©ng cho phÃ©p model cá»§a chÃºng ta lÃ m Ä‘iá»u tÆ°Æ¡ng tá»±, má»¥c tiÃªu cá»§a phÆ°Æ¡ng phÃ¡p lÃ  Ä‘á»‘i chiáº¿u sá»± tÆ°Æ¡ng pháº£n giá»¯a `embedding`cá»§a cÃ¡c phiÃªn báº£n biáº¿n Ä‘á»•i cá»§a cÃ¹ng má»™t sample Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng báº¥t biáº¿n cá»§a nÃ³ trong khi váº«n phÃ¢n biá»‡t Ä‘Æ°á»£c embedding cá»§a cÃ¡c sample khÃ¡c.

### Táº¡i sao Contrastive learning láº¡i Ä‘Æ°á»£c coi lÃ  má»™t phÆ°Æ¡ng phÃ¡p cá»±c ká»³ máº¡nh máº½ vÃ  cÃ ng ngÃ y cÃ ng phá»• biáº¿n ?

- CÃ¡c phÆ°Æ¡ng phÃ¡p `supervised learning` truyá»n thá»‘ng phá»¥ thuá»™c ráº¥t nhiá»u vÃ o lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n sáºµn trong khi lÆ°á»£ng data chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n lÃ  vÃ´ cÃ¹ng khá»•ng lá»“. Máº·c dÃ¹ sá»‘ dataset Ä‘Ã£ cÃ³ nhÃ£n lÃ  ráº¥t lá»›n nhÆ°ng vá»›i sá»± gia tÄƒng máº¡nh máº½ cá»§a nhu cáº§u vá» lÆ°á»£ng dá»¯ liá»‡u cÅ©ng nhÆ° cÃ¡c bÃ i toÃ¡n má»›i thÃ¬ sá»‘ dataset Ä‘áº¥y lÃ  hoÃ n toÃ n khÃ´ng Ä‘á»§. Nháº¥t lÃ  vá»›i nhá»¯ng bÃ i toÃ¡n cáº§n gÃ¡n nhÃ£n chÃ­nh xÃ¡c Ä‘áº¿n tá»«ng pixel nhÆ° `semantic segmentation` thÃ¬ Ä‘Ã¢y lÃ  viá»‡c vÃ´ cÃ¹ng tá»‘n cÃ´ng sá»©c vÃ  thá»i gian. VÃ  tá»« Ä‘áº¥y cÃ¡c phÆ°Æ¡ng phÃ¡p há»c `self/semi - supervised learning` lÃªn ngÃ´i. Vá»›i cÃ¡c ká»¹ thuáº­t `self-supervised learning` thÃ¬ chÃºng ta cÃ³ thá»ƒ train Ä‘á»ƒ model há»c ráº¥t tá»‘t tá»« data chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n vÃ  `self - supervised learning` Ä‘Æ°á»£c Ã¡p dá»¥ng khÃ¡ phá»• biáº¿n vá»›i 2 hÆ°á»›ng Ä‘i : `GANs` vÃ  `Contrastive learning`. Trong vÃ i nÄƒm qua, cÃ¡c phÆ°Æ¡ng phÃ¡p `self - supervised learning` tá»‘t nháº¥t Ä‘Ã£ dáº§n dáº§n chuyá»ƒn tá»« `pretext task learning` nhÆ° `rotation`, `colorization` vÃ  `jigsaw puzzling` sang `contrastive learning` vÃ  xu hÆ°á»›ng nÃ y Ä‘ang ngÃ y cÃ ng Ä‘Æ°á»£c cá»§ng cá»‘ nhá» viá»‡c cÃ³ nhiá»u hÆ¡n nhá»¯ng nghiÃªn cá»©u mang tÃ­nh Ä‘á»™t phÃ¡ trong `computer vision` vá»›i `contrastive learning`.


## 2. CÃ¡ch thá»±c hiá»‡n cá»§a Contrastive Representation Learning

![anh](./image/248.gif)



- Vá» pipeline thá»±c hiá»‡n cá»§a `contrastive learning` thÃ¬ mÃ¬nh sáº½ diá»…n Ä‘áº¡t theo hÆ°á»›ng cá»§a bÃ i [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709v3.pdf), vÃ¬ hÆ°á»›ng nÃ y lÃ  hÆ°á»›ng chÃ­nh cá»§a contrastive learning vÃ  khÃ¡ dá»… hiá»ƒu Ä‘á»“ng thá»i mÃ¬nh cÅ©ng tÃ¬m Ä‘Æ°á»£c 1 implementation khÃ¡ Ä‘Æ¡n giáº£n cá»§a bÃ i bÃ¡o, cÃ¡c báº¡n cÃ³ thá»ƒ xem á»Ÿ cuá»‘i bÃ i viáº¿t ğŸ˜‰. CÃ¡c bÆ°á»›c thá»±c hiá»‡n nhÆ° sau:
1. Data Augmentation
2. Encoder
3. Projection head
4. Loss function

### Data augmentation

- `Data augmentation` lÃ  1 pháº§n khÃ¡ quan trá»ng trong cÃ¡c mÃ´ hÃ¬nh `contrastive learning` vÃ  gáº§n nhÆ° lÃ  khÃ´ng thá»ƒ thiáº¿u Ä‘Æ°á»£c. Trong bÃ i `SoTA SImCLR` cá»§a nhÃ³m tÃ¡c giáº£ Ä‘áº¿n tá»« Google Research, ngÆ°á»i ta Ä‘Ã£ dÃ nh háº³n 1 chÆ°Æ¡ng Ä‘á»ƒ nÃ³i vá» data augmentation vÃ  táº§m quan trá»ng cá»§a nÃ³ Ä‘á»‘i vá»›i contrastive learning :`Composition of data augmentation operations is crucial for learning good representations` vÃ  `Contrastive learning needs stronger data augmentation than supervised learning`. Náº¿u `data augmentation` khÃ´ng Ä‘á»§ phá»©c táº¡p thÃ¬ gradient sáº½ khÃ´ng Ä‘á»§ tá»‘t Ä‘á»ƒ model cÃ³ thá»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng trong dá»¯ liá»‡u vÃ  ngÆ°á»£c láº¡i. CÃ¡c tÃ¡c giáº£ chá»§ yáº¿u sá»­ dá»¥ng tuáº§n tá»± 3 loáº¡i augment : `random crop`, `random color distortions`, vÃ  `random Gaussian blur`. VÃ  nháº­n ra ráº±ng `random crop` vÃ  `color distortion` lÃ  khÃ¡ quan trá»ng Ä‘á»ƒ model Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t. NhÆ°ng náº¿u chá»‰ sá»­ dá»¥ng riÃªng ráº½ 1 loáº¡i biáº¿n Ä‘á»•i thÃ¬ sáº½ khÃ´ng cho ra káº¿t quáº£ vÆ°á»£t trá»™i, chá»‰ khi káº¿t há»£p 2 loáº¡i biáº¿n Ä‘á»•i vá»›i nhau thÃ¬ káº¿t quáº£ má»›i Ä‘áº¡t tá»›i "state-of-the-art".


![anh](./image/250.png)

### Encoder

- ThÃ´ng thÆ°á»ng, encoder lÃ  1 máº¡ng CNN cÃ³ nhiá»‡m vá»¥ map cÃ¡c Ä‘áº§u vÃ o lÃ  áº£nh thÃ nh cÃ¡c `embedding vector` lÃ m Ä‘áº§u vÃ o cho `contrastive loss`. Náº¿u máº¡ng encoder trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng Ä‘á»§ tá»‘t thÃ¬ model sáº½ ráº¥t khÃ³ Ä‘á»ƒ há»c Ä‘Æ°Æ¡c cÃ¡ch phÃ¢n biá»‡t cÃ¡c Ä‘áº·c Ä‘iá»ƒm cÃ³ trong 1 Ä‘iá»ƒm dá»¯ liá»‡u. Äa sá»‘ máº¡ng encoder Ä‘Æ°á»£c sá»­ dá»¥ng hiá»‡n nay lÃ  `Resnet` vÃ  biáº¿n thá»ƒ cá»§a nÃ³, trong sá»‘ Ä‘Ã³ thÃ¬ cÃ³ láº½ `Resnet-50` lÃ  biáº¿n thá»ƒ Ä‘Æ°á»£c cÃ¡c nhÃ  nghiÃªn cá»©u sá»­ dá»¥ng nhiá»u nháº¥t vÃ¬ tÃ­nh cÃ¢n báº±ng giá»¯a kÃ­ch thÆ°á»›c vÃ  hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.


![anh](./image/251.png)

- Äá»ƒ `model encoder` há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm cá»§a váº­t thá»ƒ vá»›i hÆ°á»›ng Ä‘i cá»§a self-supervised thÃ¬ pháº£i tráº£i qua pretext task ( sáº½ Ä‘Æ°á»£c nÃ³i á»Ÿ pháº§n sau ) sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n qua vá»›i pretext task thÃ¬ model sáº½ cÃ³ kháº£ nÄƒng trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»‘t hÆ¡n, Ä‘á»“ng thá»i cÃ³ thá»ƒ kiÃªm thÃªm downstream task. Má»™t sá»‘ model thuá»™c lÄ©nh vá»±c unsupervised learning tháº­m chÃ­ cÃ²n cÃ³ thá»ƒ "outperform" cÃ¡c model SoTA cá»§a supervised learning, tiÃªu biá»ƒu nhÆ° MoCo cá»§a Facebook hay SimCLR cá»§a Google. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ dáº§n thu háº¹p Ä‘Æ°á»£c khoáº£ng cÃ¡ch giá»¯a unsupervised vÃ  supervised representation learning trong má»™t sá»‘ task computer vision.

###  Projection Head

- Sau khi data Ä‘i qua khá»‘i encoder thÃ¬ sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n vá»›i projection head Ä‘Æ°á»£c cáº¥u thÃ nh tá»« cÃ¡c fully-connected layer Ä‘á»ƒ cho ra output lÃ  1 embedding vector biá»ƒu diá»…n cho áº£nh á»Ÿ Ä‘áº§u vÃ o. Máº¡ng nÃ y cÃ³ tÃ¡c dá»¥ng khuáº¿ch Ä‘áº¡i cÃ¡c Ä‘áº·c trÆ°ng báº¥t biáº¿n cá»§a dá»¯ liá»‡u tá»« Ä‘Ã³ tá»‘i Ä‘a hÃ³a kháº£ nÄƒng phÃ¢n biá»‡t cÃ¡c phÃ©p biáº¿n Ä‘á»•i tá»« cÃ¹ng 1 bá»©c áº£nh cá»§a máº¡ng.


### Loss function

- Má»™t Ä‘iá»u cuá»‘i cÃ¹ng khÃ´ng thá»ƒ thiáº¿u Ä‘Ã³ chÃ­nh lÃ  hÃ m loss, sá»± thÃ nh cÃ´ng cá»§a contrastive learning theo mÃ¬nh tháº¥y thÃ¬ chá»§ yáº¿u nhá» sá»± Ä‘Ã³ng gÃ³p cá»§a cÃ¡c hÃ m `contrastive loss`, model cÃ³ há»c Ä‘Æ°á»£c "contrastive representation" hay khÃ´ng phá»¥ thuá»™c ráº¥t nhiá»u vÃ o hÃ m loss mÃ  framework sá»­ dá»¥ng. VÃ¬ pháº§n nÃ y khÃ¡ quan trá»ng nÃªn mÃ¬nh sáº½ cá»‘ gáº¯ng giáº£i thÃ­ch chi tiáº¿t nháº¥t cÃ³ thá»ƒ. 
- HÃ m loss Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ trong bÃ i bÃ¡o SimCLR Ä‘Æ°á»£c gá»i lÃ  NT-Xent (normalized temperature-scaled cross entropy loss). Äáº§u tiÃªn, ngÆ°á»i ta láº¥y máº«u ngáº«u nhiÃªn má»™t minibatch gá»“m $N$ Ä‘iá»ƒm dá»¯ liá»‡u vÃ  Ã¡p dá»¥ng 2 phÃ©p data augmentation khÃ¡c nhau táº¡o thÃ nh $2N$ Ä‘iá»ƒm :

$$
\tilde{x}_i = t(\mathbf{x}), \quad \tilde{x}_j = t'(\mathbf{x}), \quad t, t' \sim \mathcal{T}
$$

- Giáº£ sá»­ cÃ³ 1 cáº·p dá»¯ liá»‡u positive $(\tilde{x}_i, \tilde{x}_j)$ chÃ­nh lÃ  cÃ¹ng 1 áº£nh nhÆ°ng Ä‘Æ°á»£c Ã¡p dá»¥ng 2 phÃ©p augment khÃ¡c nhau táº¡o thÃ nh 2 phiÃªn báº£n gáº§n khÃ¡c nhau 1 chÃºt nhÆ°ng ná»™i dung váº«n giá»‘ng nhau, $2(N-1)$ Ä‘iá»ƒm cÃ²n láº¡i Ä‘Æ°á»£c coi lÃ  cÃ¡c Ä‘iá»ƒm negative. XÃ¡c Ä‘á»‹nh vector biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o bá»Ÿi encoder $f(.)$

$$
\mathbf{h}_i = f(\tilde{\mathbf{x}}_i), \quad \mathbf{h}_j = f(\tilde{\mathbf{x}}_j)
$$

- HÃ m loss sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn vector biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o bá»Ÿi projection head $g(.)$ . Gá»i $\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u}^T \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}$ lÃ  hÃ m cosine similarity cá»§a 2 vector Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a $l_2$ norm. Khi Ä‘Ã³ hÃ m loss cho 1 cáº·p dá»¯ liá»‡u positive $(i,j)$ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:



$$
z_i = g(h_i), \quad z_j = g(h_j)
$$

$$
\mathcal{L}_{\text{SimCLR}}^{(i,j)} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}
$$

$$
\mathbf{1}_{[k \neq i]} = 
\begin{cases} 
1 & k \neq i \\
0 & \text{cÃ²n láº¡i}
\end{cases}
$$

- Vá» cÆ¡ báº£n thÃ¬ hÃ m loss sáº½ hoáº¡t Ä‘á»™ng nhÆ° sau, Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a hÃ m loss thÃ¬ optimizer sáº½ cáº§n Ä‘Æ°a giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a hÃ m loss vá» gáº§n 0 nháº¥t cÃ³ thá»ƒ, nghÄ©a lÃ  giÃ¡ trá»‹ cá»§a hÃ m log pháº£i xáº¥p xá»‰ 1. Äá»ƒ lÃ m Ä‘iá»u nÃ y thÃ¬ tá»­ sá»‘ pháº£i xáº¥p xá»‰ máº«u sá»‘, váº­y lÃ m sao Ä‘á»ƒ tá»­ sá»‘ xáº¥p xá»‰ máº«u sá»‘ ? NhÃ¬n vÃ o láº§n lÆ°á»£t tá»«ng váº¿ má»™t nhÃ©. á» tá»­ sá»‘ chÃºng ta cÃ³ cá»¥m $\exp(\text{sim}(z_i, z_j) / \tau)$, Ä‘Ã¢y lÃ  hÃ m Ä‘o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 1 cáº·p positive, vector biá»ƒu diá»…n cá»§a cáº·p positive cÃ ng giá»‘ng nhau thÃ¬ giÃ¡ trá»‹ cá»§a $\text{sim}(z_i, z_j)$ cÃ ng gáº§n 1, náº¿u khÃ¡c nhau thÃ¬ giÃ¡ trá»‹ nÃ y sáº½ gáº§n 0. á» máº«u sá»‘ ta cÃ³ cá»¥m $\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)$, Ä‘Ã¢y lÃ  tá»•ng cá»§a hÃ m Ä‘o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cá»§a táº¥t cáº£ $2N$ Ä‘iá»ƒm dá»¯ liá»‡u, nghÄ©a lÃ  gá»“m 1 cáº·p positive giá»‘ng nhÆ° á»Ÿ tá»­ sá»‘ cá»™ng thÃªm $(N-1)$ cáº·p negative. Quay láº¡i má»¥c tiÃªu ban Ä‘áº§u cá»§a chÃºng ta lÃ  cá»‘ gáº¯ng Ã©p cho giÃ¡ trá»‹ cá»§a tá»­ sá»‘ ngang báº±ng vá»›i máº«u sá»‘, Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y thÃ¬ buá»™c pháº£i Ã©p cho vector biá»ƒu diá»…n cá»§a cáº·p positive cÃ ng giá»‘ng nhau cÃ ng tá»‘t vÃ  Ã©p cho vector biá»ƒu diá»…n cá»§a cÃ¡c cáº·p negative cÃ ng khÃ¡c nhau cÃ¡c tá»‘t, trÆ°á»ng há»£p lÃ½ tÆ°á»Ÿng nháº¥t khi $\mathcal{L} = 0$ lÃ  lÃºc giÃ¡ trá»‹ similarity cá»§a cáº·p positive báº±ng 1, vÃ  giÃ¡ trá»‹ cá»§a táº¥t cáº£ cÃ¡c cáº·p negative báº±ng 0, khi Ä‘Ã³ ta sáº½ cÃ³ $\mathcal{L} = -\log \frac{1}{1 + 0 + 0 + ... + 0} = -\log(1) = 0$. Náº¿u vector biá»ƒu diá»…n cá»§a cÃ¡c cáº·p negative giá»‘ng nhau thÃ¬ máº«u sá»‘ sáº½ lá»›n hÆ¡n tá»­ sá»‘ ráº¥t nhiá»u, dáº«n Ä‘áº¿n giÃ¡ trá»‹ hÃ m loss sáº½ ráº¥t lá»›n. Do Ä‘Ã³, hÃ m loss nÃ y cÃ³ 2 nhiá»‡m vá»¥ cáº§n lÃ m : kÃ©o giÃ¡ trá»‹ cá»§a vector biá»ƒu diá»…n cáº·p positive láº¡i gáº§n nhau vÃ  Ä‘áº©y giÃ¡ trá»‹ cá»§a vector biá»ƒu diá»…n cÃ¡c cáº·p negative ra xa nhau.

### Pretext task

- Pretext task chÃ­nh lÃ  self-supervised task vá»›i nhiá»‡m vá»¥ huáº¥n luyá»‡n Ä‘á»ƒ model há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng vá»›i dá»¯ liá»‡u chÆ°a Ä‘Æ°á»£c Ä‘Ã¡nh nhÃ£n, vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng thÃ¬ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c gÃ¡n nhÃ£n giáº£ báº±ng cÃ¡ch: biáº¿n Ä‘á»•i áº£nh gá»‘c dá»±a trÃªn cÃ¡c phÃ©p data augmentation (augmented data) lÃ m input cho model vÃ  output sáº½ lÃ  áº£nh gá»‘c Ä‘áº¥y. Tá»©c lÃ  chÃºng ta sáº½ Ä‘á»ƒ model há»c Ä‘Æ°á»£c cÃ¡ch khÃ´i phá»¥c áº£nh gá»‘c dá»±a trÃªn cÃ¡c bá»©c áº£nh Ä‘Ã£ bá»‹ biáº¿n Ä‘á»•i. Má»¥c tiÃªu cá»§a pretext task thÃ´ng thÆ°á»ng khÃ¡c pretext task cá»§a contrastive learning - contrastive prediction task á»Ÿ chá»— pretext task sáº½ cá»‘ gáº¯ng khÃ´i phá»¥c láº¡i áº£nh cÅ© tá»« áº£nh Ä‘Ã£ biáº¿n Ä‘á»•i, cÃ²n contrastive prediction task sáº½ cá»‘ gáº¯ng há»c nhá»¯ng Ä‘áº·c trÆ°ng báº¥t biáº¿n cá»§a áº£nh gá»‘c tá»« áº£nh Ä‘Ã£ biáº¿n Ä‘á»•i trong khi váº«n phÃ¢n biá»‡t Ä‘Æ°á»£c vá»›i cÃ¡c áº£nh negative khÃ¡c trong batch. Model Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n qua pretext task cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhÆ° fine-tune vá»›i cÃ¡c bá»™ dataset Ä‘Ã£ gÃ¡n nhÃ¡n Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c downstream task. CÃ³ 4 loáº¡i pretext task thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘Ã³ lÃ  : color transformation, geometric transformation, context-based tasks, vÃ  cross-modal-based tasks.

### Downstream task

- Downstream task nÃ³i nÃ´m na chÃ­nh lÃ  viá»‡c chÃºng ta táº­n dá»¥ng cÃ¡c kiáº¿n thá»©c model Ä‘Ã£ Ä‘Æ°á»£c há»c qua pretext task Ä‘á»ƒ sá»­ dá»¥ng chÃºng vÃ o má»™t má»¥c Ä‘Ã­ch cá»¥ thá»ƒ nÃ o Ä‘Ã³. Model Ä‘Ã£ Ä‘Æ°á»£c pretrain á»Ÿ pretext task chá»©a nhá»¯ng trá»ng sá»‘ giÃ u thÃ´ng tin vÃ  hoÃ n toÃ n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i nhá»¯ng bÃ i toÃ¡n thÃ­ch há»£p khÃ¡c thÃ´ng qua ká»¹ thuáº­t fine-tune model hay tá»•ng thá»ƒ Ä‘Æ°á»£c gá»i lÃ  transfer learning. Káº¿t quáº£ cá»§a transfer learning vá»›i nhá»¯ng task cÃ³ level cao hÆ¡n kia sáº½ miÃªu táº£ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c há»c. Má»™t sá»‘ downstream task thÆ°á»ng Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  clasification, detection, segmentation, future prediction. Viá»‡c sá»­ dá»¥ng má»™t hay nhiá»u trong cÃ¡c task Ä‘Ã£ Ä‘Æ°á»£c liá»‡t kÃª cÅ©ng cÃ³ thá»ƒ coi lÃ  Ä‘ang kiá»ƒm thá»­ vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a model. Äá»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c há»c cá»§a hÆ°á»›ng self-supervised cho cÃ¡c downstream task, ngÆ°á»i ta thÆ°á»ng sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° kernel visualization, feature map visualization, vÃ  nearest-neighbor-based. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y cÅ©ng há»— trá»£ viá»‡c phÃ¢n tÃ­ch hiá»‡u quáº£ cá»§a pretext task trong training model


### CÃ¡c dáº¡ng kiáº¿n trÃºc khÃ¡c

![anh](./image/252.png)

- Coi 1 áº£nh anchor Ä‘Ã£ Ä‘Æ°á»£c encode lÃ  query $q$ vÃ  1 táº­p cÃ¡c sample Ä‘Ã£ Ä‘Æ°á»£c encode lÃ  {$k_0,k_1,k_2,...$} , táº­p nÃ y ta coi lÃ  key cá»§a 1 bá»™ dictionary. Giáº£ sá»­ trong dictionary cÃ³ khÃ³a $k_{+}$ trÃ¹ng vá»›i query $q$. Váº­y thÃ¬ hÃ m contrastive loss sáº½ lÃ  hÃ m tÃ­nh toÃ¡n ra giÃ¡ trá»‹ tháº¥p náº¿u $q$ giá»‘ng vá»›i positive key $k_{+}$ Ã  khÃ¡c vá»›i táº¥t cáº£ key cÃ²n láº¡i (coi nhá»¯ng key nÃ y lÃ  negative vá»›i $q$). NhÆ° váº­y lÃ  ngÆ°á»i ta Ä‘Ã£ mÃ´ hÃ¬nh hÃ³a contrastive learning thÃ nh bÃ i toÃ¡n Ä‘i tÃ¬m key trong 1 bá»™ dictionary gá»“m ráº¥t nhiá»u key khÃ¡c nhau Ä‘Æ°á»£c lÆ°u trá»¯ dÆ°á»›i dáº¡ng hÃ ng Ä‘á»£i vÃ  kÃ­ch thÆ°á»›c cá»§a dictionary pháº£i lá»›n hÆ¡n mini-batch size ráº¥t nhiá»u.

#### Kiáº¿n trÃºc Memory bank

- Vá»›i SimCLR, ngÆ°á»i ta Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c training vá»›i batch size cÃ ng lá»›n thÃ¬ káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c cÃ ng tá»‘t vÃ  cÃ¡c nhÃ  nghiÃªn cá»©u cá»§a google Ä‘Ã£ cho ra káº¿t quáº£ SoTA vá»›i batch size 4096 trong 100 epoch. CÆ¡ sá»Ÿ cho káº¿t luáº­n nÃ y náº±m á»Ÿ sá»‘ lÆ°á»£ng negative sample cÃ³ trong 1 batch vÃ  hiá»ƒn nhiÃªn lÃ  batch size cÃ ng lá»›n thÃ¬ sá»‘ negative sample cÃ ng nhiá»u. NhÆ°ng Ä‘iá»u nÃ y láº¡i cÃ³ 1 vÃ i báº¥t lá»£i Ä‘Ã³ lÃ  quÃ¡ trÃ¬nh training khÃ¡ tá»‘n tÃ i nguyÃªn vÃ  batch size lá»›n cÅ©ng cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n hÃ m tá»‘i Æ°u hÃ³a, tá»« Ä‘Ã³ ngÆ°á»i ta Ä‘Æ°a ra 1 giáº£i phÃ¡p kháº¯c phá»¥c Ä‘Ã³ lÃ  duy trÃ¬ má»™t bá»™ dictionary gá»i lÃ  Memory Bank. Má»¥c tiÃªu chÃ­nh cá»§a memory bank lÃ  tÃ­ch lÅ©y sá»‘ lÆ°á»£ng lá»›n cÃ¡c Ä‘áº·c trÆ°ng biá»ƒu diá»…n cá»§a negative sample trong suá»‘t quÃ¡ trÃ¬nh training thÃ´ng qua viá»‡c sá»­ dá»¥ng má»™t bá»™ dictionary Ä‘á»ƒ lÆ°u trá»¯ vÃ  Ä‘á»“ng thá»i update liÃªn tá»¥c cÃ¡c embedding vector má»›i nháº¥t. Tuy nhiÃªn, viá»‡c duy trÃ¬ memory bank cÅ©ng gáº·p 1 sá»‘ báº¥t lá»£i, má»™t trong sá»‘ Ä‘Ã³ chÃ­nh lÃ  khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n khÃ¡ lá»›n khi pháº£i update cÃ¡c vector biá»ƒu diá»…n trong memory bank liÃªn tá»¥c bá»Ÿi vÃ¬ cÃ¡c vector biá»ƒu diá»…n nÃ y lá»—i thá»i khÃ¡ nhanh.

#### Kiáº¿n trÃºc Momentum encoder

- Sá»­ dá»¥ng hÃ ng Ä‘á»£i khÃ´ng chá»‰ khiáº¿n dictionary lá»›n mÃ  cÃ²n khiáº¿n key encoder khÃ³ cáº­p nháº­t thÃ´ng qua back-propagation (gradient sáº½ pháº£i Ä‘Æ°á»£c lan truyá»n Ä‘áº¿n táº¥t cáº£ cÃ¡c sample trong hÃ ng Ä‘á»£i). Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã£ nÃªu, ngÆ°á»i ta Ä‘á» xuáº¥t copy luÃ´n gradient tá»« query encoder $f_{q}$ cho key encoder $f_{k}$, nhÆ°ng hÆ°á»›ng Ä‘i nÃ y láº¡i cho káº¿t quáº£ khÃ¡ tá»‡, nguyÃªn nhÃ¢n cÃ³ thá»ƒ lÃ  do viá»‡c thay Ä‘á»•i gradient quÃ¡ nhanh Ä‘Ã£ lÃ m giáº£m tÃ­nh nháº¥t quÃ¡n cá»§a key encoder. VÃ¬ tháº¿ ngÆ°á»i ta Ä‘Ã£ Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p momentum update Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y: Coi cÃ¡c tham sá»‘ cá»§a $f_{k}$ lÃ  $\theta_k$ vÃ  cá»§a $f_{q}$ lÃ  $\theta_q$. $\theta_k$ sáº½ Ä‘Æ°á»£c update vá»›i cÃ´ng thá»©c sau:

$$
\theta_k \leftarrow m\theta_k + (1 - m)\theta_q.
$$

- Vá»›i $m \in [0,1)$ lÃ  há»‡ sá»‘ momentum, vÃ  chá»‰ cÃ³ $\theta_q$ Ã  Ä‘Æ°á»£c cáº­p nháº­t thÃ´ng qua back-propagation. Momentum update sáº½ giÃºp $\theta_k$ Ä‘Æ°á»£c cáº­p nháº­t trÆ¡n tru hÆ¡n $\theta_q$. Lá»£i tháº¿ cá»§a viá»‡c sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y lÃ  khÃ´ng cáº§n pháº£i traing 2 model riÃªng biá»‡t, Ä‘á»“ng thá»i cÅ©ng khÃ´ng cáº§n giá»¯ láº¡i memory bank trÃ¡nh khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n lá»›n vÃ  tá»‘n nhiá»u tÃ i nguyÃªn.

#### Kiáº¿n trÃºc Clustering Feature Representations

- Hai kiáº¿n trÃºc ká»ƒ trÃªn Ä‘á»u táº­p trung vÃ o hÆ°á»›ng so sÃ¡nh cÃ¡c sample sá»­ dá»¥ng similarity metric vÃ  cá»‘ gáº¯ng kÃ©o cÃ¡c sample giá»‘ng nhau láº¡i gáº§n, Ä‘áº©y cÃ¡c sample khÃ¡c nhau ra xa. NhÆ°ng kiáº¿n trÃºc á»Ÿ pháº§n nÃ y láº¡i Ä‘i ngÆ°á»£c vá»›i 2 hÆ°á»›ng ká»ƒ trÃªn, ngÆ°á»i ta Ä‘i theo hÆ°á»›ng end-to-end vá»›i 2 encoders chia sáº» trá»ng sá»‘ vá»›i nhau, nhÆ°ng thay vÃ¬ Ä‘i theo hÆ°á»›ng contrastive thÃ¬ há» sá»­ dá»¥ng thuáº­t toÃ¡n phÃ¢n cá»¥m Ä‘á»ƒ nhÃ³m cÃ¡c Ä‘áº·c trÆ°ng giá»‘ng nhau láº¡i. Má»¥c Ä‘Ã­ch cá»§a viá»‡c nÃ y khÃ´ng chá»‰ lÃ  Ä‘á»ƒ kÃ©o 1 cáº·p sample láº¡i vá»›i nhau mÃ  cÃ²n Ä‘áº£m báº£o Ä‘Æ°á»£c viá»‡c táº¥t cáº£ cÃ¡c sample tÆ°Æ¡ng tá»± vá»›i nhau Ä‘Æ°á»£c phÃ¢n vÃ o 1 cá»¥m vá»›i nhau. VÃ­ dá»¥ trong khÃ´ng gian embedding cá»§a 1 táº­p Ä‘a dáº¡ng cÃ¡c loáº¡i áº£nh, cÃ¡c Ä‘áº·c trÆ°ng cá»§a mÃ¨o sáº½ Ä‘uá»£c kÃ©o láº¡i gáº§n vá»›i cá»§a chÃ³ (cÃ¹ng lÃ  con váº­t) vÃ  Ä‘áº©y ra xa vá»›i cá»§a nhÃ . Má»™t trong nhá»¯ng nghiÃªn cá»©u Ä‘Æ°á»£c Ä‘á» xuáº¥t gáº§n Ä‘Ã¢y á»©ng dá»¥ng phÆ°Æ¡ng phÃ¡p phÃ¢n cá»¥m nÃ³i trÃªn chÃ­nh lÃ  [SwAV](https://proceedings.neurips.cc//paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf):


![anh](./image/253.png)


- Vá»›i contrastive learning thÃ´ng thÆ°á»ng, má»—i sample sáº½ Ä‘Æ°á»£c coi lÃ  1 class riÃªng trong dataset, Ä‘iá»u nÃ y trá»Ÿ nÃªn mÃ¢u thuáº«n khi cÃ³ má»™t áº£nh thuá»™c vá» negative sample nhÆ°ng láº¡i cÃ¹ng lá»›p vá»›i áº£nh gá»‘c, trÆ°á»ng há»£p nÃ y Ä‘Æ°á»£c coi lÃ  false negative. VÃ­ dá»¥ nhÆ° chÃºng ta cÃ³ áº£nh 1 con mÃ¨o lÃ m gá»‘c vÃ  cÃ¡c áº£nh khÃ¡c trong batch sáº½ Ä‘Æ°á»£c gá»i lÃ  negative, váº¥n Ä‘á» sáº½ xáº£y ra khi trong sá»‘ Ä‘áº¥y cÃ³ áº£nh cá»§a 1 hay vÃ i con mÃ¨o ná»¯a. Trong trÆ°á»ng há»£p nÃ y, model báº¯t buá»™c pháº£i há»c ráº±ng 2 bá»©c áº£nh cá»§a 2 con mÃ¨o lÃ  khÃ¡c nhau trong khi chÃºng láº¡i thuá»™c vá» cÃ¹ng 1 class vÃ  cÃ³ thá»ƒ gÃ¢y ra "degradation" - suy thoÃ¡i Ä‘á»‘i vá»›i cháº¥t lÆ°á»£ng vector biá»ƒu diá»…n. Váº¥n Ä‘á» nÃ y Ä‘Æ°á»£c giáº£i quyáº¿t ngáº§m báº±ng cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn phÃ¢n cá»¥m.

## Supervised Contrastive Learning

- Trong paper [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362) Ä‘Æ°á»£c trÃ¬nh bÃ y táº¡i NeurIPS 2020, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ Ä‘á» xuáº¥t má»™t hÃ m loss gá»i lÃ  SupCon - Supervised Contrastive Loss, nháº±m thu háº¹p khoáº£ng cÃ¡ch vÃ´ hÃ¬nh giá»¯a self-supervised learning vÃ  fully-supervised learning vÃ  cho phÃ©p contrastive learning cÃ³ thá»ƒ Ä‘Æ°á»£c á»©ng dá»¥ng vá»›i cÃ¡c bÃ i toÃ¡n supervised learning. Váº«n giá»¯ nguyÃªn Ã½ tÆ°á»Ÿng cá»§a contrastive learning, hÃ m SupCon sáº½ cá»‘ gáº¯ng táº­n dá»¥ng dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»ƒ kÃ©o cÃ¡c embedding vector Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a cá»§a cÃ¹ng 1 class láº¡i gáº§n nhau vÃ  ngÆ°á»£c láº¡i Ä‘á»‘i vá»›i nhá»¯ng embedding vector khÃ¡c class. Viá»‡c cÃ³ thÃªm dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n sáº½ lÃ m Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh chá»n positive sample vÃ  trÃ¡nh Ä‘Æ°á»£c cÃ¡c trÆ°á»ng há»£p false negative.


![anh](./image/254.png)


> Cross-entropy, self-supervised contrastive loss vÃ  supervised contrastive loss. TrÃ¡i: Cross-entropy loss sá»­ dá»¥ng nhÃ£n lÃ  hÃ m softmax loss Ä‘á»ƒ huáº¥n luyá»‡n 1 bá»™ classifier. Giá»¯a: Self-supervised contrastive loss sá»­ dá»¥ng hÃ m contrastive loss vÃ  data augmentation Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng biá»ƒu diá»…n. Pháº£i: HÃ m supervised contrastive loss cÅ©ng há»c cÃ¡c Ä‘áº·c trÆ°ng biá»ƒu diá»…n thÃ´ng qua contrastive loss nhÆ°ng cÃ³ sá»­ dá»¥ng thÃªm thÃ´ng tin tá»« nhÃ£n cá»§a dá»¯ liá»‡u Ä‘á»ƒ láº¥y máº«u positive (cÃ¡c bá»©c áº£nh cÃ¹ng lá»›p vá»›i anchor) ngoÃ i dá»¯ liá»‡u augmentation cá»§a 1 bá»©c áº£nh.


## 3. Tá»•ng káº¿t

- Máº·c dÃ¹ thoáº¡t nhÃ¬n, contrastive learning cÃ³ váº» dá»… dÃ ng nhÆ°ng viá»‡c khiáº¿n chÃºng há»c cÃ¡ch biá»ƒu diá»…n Ä‘áº§u vÃ o cÃ³ Ã½ nghÄ©a lÃ  Ä‘iá»u khÃ¡ khÃ³ khÄƒn. HÆ°á»›ng Ä‘i nÃ y cÅ©ng cÃ²n ráº¥t nhiá»u tiá»m nÄƒng Ä‘á»ƒ khai thÃ¡c, hi vá»ng chÃºng ta sáº½ tháº¥y nhiá»u bÃ i nghiÃªn cá»©u mang káº¿t quáº£ Ä‘á»™t phÃ¡ hÆ¡n ná»¯a trong tÆ°Æ¡ng lai.

## 4. TÃ i liá»‡u tham kháº£o

- [Tá»•ng quan vá» Contrastive Learning](https://viblo.asia/p/tong-quan-ve-contrastive-learning-Az45bRNq5xY)