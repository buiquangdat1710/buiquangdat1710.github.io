---
title: "Transformer Anatomy"
date: 2025-01-19 00:00:00  + 0800
categories: [Natural Language Processing with Transformers Book]
tags: [transfomers]
---
---


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>

- Trong blog "Text Classification", chÃºng ta Ä‘Ã£ tháº¥y nhá»¯ng gÃ¬ cáº§n thiáº¿t Ä‘á»ƒ fine-tune vÃ  Ä‘Ã¡nh giÃ¡ transformer. BÃ¢y giá» chÃºng ta hÃ£y xem cÃ¡ch chÃºng hoáº¡t Ä‘á»™ng cá»¥ thá»ƒ. Trong blog nÃ y, chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡c khá»‘i xÃ¢y dá»±ng chÃ­nh cá»§a cÃ¡c mÃ´ hÃ¬nh transformer vÃ  cÃ¡ch triá»ƒn khai chÃºng báº±ng PyTorch. ChÃºng tÃ´i cÅ©ng sáº½ cung cáº¥p hÆ°á»›ng dáº«n vá» cÃ¡ch thá»±c hiá»‡n tÆ°Æ¡ng tá»± trong TensorFlow. Äáº§u tiÃªn chÃºng ta sáº½ táº­p trung vÃ o viá»‡c xÃ¢y dá»±ng cÆ¡ cháº¿ chÃº Ã½, sau Ä‘Ã³ thÃªm cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t Ä‘á»ƒ lÃ m cho transforrmer encoder hoáº¡t Ä‘á»™ng. ChÃºng ta cÅ©ng sáº½ cÃ³ má»™t cÃ¡i nhÃ¬n ngáº¯n gá»n vá» sá»± khÃ¡c biá»‡t vá» kiáº¿n trÃºc giá»¯a cÃ¡c encoder vÃ  decoder. Äáº¿n cuá»‘i blog nÃ y, báº¡n sáº½ cÃ³ thá»ƒ tá»± mÃ¬nh thá»±c hiá»‡n má»™t mÃ´ hÃ¬nh transformer Ä‘Æ¡n giáº£n!

- Máº·c dÃ¹ hiá»ƒu biáº¿t ká»¹ thuáº­t sÃ¢u sáº¯c vá» kiáº¿n trÃºc Transformer thÆ°á»ng khÃ´ng cáº§n thiáº¿t Ä‘á»ƒ sá»­ dá»¥ng ğŸ¤— Transformers vÃ  fine-tune cÃ¡c mÃ´ hÃ¬nh cho trÆ°á»ng há»£p sá»­ dá»¥ng cá»§a báº¡n, nhÆ°ng nÃ³ cÃ³ thá»ƒ há»¯u Ã­ch Ä‘á»ƒ hiá»ƒu vÃ  Ä‘iá»u hÆ°á»›ng cÃ¡c háº¡n cháº¿ cá»§a Transformer vÃ  sá»­ dá»¥ng chÃºng trong cÃ¡c lÄ©nh vá»±c má»›i.
- Blog nÃ y cÅ©ng giá»›i thiá»‡u nhiá»u kiá»ƒu transformers Ä‘á»ƒ giÃºp báº¡n hiá»ƒu hÃ ng loáº¡t cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ xuáº¥t hiá»‡n trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y. TrÆ°á»›c khi Ä‘i sÃ¢u vÃ o code, hÃ£y báº¯t Ä‘áº§u vá»›i cÃ¡i nhÃ¬n tá»•ng quan vá» kiáº¿n trÃºc Ä‘áº§u tiÃªn cá»§a transformers - thá»© mÃ  Ä‘Ã£ khá»Ÿi Ä‘á»™ng cuá»™c cÃ¡ch máº¡ng vá» transformer.

## 1. Kiáº¿n trÃºc cá»§a Transformer

- NhÆ° chÃºng ta Ä‘Ã£ biáº¿t, Transformer ban Ä‘áº§u dá»±a trÃªn kiáº¿n trÃºc encoder-decoder Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho cÃ¡c tÃ¡c vá»¥ nhÆ° dá»‹ch mÃ¡y, trong Ä‘Ã³ má»™t chuá»—i tá»« Ä‘Æ°á»£c dá»‹ch tá»« ngÃ´n ngá»¯ nÃ y sang ngÃ´n ngá»¯ khÃ¡c. Kiáº¿n trÃºc nÃ y bao gá»“m hai thÃ nh pháº§n:

1. **Encoder**: Chuyá»ƒn Ä‘á»•i má»™t chuá»—i token Ä‘áº§u vÃ o thÃ nh má»™t chuá»—i embedding vectors, thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  hidden state hoáº·c context.
2. **Decoder**: Sá»­ dá»¥ng hidden state cá»§a encoder Ä‘á»ƒ táº¡o ra má»™t chuá»—i token ouput, má»—i token má»™t láº§n (tá»©c lÃ  láº§n lÆ°á»£t táº¡o ra tá»«ng token má»™t, khÃ´ng pháº£i song song).

- NhÆ° minh há»a trong HÃ¬nh 3-1, báº£n thÃ¢n bÃªn trong encoder vÃ  decoder bao gá»“m má»™t sá»‘ khá»‘i xÃ¢y dá»±ng.

![anh](./image/287.png)

- ChÃºng ta sáº½ xem xÃ©t chi tiáº¿t tá»«ng thÃ nh pháº§n sau, nhÆ°ng chÃºng ta cÃ³ thá»ƒ tháº¥y má»™t sá»‘ Ä‘iá»u trong HÃ¬nh 3-1 Ä‘áº·c trÆ°ng cho kiáº¿n trÃºc Transformer:
  - VÄƒn báº£n Ä‘áº§u vÃ o Ä‘Æ°á»£c tokenized vÃ  chuyá»ƒn Ä‘á»•i thÃ nh token embeddings báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t mÃ  chÃºng ta Ä‘Ã£ nÃ³i trong cÃ¡c blog khÃ¡c. VÃ¬ cÆ¡ cháº¿ chÃº Ã½ khÃ´ng nháº­n thá»©c Ä‘Æ°á»£c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c token, chÃºng ta cáº§n má»™t cÃ¡ch Ä‘á»ƒ Ä‘Æ°a thÃ´ng tin vá» vá»‹ trÃ­ token vÃ o Ä‘áº§u vÃ o Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a báº£n cháº¥t tuáº§n tá»± cá»§a vÄƒn báº£n. Do Ä‘Ã³, cÃ¡c token embeddings Ä‘Æ°á»£c káº¿t há»£p vá»›i positional embeddings chá»©a thÃ´ng tin vá»‹ trÃ­ cho má»—i token.
  - Encoder bao gá»“m má»™t chá»“ng cÃ¡c encoder stack hoáº·c gá»i lÃ  cÃ¡c khá»‘i (blocks), tÆ°Æ¡ng tá»± nhÆ° xáº¿p chá»“ng cÃ¡c lá»›p tÃ­ch cháº­p trong thá»‹ giÃ¡c mÃ¡y tÃ­nh. Äiá»u nÃ y cÅ©ng Ä‘Ãºng vá»›i decoder, cÃ³ cÃ¡c decoder stack.
  - Äáº§u ra cá»§a encoder Ä‘Æ°á»£c Ä‘Æ°a Ä‘áº¿n tá»«ng lá»›p encoder layer vÃ  sau Ä‘Ã³ decoder táº¡o ra dá»± Ä‘oÃ¡n cho token tiáº¿p theo cÃ³ kháº£ nÄƒng xáº£y ra nháº¥t trong chuá»—i. Äáº§u ra cá»§a bÆ°á»›c nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a trá»Ÿ láº¡i decoder Ä‘á»ƒ táº¡o token tiáº¿p theo, v.v. cho Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c token káº¿t thÃºc chuá»—i (EOS). Trong vÃ­ dá»¥ tá»« HÃ¬nh 3-1, hÃ£y tÆ°á»Ÿng tÆ°á»£ng decoder Ä‘Ã£ dá»± Ä‘oÃ¡n "Die" vÃ  "Zeit". BÃ¢y giá» nÃ³ sáº½ nháº­n hai cÃ¡i nÃ y lÃ m Ä‘áº§u vÃ o cÅ©ng nhÆ° táº¥t cáº£ cÃ¡c Ä‘áº§u ra cá»§a encoder Ä‘á»ƒ dá»± Ä‘oÃ¡n token tiáº¿p theo, "fliegt". Trong bÆ°á»›c tiáº¿p theo, decoder láº¡i nháº­n Ä‘Æ°á»£c "fliegt" nhÆ° má»™t Ä‘áº§u vÃ o bá»• sung. ChÃºng ta láº·p láº¡i quÃ¡ trÃ¬nh nÃ y cho Ä‘áº¿n khi decoder dá»± Ä‘oÃ¡n token EOS hoáº·c chÃºng ta Ä‘áº¡t Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a output.
- Kiáº¿n trÃºc Transformer ban Ä‘áº§u Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c tÃ¡c vá»¥ sequence-sequence nhÆ° dá»‹ch mÃ¡y, nhÆ°ng cáº£ khá»‘i encoder vÃ  decoder Ä‘á»u sá»›m Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thÃ nh cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c láº­p. Máº·c dÃ¹ cÃ³ hÃ ng trÄƒm kiáº¿n trÃºc transformer khÃ¡c nhau, nhÆ°ng háº§u háº¿t chÃºng thuá»™c má»™t trong ba loáº¡i:

1. **Encoder-only:** CÃ¡c mÃ´ hÃ¬nh nÃ y chuyá»ƒn Ä‘á»•i má»™t chuá»—i vÄƒn báº£n Ä‘áº§u vÃ o thÃ nh má»™t biá»ƒu diá»…n sá»‘ phong phÃº (vector embedding), ráº¥t phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n loáº¡i vÄƒn báº£n hoáº·c nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (NER). BERT vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³, nhÆ° RoBERTa vÃ  DistilBERT, thuá»™c loáº¡i kiáº¿n trÃºc nÃ y. Vector embedding Ä‘Æ°á»£c tÃ­nh toÃ¡n cho token nháº¥t Ä‘á»‹nh trong kiáº¿n trÃºc nÃ y phá»¥ thuá»™c vÃ o cáº£ bÃªn trÃ¡i (trÆ°á»›c token Ä‘Ã³) vÃ  bÃªn pháº£i (sau token Ä‘Ã³). Äiá»u nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  cÆ¡ cháº¿ chÃº Ã½ hai chiá»u (**bidirectional attention**).
2. **Decoder-only:** Cho má»™t Ä‘oáº¡n vÄƒn báº£n nhÆ° "Thanks for a lunch, I had a..." CÃ¡c mÃ´ hÃ¬nh nÃ y sáº½ tá»± Ä‘á»™ng hoÃ n thÃ nh cÃ¢u báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo cÃ³ kháº£ nÄƒng xáº£y ra nháº¥t. Há» mÃ´ hÃ¬nh GPT thuá»™c lá»›p nÃ y. Vector embeddings Ä‘Æ°á»£c tÃ­nh toÃ¡n cho má»™t token nháº¥t Ä‘á»‹nh trong kiáº¿n trÃºc nÃ y chá»‰ phá»¥ thuá»™c vÃ o ngá»¯ cáº£nh bÃªn trÃ¡i. Äiá»u nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  **causal** hoáº·c **autoregressive attention**.
3. **Encoder-Decoder:** ChÃºng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a Ã¡nh xáº¡ phá»©c táº¡p tá»« chuá»—i vÄƒn báº£n nÃ y sang chuá»—i vÄƒn báº£n khÃ¡c. ChÃºng phÃ¹ há»£p vá»›i cÃ¡c tÃ¡c vá»¥ dá»‹ch mÃ¡y vÃ  tÃ³m táº¯t. NgoÃ i kiáº¿n trÃºc Transformer, cÃ¡c mÃ´ hÃ¬nh BART vÃ  T5 thuá»™c lá»›p nÃ y.

> Thá»±c táº¿, sá»± phÃ¢n biá»‡t giá»¯a cÃ¡c á»©ng dá»¥ng dÃ nh cho kiáº¿n trÃºc decoder-only so vá»›i encoder-only cÃ³ pháº§n má» nháº¡t. VÃ­ dá»¥, cÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng decoder nhÆ° há» GPT cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¹y chá»‰nh Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ nhÆ° dá»‹ch thuáº­t, vá»‘n thÆ°á»ng Ä‘Æ°á»£c xem lÃ  cÃ¡c nhiá»‡m vá»¥ sequence-to-sequence. TÆ°Æ¡ng tá»±, cÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng encoder nhÆ° BERT cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c nhiá»‡m vá»¥ tÃ³m táº¯t, mÃ  thÃ´ng thÆ°á»ng gáº¯n liá»n vá»›i cÃ¡c mÃ´ hÃ¬nh encoder-decoder hoáº·c decoder-only ([paper](https://arxiv.org/abs/1908.08345)).

- BÃ¢y giá» báº¡n Ä‘Ã£ cÃ³ hiá»ƒu biáº¿t sÃ¢u sáº¯c vá» kiáº¿n trÃºc Transformer, chÃºng ta hÃ£y xem xÃ©t ká»¹ hÆ¡n hoáº¡t Ä‘á»™ng bÃªn trong cá»§a encoder.

### The Encoder

- NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, encoder cá»§a transformer bao gá»“m nhiá»u lá»›p encoder xáº¿p cáº¡nh nhau. NhÆ° minh há»a trong HÃ¬nh 3-2, má»—i lá»›p encoder nháº­n Ä‘Æ°á»£c chuá»—i embedding lÃ  Ä‘áº§u vÃ o vÃ  cung cáº¥p chÃºng qua cÃ¡c sublayers sau:
  - Má»™t lá»›p multi-head self-attention.
  - Má»™t lá»›p fully connected feed-forward Ä‘Æ°á»£c Ã¡p dá»¥ng cho má»—i input embedding.
- CÃ¡c output embedding cá»§a má»—i lá»›p encoder cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c vá»›i input vÃ  chÃºng ta sáº½ sá»›m tháº¥y ráº±ng vai trÃ² chÃ­nh cá»§a lá»›p encoder lÃ  "cáº­p nháº­t" cÃ¡c input embedding Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n mÃ£ hÃ³a má»™t sá»‘ thÃ´ng tin ngá»¯ cáº£nh trong cÃ¢u. VÃ­ dá»¥, tá»« "apple" sáº½ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ "company-like" hÆ¡n vÃ  Ã­t "fruit-like" hÆ¡n náº¿u cÃ¡c tá»« "keynote" hoáº·c "phone" cÃ³ trong cÃ¢u (NÃ³i chung lÃ , má»¥c Ä‘Ã­ch cá»§a cÃ¡c lá»›p encoder lÃ  nháº±m giÃ u thÃ´ng tin cá»§a embedding dá»±a vÃ o ngá»¯ cáº£nh trong cÃ¢u).

![anh](./image/288.png)

- CÃ¡c sublayers trong cÃ¡c lá»›p encoder cÅ©ng sá»­ dá»¥ng skip connections vÃ  layer normalization, Ä‘iá»u nÃ y giÃºp cho mÃ´ hÃ¬nh há»c sÃ¢u Ä‘Æ°á»£c train hiá»‡u quáº£ hÆ¡n. NhÆ°ng Ä‘á»ƒ thá»±c sá»± hiá»ƒu Ä‘iá»u gÃ¬ lÃ m cho transformer hoáº¡t Ä‘á»™ng, chÃºng ta pháº£i Ä‘i sÃ¢u hÆ¡n. HÃ£y báº¯t Ä‘áº§u vá»›i khá»‘i quan trá»ng nháº¥t: self-attention layer.

### Self-Attention

- NhÆ° chÃºng ta Ä‘Ã£ tháº£o luáº­n trong blog trÆ°á»›c, cÆ¡ cháº¿ attention lÃ  má»™t phÆ°Æ¡ng phÃ¡p cho phÃ©p máº¡ng nÆ¡-ron gÃ¡n má»™t má»©c Ä‘á»™ trá»ng sá»‘ khÃ¡c nhau, hay gá»i lÃ  â€œsá»± chÃº Ã½â€ cho tá»«ng pháº§n tá»­ trong má»™t chuá»—i. Äá»‘i vá»›i chuá»—i vÄƒn báº£n, cÃ¡c pháº§n tá»­ lÃ  cÃ¡c token embedding giá»‘ng nhÆ° nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ gáº·p trong blog trÆ°á»›c, nÆ¡i má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh má»™t vector cÃ³ sá»‘ chiá»u cá»‘ Ä‘á»‹nh. VÃ­ dá»¥, trong mÃ´ hÃ¬nh BERT, má»—i token Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t vector 768 chiá»u. Pháº§n â€œselfâ€ cá»§a self-attention Ã¡m chá»‰ ráº±ng cÃ¡c trá»ng sá»‘ nÃ y Ä‘Æ°á»£c tÃ­nh toÃ¡n cho táº¥t cáº£ hidden states trong cÃ¹ng má»™t táº­p há»£p â€” vÃ­ dá»¥, táº¥t cáº£ cÃ¡c hidden states cá»§a encoder. NgÆ°á»£c láº¡i, cÆ¡ cháº¿ attention trong cÃ¡c mÃ´ hÃ¬nh há»“i quy (recurrent models) liÃªn quan Ä‘áº¿n viá»‡c tÃ­nh toÃ¡n má»©c Ä‘á»™ liÃªn quan cá»§a má»—i hidden state cá»§a encoder Ä‘áº¿n hidden state cá»§a decoder táº¡i thá»i Ä‘iá»ƒm decoding.
- Ã tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau self-attention lÃ  thay vÃ¬ sá»­ dá»¥ng embedding cá»‘ Ä‘á»‹nh cho tá»«ng token, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng toÃ n bá»™ chuá»—i Ä‘á»ƒ tÃ­nh má»™t trung bÃ¬nh cÃ³ trá»ng sá»‘ (weighted average) cá»§a má»—i embedding. Má»™t cÃ¡ch khÃ¡c Ä‘á»ƒ diá»…n Ä‘áº¡t Ä‘iá»u nÃ y lÃ  vÃ­ dá»¥ nhÆ° ta cÃ³ má»—i chuá»—i cÃ¡c token embedding $x_1. \dots, x_n$, self-attention táº¡o ra má»™t chuá»—i token embedding má»›i $x_1', \dots, x_n'$ sao cho $x_i'$ lÃ  tá»• há»£p tuyáº¿n tÃ­nh cá»§a toÃ n bá»™ $x_j$:

$$
x_i' = \displaystyle \sum_{j=1}^{n}w_{ji}x_j
$$

- CÃ¡c há»‡ sá»‘ $w_{ji}$ Ä‘Æ°á»£c gá»i lÃ  trá»ng sá»‘ attention (attention weights) vÃ  Ä‘Æ°á»£c chuáº©n hÃ³a sao cho $\displaystyle \sum_{j=1}^{n}w_{ji} = 1$.  Äá»ƒ hiá»ƒu táº¡i sao viá»‡c tÃ­nh trung bÃ¬nh cÃ¡c token embedding láº¡i lÃ  má»™t Ã½ tÆ°á»Ÿng hay, hÃ£y xem xÃ©t vÃ­ dá»¥ khi báº¡n tháº¥y tá»« â€œfliesâ€. Báº¡n cÃ³ thá»ƒ nghÄ© Ä‘áº¿n con ruá»“i, nhÆ°ng náº¿u Ä‘Æ°á»£c cung cáº¥p thÃªm ngá»¯ cáº£nh, nhÆ° â€œtime flies like an arrow,â€ thÃ¬ báº¡n sáº½ nháº­n ra ráº±ng â€œfliesâ€ thá»±c ra lÃ  Ä‘á»™ng tá»«. TÆ°Æ¡ng tá»±, chÃºng ta cÃ³ thá»ƒ táº¡o má»™t biá»ƒu diá»…n cho tá»« â€œfliesâ€ mÃ  tÃ­ch há»£p ngá»¯ cáº£nh nÃ y báº±ng cÃ¡ch káº¿t há»£p táº¥t cáº£ cÃ¡c token embedding vá»›i cÃ¡c tá»· lá»‡ khÃ¡c nhau, cÃ³ thá»ƒ báº±ng cÃ¡ch gÃ¡n má»™t trá»ng sá»‘ lá»›n hÆ¡n cho token embedding cá»§a â€œtimeâ€ vÃ  â€œarrow.â€ CÃ¡c embedding Ä‘Æ°á»£c táº¡o ra theo cÃ¡ch nÃ y Ä‘Æ°á»£c gá»i lÃ  **embedding cÃ³ ngá»¯ cáº£nh hÃ³a (contextualized embeddings)** vÃ  Ä‘Ã£ xuáº¥t hiá»‡n trÆ°á»›c khi cÃ³ sá»± ra Ä‘á»i cá»§a cÃ¡c mÃ´ hÃ¬nh transformer trong xá»­ lÃ½ ngÃ´n ngá»¯, cháº³ng háº¡n nhÆ° ELMo ([paper](https://arxiv.org/abs/1802.05365)).
- Má»™t sÆ¡ Ä‘á»“ cá»§a quy trÃ¬nh nÃ y Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3-3, nÆ¡i chÃºng ta minh há»a hai cÃ¡ch biá»ƒu diá»…n khÃ¡c nhau cho tá»« â€œfliesâ€ dá»±a vÃ o ngá»¯ cáº£nh, nhá» self-attention.

- BÃ¢y giá» hÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n trá»ng sá»‘ chÃº Ã½ (attention weights).

#### Scaled dot-product attention

- CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ code self-attention layer, nhÆ°ng cÃ¡ch phá»• biáº¿n nháº¥t lÃ  scaled dot-product attention, tá»« bÃ i bÃ¡o giá»›i thiá»‡u kiáº¿n trÃºc [Transformer](https://arxiv.org/abs/1706.03762). CÃ³ bá»‘n bÆ°á»›c chÃ­nh cáº§n thiáº¿t Ä‘á»ƒ code cÃ¡i nÃ y:
1. Biáº¿n má»—i token embedding thÃ nh ba vectÆ¡ Ä‘Æ°á»£c gá»i lÃ  query, key vÃ  value.
2. TÃ­nh attention scores. ChÃºng ta xÃ¡c Ä‘á»‹nh má»©c "giá»‘ng nhau" cá»§a vector query vÃ  vector key báº±ng cÃ¡ch sá»­ dá»¥ng **similarity function**. ÄÃºng nhÆ° cÃ¡i tÃªn, similarity function cho scaled dot-product attention chá»‰ Ä‘Æ¡n giáº£n lÃ  dot-product, dot-product Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£ báº±ng cÃ¡ch sá»­ dá»¥ng phÃ©p nhÃ¢n ma tráº­n cá»§a embedding. Query vÃ  key náº¿u nhÆ° cÃ³ sá»± "giá»‘ng nhau" lá»›n thÃ¬ dot-product sáº½ lá»›n, ngÆ°á»£c láº¡i, náº¿u nhÆ° query vÃ  key mÃ  khÃ´ng cÃ³ sá»± "giá»‘ng nhau" thÃ¬ dot-product sáº½ bÃ©. Äáº§u ra cá»§a dot-product nÃ y Ä‘Æ°á»£c gá»i lÃ  attention scores, vÃ  náº¿u nhÆ° chuá»—i Ä‘áº§u vÃ o cÃ³ $n$ token thÃ¬ sáº½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i ma tráº­n $n \times n$ chá»©a cÃ¡c attention scores.
3. TÃ­nh toÃ¡n attention weights. Dot-product cÃ³ thá»ƒ  táº¡o ra sá»‘ ráº¥t lá»›n, Ä‘iá»u nÃ y lÃ m máº¥t Ä‘i tÃ­nh á»•n Ä‘á»‹nh cá»§a quÃ¡ trÃ¬nh train. Äá»ƒ xá»­ lÃ½ Ä‘iá»u nÃ y, attention scores Ä‘áº§u tiÃªn sáº½ Ä‘Æ°á»£c nhÃ¢n vá»›i há»‡ sá»‘ tá»· lá»‡ Ä‘á»ƒ chuáº©n hÃ³a phÆ°Æ¡ng sai, sau Ä‘Ã³ láº¡i tiáº¿p tá»¥c Ä‘Æ°á»£c chuáº©n hÃ³a báº±ng hÃ m softmax Ä‘á»ƒ Ä‘áº£m báº£o tá»•ng táº¥t cáº£ giÃ¡ trá»‹ báº±ng 1. Ma tráº­n $n \times n$ lÃºc nÃ y sáº½ chá»©a táº¥t cáº£ cÃ¡c attention weights, $w_{ji}$
4. Cáº­p nháº­p token embeddings. Khi attention weights Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh toÃ¡n xong, chÃºng ta nhÃ¢n nÃ³ vá»›i value vector $v_1,\dots,v_n$ Ä‘á»ƒ nháº­n Ä‘Æ°á»£c káº¿t quáº£ má»›i cá»§a embedding: 

$$
x_i' = \displaystyle \sum_{j=1}^{n}w_{ji}v_j
$$

- ChÃºng ta cÃ³ thá»ƒ hÃ¬nh dung cÃ¡ch tÃ­nh toÃ¡n attention weights vá»›i má»™t thÆ° viá»‡n tiá»‡n lá»£i cÃ³ tÃªn lÃ  `BertViz` cho Jupyter. ThÆ° viá»‡n nÃ y cung cáº¥p má»™t sá»‘ chá»©c nÄƒng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trá»±c quan hÃ³a cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau cá»§a attention trong cÃ¡c mÃ´ hÃ¬nh transformer. Äá»ƒ trá»±c quan hÃ³a attention weights, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng `neuron_view`, mÃ´-Ä‘un nÃ y theo dÃµi viá»‡c tÃ­nh toÃ¡n cÃ¡c trá»ng sá»‘ Ä‘á»ƒ cho tháº¥y cÃ¡ch query vÃ  key Ä‘Æ°á»£c káº¿t há»£p Ä‘á»ƒ táº¡o ra trá»ng sá»‘ cuá»‘i cÃ¹ng. VÃ¬ `BertViz` cáº§n khai thÃ¡c cÃ¡c attention layers cá»§a mÃ´ hÃ¬nh, chÃºng ta sáº½ khá»Ÿi táº¡o BERT checkpoint vá»›i lá»›p mÃ´ hÃ¬nh tá»« BertViz vÃ  sau Ä‘Ã³ sá»­ dá»¥ng hÃ m `show()` Ä‘á»ƒ táº¡o ra hÃ¬nh áº£nh tÆ°Æ¡ng tÃ¡c cho má»™t encoder layer cá»¥ thá»ƒ vÃ  attention head. LÆ°u Ã½ ráº±ng báº¡n cáº§n nháº¥p vÃ o dáº¥u "+" á»Ÿ bÃªn trÃ¡i Ä‘á»ƒ kÃ­ch hoáº¡t trá»±c quan hÃ³a attention:

```python
from transformers import AutoTokenizer
from bertviz.transformers_neuron_view import BertModel
from bertviz.neuron_view import show
model_ckpt = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BertModel.from_pretrained(model_ckpt)
text = "time flies like an arrow"
show(model, "bert", tokenizer, text, display_mode="light", layer=0, head=8)
```

![anh](./image/290.png)

- Tá»« hÃ¬nh áº£nh trá»±c quan, chÃºng ta cÃ³ thá»ƒ tháº¥y cÃ¡c giÃ¡ trá»‹ cá»§a vector query vÃ  vectÆ¡ key Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng dáº£i dá»c, trong Ä‘Ã³ cÆ°á»ng Ä‘á»™ cá»§a má»—i dáº£i tÆ°Æ¡ng á»©ng vá»›i Ä‘á»™ lá»›n. CÃ¡c Ä‘Æ°á»ng káº¿t ná»‘i lÃ  trá»ng sá»‘ theo attention giá»¯a cÃ¡c token vÃ  chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng vectÆ¡ query cá»§a token "flies" cÃ³ sá»± liÃªn quan nháº¥t vá»›i vectÆ¡c key cá»§a token "arrow".

> KhÃ¡i niá»‡m vá» vectÆ¡ query, key vÃ  value cÃ³ váº» hÆ¡i khÃ³ hiá»ƒu trong láº§n Ä‘áº§u tiÃªn báº¡n gáº·p chÃºng. TÃªn cá»§a chÃºng Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cÃ¡c há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin, nhÆ°ng chÃºng ta cÃ³ thá»ƒ thÃºc Ä‘áº©y Ã½ nghÄ©a cá»§a chÃºng báº±ng má»™t phÃ©p so sÃ¡nh Ä‘Æ¡n giáº£n. HÃ£y tÆ°á»Ÿng tÆ°á»£ng ráº±ng báº¡n Ä‘ang á»Ÿ siÃªu thá»‹ mua táº¥t cáº£ cÃ¡c nguyÃªn liá»‡u báº¡n cáº§n cho bá»¯a tá»‘i cá»§a mÃ¬nh. Báº¡n cÃ³ cÃ´ng thá»©c cá»§a mÃ³n Äƒn vÃ  má»—i thÃ nh pháº§n cáº§n thiáº¿t cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t query. Khi báº¡n xem qua cÃ¡c ká»‡ chá»©a nguyÃªn liá»‡u, báº¡n nhÃ¬n vÃ o nhÃ£n (key) vÃ  kiá»ƒm tra xem chÃºng cÃ³ khá»›p vá»›i má»™t thÃ nh pháº§n trong danh sÃ¡ch cá»§a báº¡n hay khÃ´ng (similarity function). Náº¿u báº¡n cÃ³ má»™t sáº£n pháº©m trÃ¹ng khá»›p, báº¡n sáº½ láº¥y máº·t hÃ ng (value) tá»« ká»‡. Trong phÃ©p so sÃ¡nh nÃ y, báº¡n chá»‰ nháº­n Ä‘Æ°á»£c má»™t máº·t hÃ ng táº¡p hÃ³a cho má»—i nhÃ£n phÃ¹ há»£p vá»›i thÃ nh pháº§n. Self-attention lÃ  má»™t phiÃªn báº£n trá»«u tÆ°á»£ng vÃ  "mÆ°á»£t mÃ " hÆ¡n cá»§a Ä‘iá»u nÃ y: má»i nhÃ£n trong siÃªu thá»‹ khá»›p vá»›i thÃ nh pháº§n á»Ÿ má»©c Ä‘á»™ mÃ  má»—i key phÃ¹ há»£p vá»›i query. VÃ¬ váº­y, náº¿u danh sÃ¡ch cá»§a báº¡n bao gá»“m má»™t tÃ¡ quáº£ trá»©ng, thÃ¬ báº¡n cÃ³ thá»ƒ láº¥y Ä‘Æ°á»£c 10 quáº£ trá»©ng, má»™t quáº£ trá»©ng trÃ¡ng vÃ  má»™t cÃ¡nh gÃ .

- ChÃºng ta hÃ£y xem xÃ©t quÃ¡ trÃ¬nh nÃ y chi tiáº¿t hÆ¡n báº±ng cÃ¡ch triá»ƒn khai sÆ¡ Ä‘á»“ cÃ¡c phÃ©p toÃ¡n Ä‘á»ƒ tÃ­nh toÃ¡n scaled dot-product attention, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 3-4.

![anh](./image/291.png)


- ChÃºng ta sáº½ sá»­ dá»¥ng PyTorch Ä‘á»ƒ triá»ƒn khai kiáº¿n trÃºc Transformer trong blog nÃ y, nhÆ°ng cÃ¡c bÆ°á»›c trong TensorFlow lÃ  tÆ°Æ¡ng tá»±. ChÃºng ta sáº½ nhÃ¬n qua cÃ¡c chá»©c nÄƒng giá»‘ng nhau trong hai framewwork á»Ÿ Báº£ng 3-1.

![anh](./image/292.png)

- Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  tokenize vÄƒn báº£n, vÃ¬ váº­y hÃ£y sá»­ dá»¥ng `tokenizer` cá»§a chÃºng ta Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c input IDs:

```python
inputs = tokenizer(text, return_tensors = "pt", add_special_tokens = False)
inputs.input_ids
# tensor([[ 2051, 10029,  2066,  2019,  8612]])
```


- NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong blog "Text Classification", má»—i token trong cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n má»™t ID duy nháº¥t trong tokenizer's vocabulary. Äá»ƒ Ä‘Æ¡n giáº£n, chÃºng ta loáº¡i trá»« token Ä‘áº·c biá»‡t [CLS] vÃ  [SEP] báº±ng cÃ¡ch Ä‘áº·t `add_special_tokens=False`. Tiáº¿p theo, chÃºng ta cáº§n táº¡o má»™t sá»‘ embeddings dÃ y Ä‘áº·c. "DÃ y Ä‘áº·c" trong ngá»¯ cáº£nh nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c giÃ¡ trá»‹ trong embedding lÃ  khÃ¡c 0. NgÆ°á»£c láº¡i, cÃ¡c one-hot embedding mÃ  chÃºng ta tháº¥y trong blog "Text Classification" lÃ  thÆ°a thá»›t, vÃ¬ táº¥t cáº£ Ä‘á»u cÃ³ giÃ¡ trá»‹ lÃ  0 trá»« má»™t pháº§n tá»­ (Hiá»ƒu Ä‘Æ¡n giáº£n, vecotr dÃ y Ä‘áº·c cÃ³ nghÄ©a lÃ  Ã­t pháº§n tá»­ báº±ng 0, vector thÆ°a thá»›t lÃ  cÃ³ nhiá»u pháº§n tá»« báº±ng 0). Trong PyTorch, chÃºng ta cÃ³ thá»ƒ lÃ m Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng lá»›p `torch.nn.Embedding` hoáº¡t Ä‘á»™ng nhÆ° má»™t báº£ng tra cá»©u cho má»—i ID Ä‘áº§u vÃ o:

```python
from torch import nn
from transformers import AutoConfig
config = AutoConfig.from_pretrained(model_ckpt)
token_emb = nn.Embedding(config.vocab_size, config.hidden_size)
token_emb
```

- á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ sá»­ dá»¥ng lá»›p `AutoConfig` Ä‘á»ƒ táº£i tá»‡p `config.json` Ä‘Æ°á»£c liÃªn káº¿t vá»›i `bert-base-uncased` checkpoint. Trong ğŸ¤— Transformers, má»—i checkpoint Ä‘Æ°á»£c gÃ¡n má»™t tá»‡p cáº¥u hÃ¬nh chá»‰ Ä‘á»‹nh cÃ¡c siÃªu tham sá»‘ khÃ¡c nhau nhÆ°  `vocab_size` vÃ  `hidden_size`, trong vÃ­ dá»¥ trÃªn chÃºng ta tháº¥y ráº±ng má»—i ID Ä‘áº§u vÃ o sáº½ Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n má»™t trong 30.522 vectÆ¡ embedding Ä‘Æ°á»£c lÆ°u trá»¯ trong `nn.Embedding`, má»—i vector cÃ³ kÃ­ch thÆ°á»›c 768. Lá»›p `AutoConfig` cÅ©ng lÆ°u trá»¯ metadata bá»• sung, cháº³ng háº¡n nhÆ° tÃªn nhÃ£n, Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘á»‹nh dáº¡ng dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh.
- LÆ°u Ã½ ráº±ng cÃ¡c token embeddings táº¡i thá»i Ä‘iá»ƒm nÃ y Ä‘á»™c láº­p vá»›i ngá»¯ cáº£nh cá»§a chÃºng. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c tá»« Ä‘á»“ng Ã¢m (cÃ¡c tá»« cÃ³ cÃ¹ng chÃ­nh táº£ nhÆ°ng cÃ³ nghÄ©a khÃ¡c nhau), nhÆ° "flies" trong vÃ­ dá»¥ trÆ°á»›c, cÃ³ cÃ¹ng biá»ƒu diá»…n trong cÃ¡c cÃ¢u khÃ¡c nhau. Táº¡i vÃ¬ chÃºng ta má»›i chá»‰ Ä‘ang gá»i `nn.Embedding` chá»© chÆ°a train gÃ¬ (cÃ¡c trá»ng sá»‘ cá»§a `nn.Embedding` Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn) nÃªn cÃ¡c vector hay cÃ¡c trá»ng sá»‘ trong `nn.Embedding` chÆ°a Ä‘Æ°á»£c há»c gÃ¬, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n lÃ  náº¿u báº¡n embedding tá»« "flies" trong cÃ¢u "time flies like an arrow" vÃ  trong cÃ¢u "fruit flies like a banana" thÃ¬ vector embedding giá»‘ng há»‡t nhau. Vai trÃ² cá»§a cÃ¡c attention layer sáº½ lÃ  káº¿t há»£p cÃ¡c token embedding nÃ y Ä‘á»ƒ phÃ¢n biá»‡t vÃ  thÃ´ng bÃ¡o cho sá»± Ä‘áº¡i diá»‡n cá»§a tá»«ng token vá»›i ná»™i dung ngá»¯ cáº£nh cá»§a cÃ¢u.

- BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ báº£ng tra cá»©u vector embedding, chÃºng ta cÃ³ thá»ƒ táº¡o cÃ¡c embedding báº±ng cÃ¡ch cung cáº¥p cÃ¡c input ID:


```python
inputs_embeds = token_emb(inputs.input_ids)
inputs_embeds.size() # torch.Size([1, 5, 768])
```


> ğŸ’¡ **Fact:** `nn.Embedding` gáº§n giá»‘ng vá»›i `nn.Linear` nÃªn báº¡n chá»‰ cáº§n hiá»ƒu Ä‘Æ¡n giáº£n `nn.Embedding` lÃ  má»™t ma tráº­n, khi cho má»™t Ä‘áº§u vÃ o x qua `nn.Embedding` thÃ¬ chÃºng ta Ä‘ang thá»±c hiá»‡n phÃ©p nhÃ¢n x*(Ma tráº­n trá»ng sá»‘ bÃªn trong `nn.Embedding`). Xem vá» sá»± khÃ¡c biáº¿t cá»§a `nn.Embedding` vÃ  `nn.Linear` [táº¡i Ä‘Ã¢y](https://discuss.pytorch.org/t/whats-the-difference-between-nn-embedding-and-nn-linear/46426).

- Äiá»u nÃ y Ä‘Ã£ cho chÃºng ta má»™t tensor cÃ³ shape lÃ  [batch_size, seq_len, hidden_dim], giá»‘ng nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong blog "Text Classification". ChÃºng ta sáº½ táº¡m thá»i bá» qua positional encoding, bÆ°á»›c tiáº¿p theo sáº½ lÃ  táº¡o vectÆ¡ query, key, value vÃ  tÃ­nh attention score báº±ng cÃ¡ch dÃ¹ng similarity function lÃ  dot product:

```python
import torch 
from math import sqrt
query = key = value = inputs_embeds # Q,K,V
dim_k = key.size(-1)
scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k) # Q*K^T / sqrt(dim_k)
scores.size() # torch.Size([1, 5, 5])
```

- Äiá»u nÃ y Ä‘Ã£ táº¡o ra ma tráº­n $5 \times 5$ attention scores cho má»™t sample trong batch. Sau nÃ y chÃºng ta sáº½ tháº¥y ráº±ng cÃ¡c ma tráº­n query, key, value Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch nhÃ¢n ma tráº­n trá»ng lÆ°á»£ng $W_{Q,K,V}$ cho cÃ¡c embedding, nhÆ°ng hiá»‡n táº¡i chÃºng ta Ä‘á»ƒ ba ma tráº­n báº±ng nhau cho Ä‘Æ¡n giáº£n. Trong scaled dot-product attention, dot-product Ä‘Æ°á»£c chia cho kÃ­ch thÆ°á»›c vectÆ¡ embedding Ä‘á»ƒ chÃºng ta khÃ´ng nháº­n Ä‘Æ°á»£c quÃ¡ nhiá»u sá»‘ lá»›n trong quÃ¡ trÃ¬nh train, cÃ¡c sá»‘ lá»›n cÃ³ thá»ƒ gÃ¢y ra softmax sá»± bÃ£o hÃ²a.
- HÃ£y Ã¡p dá»¥ng hÃ m softmax nÃ o:

```python
import torch.nn.functional as F
weights = F.softmax(scores, dim=-1)
weights.sum(dim=-1) 
# tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)
``` 

- BÆ°á»›c cuá»‘i cÃ¹ng lÃ  nhÃ¢n attention scores vá»›i ma tráº­n values:

```python
attn_outputs = torch.bmm(weights, value)
attn_outputs.shape # torch.Size([1, 5, 768])
```

- VÃ  tháº¿ lÃ  xong - chÃºng ta Ä‘Ã£ tráº£i qua táº¥t cáº£ cÃ¡c bÆ°á»›c Ä‘á»ƒ thá»±c hiá»‡n má»™t hÃ¬nh thá»©c Ä‘Æ¡n giáº£n hÃ³a cá»§a self-attention! LÆ°u Ã½ ráº±ng toÃ n bá»™ quÃ¡ trÃ¬nh chá»‰ lÃ  hai phÃ©p nhÃ¢n ma tráº­n vÃ  softmax, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ coi self-attention chá»‰ lÃ  má»™t cÃ¡ch ráº¥t ngáº§u Ä‘á»ƒ tÃ­nh trung bÃ¬nh. HÃ£y gÃ³i cÃ¡c bÆ°á»›c nÃ y thÃ nh má»™t hÃ m mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng sau nÃ y:

```python
def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    weights = F.softmax(scores, dim=-1)
    return torch.bmm(weights, value)
```

- CÆ¡ cháº¿ attention vá»›i cÃ¡c ma tráº­n query vÃ  key giá»‘ng nhau sáº½ gÃ¡n má»™t giÃ¡ trá»‹ ráº¥t lá»›n cho cÃ¡c tá»« giá»‘ng nhau trong ngá»¯ cáº£nh, Ä‘áº·c biá»‡t lÃ  vá»›i chÃ­nh tá»« hiá»‡n táº¡i: tÃ­ch vÃ´ hÆ°á»›ng cá»§a má»™t query vá»›i chÃ­nh nÃ³ luÃ´n báº±ng 1. NhÆ°ng trong thá»±c táº¿, Ã½ nghÄ©a cá»§a má»™t tá»« sáº½ Ä‘Æ°á»£c hiá»ƒu rÃµ hÆ¡n thÃ´ng qua cÃ¡c tá»« bá»• sung trong ngá»¯ cáº£nh hÆ¡n lÃ  bá»Ÿi cÃ¡c tá»« giá»‘ng há»‡t nhau â€” vÃ­ dá»¥, Ã½ nghÄ©a cá»§a tá»« "flies" (bay/con ruá»“i) Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tá»‘t hÆ¡n báº±ng cÃ¡ch káº¿t há»£p thÃ´ng tin tá»« "time" (thá»i gian) vÃ  "arrow" (mÅ©i tÃªn) thay vÃ¬ tá»« má»™t láº§n xuáº¥t hiá»‡n khÃ¡c cá»§a tá»« "flies". LÃ m tháº¿ nÃ o Ä‘á»ƒ chÃºng ta thÃºc Ä‘áº©y hÃ nh vi nÃ y?

- HÃ£y Ä‘á»ƒ mÃ´ hÃ¬nh táº¡o ra má»™t ma tráº­n khÃ¡c nhau cho query, key, vÃ  value báº±ng cÃ¡ch sá»­ dá»¥ng ba phÃ©p chiáº¿u tuyáº¿n tÃ­nh khÃ¡c nhau Ä‘á»ƒ chiáº¿u vector token ban Ä‘áº§u cá»§a chÃºng ta vÃ o ba khÃ´ng gian khÃ¡c nhau.

### Multi-headed attention

- Trong vÃ­ dá»¥ Ä‘Æ¡n giáº£n, chÃºng ta chá»‰ sá»­ dá»¥ng cÃ¡c embeddings "nhÆ° nÃ³ vá»‘n cÃ³" Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘iá»ƒm sá»‘ vÃ  trá»ng sá»‘ chÃº Ã½ (attention scores and weights). NhÆ°ng thá»±c táº¿ khÃ´ng chá»‰ cÃ³ váº­y. Trong lá»›p self-attention, má»—i embedding Ä‘Æ°á»£c Ã¡p dá»¥ng ba phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh riÃªng biá»‡t Ä‘á»ƒ táº¡o ra cÃ¡c vector query, key, vÃ  value. Nhá»¯ng phÃ©p biáº¿n Ä‘á»•i nÃ y giÃºp mÃ´ hÃ¬nh hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¡c tá»« trong cÃ¢u vÃ  táº­p trung vÃ o nhá»¯ng má»‘i liÃªn há»‡ quan trá»ng giá»¯a chÃºng.

- NgoÃ i ra, viá»‡c cÃ³ nhiá»u táº­p há»£p cÃ¡c phÃ©p biáº¿n Ä‘á»•i gá»i lÃ  "Ä‘áº§u chÃº Ã½" (attention heads) cÅ©ng ráº¥t há»¯u Ã­ch. Má»—i attention head cÃ³ thá»ƒ táº­p trung vÃ o má»™t khÃ­a cáº¡nh khÃ¡c nhau cá»§a cÃ¢u. VÃ­ dá»¥, má»™t head cÃ³ thá»ƒ táº­p trung vÃ o má»‘i quan há»‡ giá»¯a chá»§ ngá»¯ vÃ  Ä‘á»™ng tá»«, trong khi má»™t head khÃ¡c chÃº Ã½ Ä‘áº¿n nhá»¯ng tá»« mÃ´ táº£ gáº§n Ä‘Ã³ nhÆ° tÃ­nh tá»«. Nhá»¯ng má»‘i quan há»‡ nÃ y khÃ´ng pháº£i do con ngÆ°á»i láº­p trÃ¬nh thá»§ cÃ´ng, mÃ  Ä‘Æ°á»£c mÃ´ hÃ¬nh tá»± há»c tá»« dá»¯ liá»‡u. Náº¿u báº¡n quen vá»›i cÃ¡c mÃ´ hÃ¬nh nháº­n diá»‡n hÃ¬nh áº£nh, Ä‘iá»u nÃ y giá»‘ng nhÆ° cÃ¡c bá»™ lá»c (filters) trong máº¡ng nÆ¡-ron tÃ­ch cháº­p (convolutional neural networks). Má»™t bá»™ lá»c cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ nháº­n diá»‡n khuÃ´n máº·t, trong khi bá»™ lá»c khÃ¡c tÃ¬m bÃ¡nh xe cá»§a xe. CÃ¡c attention heads hoáº¡t Ä‘á»™ng tÆ°Æ¡ng tá»± nhÆ° váº­y nhÆ°ng Ã¡p dá»¥ng cho ngá»¯ nghÄ©a trong cÃ¢u.

![anh](./image/293.png)

- Giá» hÃ£y thá»­ code class `AttentionHead` nhÆ° sau:

```python
class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        self.q = nn.Linear(embed_dim, head_dim)
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)
    def forward(self, hidden_state):
        attn_ouputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))
        return attn_ouputs

```

- á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ khá»Ÿi táº¡o 3 `nn.Linear` Ä‘á»™c láº­p nhau mÃ  khi cho embedding vectors qua `nn.Linear` thÃ¬ cÃ³ nghÄ©a lÃ  chÃºng ta Ä‘ang thá»±c hiá»‡n phÃ©p nhÃ¢n ma tráº­n, Ä‘áº§u ra cá»§a `nn.Linear` sáº½ cÃ³ kÃ­ch cá»¡ lÃ  [`batch_size`, `seq_len`, `head_dim`], trong Ä‘Ã³ `head_dim` lÃ  sá»‘ chiá»u mÃ  chÃºng ta muá»‘n á»Ÿ Ä‘áº§u ra. Máº·c dÃ¹ `head_dim` khÃ´ng nháº¥t thiáº¿t pháº£i nhÆ¡ hÆ¡n `embed_dim` nhÆ°ng trong thá»±c tháº¿ ngÆ°á»i ta thÆ°á»ng chá»n `head_dim` báº±ng cÃ¡ch láº¥y `embed_dim` chia cho sá»‘ heads. VÃ­ dá»¥, mÃ´ hÃ¬nh BERT cÃ³ 12 attentons heads, nÃªn ngÆ°á»i ta Ä‘Ã£ chá»n `head_dim` báº±ng `embed_dim` / `num_head` tÆ°Æ¡ng Ä‘Æ°á»ng vá»›i $\frac{768}{12} = 64$. 

- Äá»ƒ cho má»i thá»© dá»… hiá»ƒu hÆ¡n, chÃºng ta sáº½ xem xÃ©t láº¡i tá»« Ä‘áº§u. Giáº£ sá»­ Ä‘áº§u vÃ o lÃ  má»™t cÃ¢u vÄƒn báº£n: "time flies like an arrow". Sau khi Ä‘Æ°á»£c vÄƒn báº£n Ä‘Æ°á»£c tokenizer thÃ¬ ta cÃ³ input_ids báº±ng `tensor([[ 2051, 10029,  2066,  2019,  8612]])`, sau Ä‘Ã³ input_ids sáº½ Ä‘Æ°á»£c embedding thÃ nh [1, 5, 768], gá»i lÃ  `inputs_embeds`, tá»©c lÃ  1 token sáº½ Ä‘Æ°á»£c embedding thÃ nh vector cÃ³ 768 chiá»u. Äáº§u vÃ o cá»§a lá»›p `AttentionHead` sáº½ lÃ  `inputs_embeds`. Cá»¥ thá»ƒ, kiáº¿n trÃºc cá»§a `AttentionHead` sáº½ nhÆ° sau:
  - **Äáº§u vÃ o:** $[1,5,768]$ (`inputs_embeds`)
  - $Q, K, V$ Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch cho $[1, 5, 768]$ qua táº§ng Linear cÃ³ kÃ­ch thÆ°á»›c lÃ  [embed_dim, head_dim], vÃ­ dá»¥ lÃ  $(768, 64)$.
  - Khi Ä‘Ã³ $Q,K,V  = [1, 5, 64]$, lÆ°u Ã½ lÃ  theo paper gá»‘c "Attention for all you need" thÃ¬ chiá»u cuá»‘i cÃ¹ng cá»§a $V$ cÃ³ thá»ƒ khÃ¡c bá»n $Q,K$. NhÆ°ng $Q,K$ chiá»u pháº£i giá»‘ng nhau.
  - TÃ­nh $\text{Attention} = \text{Softmax}(\frac{Q.K^T}{\sqrt{d}}).V = [1 , 5, 64]$ (lÆ°u Ã½ lÃ  $d$ chÃ­nh lÃ  chiá»u cuá»‘i cÃ¹ng cá»§a ma tráº­n $Q$, tá»©c lÃ  $d = 64$ trong vÃ­ dá»¥ nÃ y)
  - **Äáº§u ra:** $[1,5,64]$

- Code trÃªn chá»‰ lÃ  má»™t attention head, class `MultiHeadAttention` sáº½ concat táº¥t cáº£ ouput cá»§a má»—i attention head láº¡i vá»›i nhau táº¡o thÃ nh má»™t ma tráº­n, vÃ  cho ma tráº­n áº¥y qua má»™t lá»›p `nn.Linear`:

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList(
            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]
        )
        self.output_linear = nn.Linear(embed_dim, embed_dim)
    def forward(self, hidden_state):
        x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)
        x = self.output_linear(x)
        return x
```

- Kiáº¿n trÃºc cá»§a `MultiHeadAttention` hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° sau: 
  - **Äáº§u vÃ o:** $[1,5,768]$ (`inputs_embeds`).
  - Ta sáº½ dÃ¹ng 12 lá»›p `AttentionHead`. Báº£n cháº¥t `head_dim` báº±ng $64$ vÃ¬ $\frac{768}{12} = 64$ (Khi thiáº¿t káº¿ kiáº¿n trÃºc, ngÆ°á»i ta thÆ°á»ng sáº½ chá»n xem kiáº¿n trÃºc cÃ³ bao nhiÃªu lá»›p attention head, xong Ä‘Ã³ má»›i tÃ­nh Ä‘áº¿n `head_dim`. Tá»©c lÃ  ngÆ°á»i ta chá»n $12$ lá»›p `AttentionHead` sau Ä‘Ã³ má»›i tÃ­nh Ä‘áº¿n sá»‘ chiá»u cá»§a má»—i head). Sau Ä‘Ã³, ta sáº½ tÃ­nh Ä‘áº§u ra cá»§a $12$ lá»›p `AttentionHead` xong rá»“i concat láº¡i theo chiá»u ngang Ä‘á»ƒ táº¡o ra $[1,5,64*12]$ = $[1,5,768]$. Sau Ä‘Ã³ Ä‘Æ°a qua táº§ng `nn.Linear` cÃ³ kÃ­ch cá»¡ $[768,768]$, Ä‘áº§u ra thÃ nh $[1,5,768]$.
  - **Äáº§u ra:** $[1,5,768]$.

- Äá»ƒ chá»©ng minh Ä‘áº§u ra cá»§a lá»›p `MultiHeadAttention` cÃ³ Ä‘Ãºng nhÆ° trÃªn khÃ´ng, chÃºng ta hÃ£y thá»­ code nhÆ° sau:

```python
multihead_attn = MultiHeadAttention(config)
attn_output = multihead_attn(inputs_embeds)
attn_output.size() 
# torch.Size([1, 5, 768])
```

- NÃ³ Ä‘Ã£ hoáº¡t Ä‘á»™ng! Äá»ƒ káº¿t thÃºc pháº§n nÃ y vá» attention, hÃ£y sá»­ dá»¥ng `BertViz` má»™t láº§n ná»¯a Ä‘á»ƒ trá»±c quan hÃ³a attention Ä‘á»‘i vá»›i hai cÃ¡ch sá»­ dá»¥ng khÃ¡c nhau cá»§a tá»« "flies". á» Ä‘Ã¢y chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m `head_view()` tá»« `BertViz` báº±ng cÃ¡ch tÃ­nh toÃ¡n attenion cá»§a pretrained checkpoint:

```python
from bertviz import head_view
from transformers import AutoModel
model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)
sentence_a = "time flies like an arrow"
sentence_b = "fruit flies like a banana"
viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')
attention = model(**viz_inputs).attentions
sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)
tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])
head_view(attention, tokens, sentence_b_start, heads=[8])
```

![anh](./image/294.png)

- HÃ¬nh minh há»a nÃ y cho tháº¥y cÃ¡c trá»ng sá»‘ chÃº Ã½ (attention weights) Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng cÃ¡c Ä‘Æ°á»ng ná»‘i giá»¯a cÃ¡c token mÃ  embedding cá»§a chÃºng Ä‘ang Ä‘Æ°á»£c cáº­p nháº­t (bÃªn trÃ¡i) vá»›i tá»«ng tá»« mÃ  nÃ³ Ä‘ang táº­p trung vÃ o (bÃªn pháº£i). Äá»™ Ä‘áº­m nháº¡t cá»§a cÃ¡c Ä‘Æ°á»ng thá»ƒ hiá»‡n Ä‘á»™ máº¡nh yáº¿u cá»§a trá»ng sá»‘ chÃº Ã½, vá»›i cÃ¡c Ä‘Æ°á»ng Ä‘áº­m Ä‘áº¡i diá»‡n cho giÃ¡ trá»‹ gáº§n 1 vÃ  cÃ¡c Ä‘Æ°á»ng má» Ä‘áº¡i diá»‡n cho giÃ¡ trá»‹ gáº§n 0.

- Trong vÃ­ dá»¥ nÃ y, Ä‘áº§u vÃ o bao gá»“m hai cÃ¢u vÃ  cÃ¡c token [CLS] vÃ  [SEP] lÃ  cÃ¡c special tokens trong tokenizer cá»§a BERT mÃ  chÃºng ta Ä‘Ã£ gáº·p á»Ÿ blog trÆ°á»›c. Má»™t Ä‘iá»u cÃ³ thá»ƒ tháº¥y tá»« hÃ¬nh minh há»a lÃ  cÃ¡c trá»ng sá»‘ chÃº Ã½ máº¡nh nháº¥t giá»¯a cÃ¡c tá»« thuá»™c cÃ¹ng má»™t cÃ¢u, Ä‘iá»u nÃ y cho tháº¥y BERT cÃ³ thá»ƒ nháº­n biáº¿t ráº±ng nÃ³ cáº§n táº­p trung vÃ o cÃ¡c tá»« trong cÃ¹ng má»™t cÃ¢u. Tuy nhiÃªn, Ä‘á»‘i vá»›i tá»« â€œflies,â€ chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng BERT Ä‘Ã£ xÃ¡c Ä‘á»‹nh â€œarrowâ€ lÃ  quan trá»ng trong cÃ¢u Ä‘áº§u tiÃªn, vÃ  â€œfruitâ€ cÅ©ng nhÆ° â€œbananaâ€ trong cÃ¢u thá»© hai. CÃ¡c trá»ng sá»‘ chÃº Ã½ nÃ y cho phÃ©p mÃ´ hÃ¬nh phÃ¢n biá»‡t viá»‡c sá»­ dá»¥ng tá»« â€œfliesâ€ nhÆ° má»™t Ä‘á»™ng tá»« hay danh tá»«, tÃ¹y thuá»™c vÃ o ngá»¯ cáº£nh mÃ  nÃ³ xuáº¥t hiá»‡n!

- BÃ¢y giá», khi Ä‘Ã£ tÃ¬m hiá»ƒu xong vá» attention, hÃ£y cÃ¹ng xem cÃ¡ch triá»ƒn khai pháº§n cÃ²n thiáº¿u cá»§a lá»›p encoder: position-wise feed-forward networks.

### The Feed-Forward Layer

- Lá»›p feed-forward trong encoder vÃ  decoder thá»±c cháº¥t chá»‰ lÃ  má»™t máº¡ng neural fully connected hai lá»›p Ä‘Æ¡n giáº£n, nhÆ°ng cÃ³ má»™t sá»± khÃ¡c biá»‡t: thay vÃ¬ xá»­ lÃ½ toÃ n bá»™ chuá»—i embedding nhÆ° má»™t vector duy nháº¥t, nÃ³ xá»­ lÃ½ tá»«ng embedding Ä‘á»™c láº­p. VÃ¬ lÃ½ do nÃ y, lá»›p nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  position-wise feed-forward layer (lá»›p feed-forward theo vá»‹ trÃ­). Báº¡n cÅ©ng cÃ³ thá»ƒ tháº¥y nÃ³ Ä‘Æ°á»£c gá»i lÃ  má»™t tÃ­ch cháº­p má»™t chiá»u (one-dimensional convolution) vá»›i kÃ­ch thÆ°á»›c kernel báº±ng má»™t, thÆ°á»ng Ä‘Æ°á»£c nhá»¯ng ngÆ°á»i lÃ m viá»‡c trong lÄ©nh vá»±c thá»‹ giÃ¡c mÃ¡y tÃ­nh sá»­ dá»¥ng (vÃ­ dá»¥, mÃ£ nguá»“n cá»§a OpenAI GPT sá»­ dá»¥ng thuáº­t ngá»¯ nÃ y).

- Má»™t trick lÃ  `intermediate_size` thÆ°á»ng gáº¥p bá»‘n láº§n kÃ­ch thÆ°á»›c cá»§a `hidden_size`, vÃ  hÃ m kÃ­ch hoáº¡t GELU thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng. ÄÃ¢y lÃ  nÆ¡i mÃ  háº§u háº¿t kháº£ nÄƒng vÃ  trÃ­ nhá»› cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c coi lÃ  diá»…n ra, vÃ  nÃ³ cÅ©ng lÃ  pháº§n thÆ°á»ng Ä‘Æ°á»£c má»Ÿ rá»™ng khi tÄƒng kÃ­ch thÆ°á»›c cá»§a cÃ¡c mÃ´ hÃ¬nh. ChÃºng ta cÃ³ thá»ƒ triá»ƒn khai Ä‘iá»u nÃ y nhÆ° má»™t `nn.Module` Ä‘Æ¡n giáº£n nhÆ° sau:

```python
class FeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
    def forward(self,x):
        x = self.linear_1(x)
        x = self.gelu(x)
        x = self.linear_2(x)
        x = self.dropout(x)
        return x

```

- Kiáº¿n trÃºc `FeedForward` hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° sau:
  - **Äáº§u vÃ o:** $[1,5,768]$
  - ÄÆ°a Ä‘áº§u vÃ o qua `nn.Linear` cÃ³ kÃ­ch cá»¡ lÃ  $(768, 3072)$ (tÃ¹y kiáº¿n trÃºc mÃ  sá»‘ $3072$ lÃ  khÃ¡c nhau, thÆ°á»ng lÃ  `intermediate_size` gáº¥p $4$ láº§n `hidden_size`)
  - ÄÆ°a qua GELU (paper "Attention is all you need" dÃ¹ng hÃ m kÃ­ch hoáº¡t RELU)
  - ÄÆ°a qua `nn.Linear` (3072, 768)
  - **Äáº§u ra:** $[1,5,768]$


- Giá» hÃ£y kiá»ƒm tra Ä‘áº§u ra báº±ng code sau:

```python
feed_forward = FeedForward(config)
ff_outputs = feed_forward(attn_outputs)
ff_outputs.size()
# torch.Size([1, 5, 768])
```

- BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ táº¥t cáº£ cÃ¡c thÃ nh pháº§n Ä‘á»ƒ táº¡o ra má»™t lá»›p transformer encoder! Quyáº¿t Ä‘á»‹nh duy nháº¥t cÃ²n láº¡i lÃ  nÃªn Ä‘áº·t skip connections vÃ  layer normalization á»Ÿ vá»‹ trÃ­ nÃ o. ChÃºng ta hÃ£y xem Ä‘iá»u nÃ y áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n kiáº¿n trÃºc mÃ´ hÃ¬nh.

### Adding Layer Normalization

- NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, kiáº¿n trÃºc Transformer sá»­ dá»¥ng layer normalization (chuáº©n hÃ³a lá»›p) vÃ  skip connections (káº¿t ná»‘i bá» qua). Layer normalization chuáº©n hÃ³a tá»«ng Ä‘áº§u vÃ o trong batch Ä‘á»ƒ cÃ³ giÃ¡ trá»‹ trung bÃ¬nh báº±ng 0 vÃ  phÆ°Æ¡ng sai báº±ng 1. Skip connections truyá»n má»™t tensor Ä‘áº¿n lá»›p tiáº¿p theo cá»§a mÃ´ hÃ¬nh mÃ  khÃ´ng qua xá»­ lÃ½ vÃ  cá»™ng nÃ³ vÃ o tensor Ä‘Ã£ qua xá»­ lÃ½. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ´ng thá»©c cá»§a layer normalization (lÆ°u Ã½, cÃ´ng thá»©c dÆ°á»›i lÃ  cho má»™t vector, trong kiáº¿n trÃºc Transfomer ta sáº½ truyá»n `inputs_embeds` qua `nn.LayerNorm`. MÃ  `inputs_embeds` lÃ  ma tráº­n cÃ³ kÃ­ch thÆ°á»›c lÃ  $[1,5,768]$ nÃªn `nn.LayerNorm` sáº½ Ä‘Æ°á»£c tÃ­nh trÃªn chiá»u cuá»‘i cÃ¹ng, tá»©c lÃ  trÃªn cÃ¡c vector hÃ ng, má»—i vector cÃ³ $768$ pháº§n tá»­):

$$
y = \frac{x - \mathbb{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} \cdot \gamma + \beta
$$

- Trong Ä‘Ã³ vector $x$ lÃ  Ä‘áº§u vÃ o, $\mathbb{E}[x]$ lÃ  ká»³ vá»ng cá»§a $x$, $\text{Var}[x]$ lÃ  phÆ°Æ¡ng sai cá»§a $x$, $\epsilon$ lÃ  má»™t háº±ng sá»‘ nhá» Ä‘á»ƒ trÃ¡nh cho viá»‡c chia cho sá»‘ $0$ , $\gamma$ lÃ  há»‡ sá»‘ Ä‘iá»u chá»‰nh Ä‘á»™ lá»›n (tham sá»‘ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c trong quÃ¡ trÃ¬nh train), $\beta$ lÃ  há»‡ sá»‘ Ä‘iá»u chá»‰nh dá»™ dá»‹ch (cÅ©ng lÃ  tham sá»‘ cÃ³ thá»ƒ há»c)

- Khi nÃ³i Ä‘áº¿n viá»‡c Ä‘áº·t layer normalization vÃ o vá»‹ trÃ­ nÃ o trong cÃ¡c lá»›p encoder hoáº·c decoder cá»§a transformer, cÃ³ hai lá»±a chá»n chÃ­nh thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng:

1. **Post layer normalization:** ÄÃ¢y lÃ  cÃ¡ch sáº¯p xáº¿p Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i bÃ¡o Transformer gá»‘c; nÃ³ Ä‘áº·t layer normalization giá»¯a cÃ¡c skip connections. CÃ¡ch sáº¯p xáº¿p nÃ y khÃ³ huáº¥n luyá»‡n tá»« Ä‘áº§u vÃ¬ cÃ¡c gradient cÃ³ thá»ƒ bá»‹ phÃ¢n ká»³. VÃ¬ lÃ½ do nÃ y, báº¡n thÆ°á»ng tháº¥y khÃ¡i niá»‡m Ä‘Æ°á»£c gá»i lÃ  learning rate warm-up (lÃ m nÃ³ng tá»‘c Ä‘á»™ há»c), trong Ä‘Ã³ tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c tÄƒng dáº§n tá»« má»™t giÃ¡ trá»‹ nhá» Ä‘áº¿n má»™t giÃ¡ trá»‹ tá»‘i Ä‘a trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
2. **Pre layer normalization:** ÄÃ¢y lÃ  cÃ¡ch sáº¯p xáº¿p phá»• biáº¿n nháº¥t Ä‘Æ°á»£c tÃ¬m tháº¥y trong tÃ i liá»‡u; nÃ³ Ä‘áº·t layer normalization trong pháº¡m vi cá»§a cÃ¡c skip connections. CÃ¡ch nÃ y cÃ³ xu hÆ°á»›ng á»•n Ä‘á»‹nh hÆ¡n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  thÆ°á»ng khÃ´ng yÃªu cáº§u báº¥t ká»³ giai Ä‘oáº¡n lÃ m nÃ³ng tá»‘c Ä‘á»™ há»c nÃ o.

- Sá»± khÃ¡c biá»‡t giá»¯a hai cÃ¡ch sáº¯p xáº¿p Ä‘Æ°á»£c minh há»a trong HÃ¬nh 3-6.

![anh](./image/295.png)


- ChÃºng ta sáº½ code lá»›p `TransformerEncoderLayer` sá»­ dá»¥ng cÃ¡ch sáº¯p xáº¿p thá»© hai:

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm1 = nn.LayerNorm(config.hidden_state)
        self.layer_norm2 = nn.LayerNorm(config.hidden_state)
        self.attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)
    def forward(self, x):
        hidden_state = self.layer_norm1(x)
        x = x + self.attention(hidden_state)
        x = x + self.feed_forward(self.layer_norm2(x))
        return x
```

- Giá» hÃ£y kiá»ƒm tra vá»›i Ä‘áº§u vÃ o lÃ  `inputs_embeds`:

```python
encoder_layer = TransformerEncoderLayer(config)
inputs_embeds.shape, encoder_layer(inputs_embeds).size()
# (torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))
```

- ChÃºng ta Ä‘Ã£ code thÃ nh cÃ´ng lá»›p transformer encoder! Tuy nhiÃªn, báº¡n Ä‘ang tháº¥y chÃºng ta Ä‘ang bá»‹ thiáº¿u gÃ¬ khÃ´ng ? Náº¿u nhÆ° muti-head attention chá»‰ Ä‘Æ¡n giáº£n lÃ  tá»•ng cÃ³ trá»ng sá»‘ cá»§a cÃ¡c token thÃ¬ cháº³ng pháº£i token náº±m á»Ÿ vá»‹ trÃ­ khÃ¡c nhau trong cÃ¢u cÅ©ng khÃ´ng khÃ¡c nhau sao ? NhÆ° váº­y thÃ´ng tin vá»‹ trÃ­ cá»§a cÃ¡c token Ä‘Ã£ bá»‹ máº¥t, Ä‘iá»u nÃ y áº£nh hÆ°á»Ÿng ráº¥t nghiÃªm trá»ng, vÃ­ dá»¥ nhÆ° cÃ¢u "The dog chased the cat." vÃ  cÃ¢u "The cat chased the dog.", hai cÃ¢u nÃ y chá»‰ khÃ¡c nhau vá»‹ trÃ­ cá»§a token dog vÃ  cat nhÆ°ng Ã½ nghÄ©a Ä‘Ã£ khÃ¡c háº³n nhau. 
- May máº¯n thay, cÃ³ má»™t máº¹o dá»… dÃ ng Ä‘á»ƒ káº¿t há»£p thÃ´ng tin vá»‹ trÃ­ báº±ng cÃ¡ch sá»­ dá»¥ng positional encoding. ChÃºng ta hÃ£y cÃ¹ng xem xÃ©t.

### Positional Embeddings

- Positional Embeddings dá»±a trÃªn má»™t Ã½ tÆ°á»Ÿng Ä‘Æ¡n giáº£n nhÆ°ng ráº¥t hiá»‡u quáº£: bá»• sung cÃ¡c token embedding báº±ng má»™t máº«u giÃ¡ trá»‹ phá»¥ thuá»™c vÃ o vá»‹ trÃ­, Ä‘Æ°á»£c sáº¯p xáº¿p trong má»™t vector. Náº¿u máº«u nÃ y lÃ  Ä‘áº·c trÆ°ng cho tá»«ng vá»‹ trÃ­, attention heads vÃ  feed-forward layer cÃ³ thá»ƒ há»c cÃ¡ch káº¿t há»£p thÃ´ng tin vá» vá»‹ trÃ­ vÃ o cÃ¡c phÃ©p biáº¿n Ä‘á»•i cá»§a chÃºng.

- CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, vÃ  má»™t trong nhá»¯ng cÃ¡ch phá»• biáº¿n nháº¥t lÃ  sá»­ dá»¥ng má»™t máº«u cÃ³ thá»ƒ há»c Ä‘Æ°á»£c, Ä‘áº·c biá»‡t khi táº­p dá»¯ liá»‡u tiá»n huáº¥n luyá»‡n Ä‘á»§ lá»›n. CÃ¡ch nÃ y hoáº¡t Ä‘á»™ng giá»‘ng há»‡t vá»›i token embedding, nhÆ°ng sá»­ dá»¥ng chá»‰ sá»‘ vá»‹ trÃ­ thay vÃ¬ chá»‰ sá»‘ token lÃ m Ä‘áº§u vÃ o. Vá»›i cÃ¡ch tiáº¿p cáº­n Ä‘Ã³, má»™t cÃ¡ch hiá»‡u quáº£ Ä‘á»ƒ mÃ£ hÃ³a vá»‹ trÃ­ cá»§a cÃ¡c token Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n ra Ä‘á»i.

- HÃ£y táº¡o má»™t lá»›p `Embeddings` Ä‘Æ¡n giáº£n nhÆ° sau:

```python
class Embeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config.vocab_size,
        config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings,
        config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout()
    def forward(self, input_ids):
        # Create position IDs for input sequence
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
        # Create token and position embeddings
        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        # Combine token and position embeddings
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
embedding_layer = Embeddings(config)
embedding_layer(inputs.input_ids).size()
# torch.Size([1, 5, 768])
``` 

- Máº·c dÃ¹ learnable position embeddings ráº¥t dá»… code vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i, nhÆ°ng cÃ³ má»™t sá»‘ lá»±a chá»n thay tháº¿:

1. **Absolute positional representations (biá»ƒu diá»…n vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i):** CÃ¡c mÃ´ hÃ¬nh transformers cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c cÃ´ng thá»©c nhÆ° sin vÃ  cosin Ä‘á»ƒ mÃ£ hÃ³a vá»‹ trÃ­ cá»§a cÃ¡c token. Äiá»u nÃ y Ä‘áº·c biá»‡t hoáº¡t Ä‘á»™ng tá»‘t khi khÃ´ng cÃ³ khá»‘i lÆ°á»£ng dá»¯ liá»‡u lá»›n. BÃ i bÃ¡o Transformers gá»‘c dÃ¹ng kiá»ƒu nÃ y.
2. **Relative positional representations (biá»ƒu diá»…n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i):** Máº·c dÃ¹ cÃ¡c vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i lÃ  quan trá»ng, nhÆ°ng cÃ³ thá»ƒ láº­p luáº­n ráº±ng khi tÃ­nh toÃ¡n má»™t embedding, cÃ¡c token xung quanh má»›i lÃ  quan trá»ng nháº¥t. Biá»ƒu diá»…n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i tuÃ¢n theo trá»±c giÃ¡c nÃ y vÃ  mÃ£ hÃ³a cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c token. Äiá»u nÃ y khÃ´ng thá»ƒ Ä‘Æ°á»£c thiáº¿t láº­p chá»‰ báº±ng cÃ¡ch thÃªm má»™t táº§ng embedding tÆ°Æ¡ng Ä‘á»‘i má»›i á»Ÿ pháº§n Ä‘áº§u, vÃ¬ embedding tÆ°Æ¡ng Ä‘á»‘i thay Ä‘á»•i cho tá»«ng token tÃ¹y thuá»™c vÃ o vá»‹ trÃ­ tá»« chuá»—i mÃ  chÃºng ta Ä‘ang quan sÃ¡t. Thay vÃ o Ä‘Ã³, cÆ¡ cháº¿ attention Ä‘Æ°á»£c sá»­a Ä‘á»•i báº±ng cÃ¡c thÃ nh pháº§n bá»• sung Ä‘á»ƒ Ä‘Æ°a vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c token vÃ o xem xÃ©t. CÃ¡c mÃ´ hÃ¬nh nhÆ° DeBERTa sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n nÃ y (Báº±ng cÃ¡ch káº¿t há»£p Ã½ tÆ°á»Ÿng vá» biá»ƒu diá»…n vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i vÃ  tÆ°Æ¡ng Ä‘á»‘i, rotary position embeddings (nhÃºng vá»‹ trÃ­ quay) Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tuyá»‡t vá»i trong nhiá»u nhiá»‡m vá»¥. GPT-Neo lÃ  má»™t vÃ­ dá»¥ vá» mÃ´ hÃ¬nh cÃ³ nhÃºng vá»‹ trÃ­ quay, cÃ³ thá»ƒ trong tÆ°Æ¡ng lai tÃ´i sáº½ lÃ m má»™t blog vá» rotary position embeddings).

- BÃ¢y giá» hÃ£y káº¿t há»£p táº¥t cáº£ nhá»¯ng Ä‘iá»u nÃ y láº¡i vá»›i nhau báº±ng cÃ¡ch code lá»›p `TransformerEncoder`:

```python
class TransformerEncoder(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.embeddings = Embeddings(config)
    self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])
  def forward(self, x):
    x = self.embeddings(x)
    for layer in self.layers:
      x = layer(x)
    return x
encoder = TransformerEncoder(config)
encoder(inputs.input_ids).size()
# torch.Size([1, 5, 768])
```

- ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng chÃºng ta nháº­n Ä‘Æ°á»£c hidden state (vector embedding) cho má»—i token trong má»™t batch. Äá»‹nh dáº¡ng Ä‘áº§u ra nÃ y lÃ m cho kiáº¿n trÃºc ráº¥t linh hoáº¡t vÃ  chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng Ä‘iá»u chá»‰nh nÃ³ cho cÃ¡c á»©ng dá»¥ng khÃ¡c nhau nhÆ° dá»± Ä‘oÃ¡n cÃ¡c token bá»‹ thiáº¿u trong masked language modeling hoáº·c dá»± Ä‘oÃ¡n vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a cÃ¢u tráº£ lá»i trong question answering. Trong pháº§n tiáº¿p theo, chÃºng ta sáº½ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ code má»™t bá»™ phÃ¢n loáº¡i giá»‘ng nhÆ° cÃ¡i mÃ  chÃºng ta Ä‘Ã£ sá»­ dá»¥ng trong blog "Text Classification".

### Adding a Classification Head

- CÃ¡c mÃ´ hÃ¬nh transformer thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh má»™t body Ä‘á»™c láº­p vá»›i nhiá»‡m vá»¥ vÃ  má»™t task-specific head.Nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng cho Ä‘áº¿n nay lÃ  body, vÃ¬ váº­y náº¿u chÃºng ta muá»‘n xÃ¢y dá»±ng má»™t bá»™ phÃ¢n loáº¡i vÄƒn báº£n, chÃºng ta sáº½ cáº§n pháº£i gáº¯n má»™t Ä‘áº§u phÃ¢n loáº¡i vÃ o body Ä‘Ã³. ChÃºng ta cÃ³ má»™t hidden state cho má»—i token, nhÆ°ng chÃºng ta chá»‰ cáº§n Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n. CÃ³ má»™t sá»‘ lá»±a chá»n Ä‘á»ƒ tiáº¿p cáº­n Ä‘iá»u nÃ y. Theo truyá»n thá»‘ng, token Ä‘áº§u tiÃªn trong cÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n vÃ  chÃºng ta cÃ³ thá»ƒ Ä‘Ã­nh kÃ¨m má»™t lá»›p dropout vÃ  má»™t lá»›p linear Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n phÃ¢n loáº¡i. Lá»›p sau má»Ÿ rá»™ng bá»™ encoder hiá»‡n cÃ³ Ä‘á»ƒ phÃ¢n loáº¡i chuá»—i:

```python
class TransformerForSequenceClassification(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.encoder = TransformerEncoder(config)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, config.num_labels)
  def forward(self, x):
    x = self.encoder(x)[:, 0, :] # chá»n hidden state cho token [CLS]
    x = self.dropout(x)
    x = self.classifier(x)
    return x
```
- TrÆ°á»›c khi khá»Ÿi táº¡o mÃ´ hÃ¬nh, chÃºng ta cáº§n xÃ¡c Ä‘á»‹nh cÃ³ bao nhiÃªu lá»›p mÃ  chÃºng ta muá»‘n dá»± Ä‘oÃ¡n:

```python
config.num_labels = 3
encoder_classifier = TransformerForSequenceClassification(config)
encoder_classifier(inputs.input_ids).size()
# torch.Size([1, 3])

```

- ÄÃ³ chÃ­nh xÃ¡c lÃ  nhá»¯ng gÃ¬ chÃºng ta Ä‘ang tÃ¬m kiáº¿m. Äá»‘i vá»›i má»—i vÃ­ dá»¥ trong batch, chÃºng ta nháº­n Ä‘Æ°á»£c cÃ¡c logit chÆ°a chuáº©n hÃ³a cho má»—i lá»›p trong Ä‘áº§u ra. Äiá»u nÃ y tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh BERT mÃ  chÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng trong blog "Text Classification" Ä‘á»ƒ phÃ¡t hiá»‡n cáº£m xÃºc trong tweet. 
- Äiá»u nÃ y káº¿t thÃºc phÃ¢n tÃ­ch cá»§a chÃºng ta vá» encoder vÃ  cÃ¡ch chÃºng ta cÃ³ thá»ƒ káº¿t há»£p nÃ³ vá»›i má»™t Ä‘áº§u nhiá»‡m vá»¥ cá»¥ thá»ƒ. BÃ¢y giá» chÃºng ta hÃ£y nÃ³i Ä‘áº¿n decoder nÃ o !

### The Decoder

- NhÆ° minh há»a trong HÃ¬nh 3-7, sá»± khÃ¡c biá»‡t chÃ­nh giá»¯a encoder vÃ  decoder lÃ  decoder cÃ³ hai lá»›p attention:

1. **Masked multi-head self-attention layer:** Äáº£m báº£o ráº±ng cÃ¡c token Ä‘Æ°á»£c táº¡o ra táº¡i má»—i bÆ°á»›c thá»i gian chá»‰ dá»±a vÃ o cÃ¡c Ä‘áº§u ra trÆ°á»›c Ä‘Ã³ vÃ  token hiá»‡n táº¡i Ä‘ang Ä‘Æ°á»£c dá»± Ä‘oÃ¡n. Náº¿u khÃ´ng cÃ³ táº§ng nÃ y, decoder cÃ³ thá»ƒ "gian láº­n" trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng cÃ¡ch Ä‘Æ¡n giáº£n sao chÃ©p cÃ¡c báº£n dá»‹ch má»¥c tiÃªu; viá»‡c sá»­ dá»¥ng máº·t náº¡ trÃªn Ä‘áº§u vÃ o Ä‘áº£m báº£o ráº±ng nhiá»‡m vá»¥ khÃ´ng trá»Ÿ nÃªn quÃ¡ dá»… dÃ ng.
2. **Encoder-decoder attention layer:** Thá»±c hiá»‡n multi-head attention trÃªn cÃ¡c vector key, vector value cá»§a encoder, vÃ  vector query cá»§a decoder (LÆ°u Ã½ ráº±ng khÃ´ng giá»‘ng nhÆ° self-attention, cÃ¡c vectÆ¡ key vÃ  query trong encoder-decoder attention cÃ³ thá»ƒ cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau. Äiá»u nÃ y lÃ  do Ä‘áº§u vÃ o encoder vÃ  decoder thÆ°á»ng sáº½ liÃªn quan Ä‘áº¿n cÃ¡c chuá»—i cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau. Káº¿t quáº£ lÃ , ma tráº­n attention scores trong lá»›p nÃ y lÃ  ma tráº­n chá»¯ nháº­t, khÃ´ng pháº£i ma tráº­n). Theo cÃ¡ch nÃ y, táº§ng attention encoder-decoder há»c cÃ¡ch liÃªn káº¿t cÃ¡c token tá»« hai chuá»—i khÃ¡c nhau, cháº³ng háº¡n nhÆ° hai ngÃ´n ngá»¯ khÃ¡c nhau. Decoder cÃ³ quyá»n truy cáº­p vÃ o cÃ¡c key vÃ  value cá»§a encoder trong má»—i khá»‘i.

- HÃ£y xem xÃ©t cÃ¡c thay Ä‘á»•i cáº§n thá»±c hiá»‡n Ä‘á»ƒ thÃªm máº·t náº¡ (masking) vÃ o táº§ng self-attention, pháº§n code lá»›p encoder-decoder attention Ä‘Æ°á»£c coi nhÆ° bÃ i táº­p vá» nhÃ  dÃ nh cho báº¡n. Má»™t máº¹o vá»›i masked self-attention lÃ  dÃ¹ng mask matrix, ma tráº­n vá»›i cÃ¡c giÃ¡ trá»‹ 1 á»Ÿ pháº§n chÃ©o dÆ°á»›i vÃ  cÃ¡c giÃ¡ trá»‹ 0 á»Ÿ trÃªn nhÆ° sau:

```python
seq_len = inputs.input_ids.size(-1)
mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)
mask[0]
# tensor([[1., 0., 0., 0., 0.],
#         [1., 1., 0., 0., 0.],
#         [1., 1., 1., 0., 0.],
#         [1., 1., 1., 1., 0.],
#         [1., 1., 1., 1., 1.]])
```

- á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ sá»­ dá»¥ng hÃ m `tril()` cá»§a PyTorch Ä‘á»ƒ táº¡o ma tráº­n tam giÃ¡c. Khi chÃºng ta cÃ³ mask matrix nÃ y, chÃºng ta cÃ³ thá»ƒ ngÄƒn má»—i attention head nhÃ¬n trá»™m cÃ¡c token trong tÆ°Æ¡ng lai báº±ng cÃ¡ch sá»­ dá»¥ng `Tensor.masked_fill()` Ä‘á»ƒ thay tháº¿ táº¥t cáº£ cÃ¡c sá»‘ $0$ báº±ng Ã¢m vÃ´ cÃ¹ng:

```python
scores.masked_fill(mask == 0, -float("inf"))
# tensor([[[26.8082, -inf, -inf, -inf, -inf],
#  [-0.6981, 26.9043, -inf, -inf, -inf],
#  [-2.3190, 1.2928, 27.8710, -inf, -inf],
#  [-0.5897, 0.3497, -0.3807, 27.5488, -inf],
#  [ 0.5275, 2.0493, -0.4869, 1.6100, 29.0893]]],
#  grad_fn=<MaskedFillBackward0>)

```

![anh](./image/296.png)

- Báº±ng cÃ¡ch Ä‘áº·t cÃ¡c giÃ¡ trá»‹ náº±m trÃªn Ä‘Æ°á»ng chÃ©o chÃ­nh báº±ng Ã¢m vÃ´ cÃ¹ng, chÃºng ta Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c trá»ng sá»‘ chÃº Ã½ báº±ng $0$ khi chÃºng ta láº¥y softmax vÃ¬ $e^{-\inf} = 0$. ChÃºng ta cÃ³ thá»ƒ dá»… dÃ ng code masked attention chá»‰ vá»›i má»™t thay Ä‘á»•i nhá» Ä‘á»‘i vá»›i hÃ m `scaled_dot_product_attention(` mÃ  chÃºng ta Ä‘Ã£ thá»±c hiá»‡n trÆ°á»›c Ä‘Ã³:


```python
def scaled_dot_product_attention(query, key, value, mask=None):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    weights = F.softmax(scores, dim=-1)
    return weights.bmm(value)
```

- Tá»« Ä‘Ã¢y, viá»‡c xÃ¢y dá»±ng lá»›p decoder lÃ  má»™t váº¥n Ä‘á» Ä‘Æ¡n giáº£n. Báº¡n cÃ³ thá»ƒ xem qua pháº§n code xuáº¥t sáº¯c cá»§a [minGPT](https://github.com/karpathy/minGPT), láº­p trÃ¬nh bá»Ÿi Andrej Karpathy Ä‘á»ƒ biáº¿t chi tiáº¿t.

- DÆ°á»›i Ä‘Ã¢y lÃ  má»™t cÃ¡ch hiá»ƒu Ä‘Æ¡n giáº£n vá» encoder-decoder attention: HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n (decoder) Ä‘ang trong lá»›p lÃ m bÃ i kiá»ƒm tra. BÃ i táº­p lÃ  dá»± Ä‘oÃ¡n tá»« tiáº¿p theo dá»±a trÃªn cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ (decoder input), nghe cÃ³ váº» Ä‘Æ¡n giáº£n nhÆ°ng cá»±c ká»³ khÃ³ (hÃ£y tá»± mÃ¬nh thá»­ vÃ  dá»± Ä‘oÃ¡n cÃ¡c tá»« tiáº¿p theo trong má»™t Ä‘oáº¡n cá»§a blog nÃ y). May máº¯n thay, hÃ ng xÃ³m cá»§a báº¡n (encoder) cÃ³ cáº£ Ä‘oáº¡n vÄƒn Ä‘Ã³. Tháº­t khÃ´ng may, ngÆ°á»i hÃ ng xÃ³m láº¡i lÃ  má»™t sinh viÃªn trao Ä‘á»•i nÆ°á»›c ngoÃ i vÃ  vÄƒn báº£n kia báº±ng tiáº¿ng máº¹ Ä‘áº» cá»§a há». NhÆ°ng báº¡n lÃ  má»™t sinh viÃªn thÃ´ng minh, báº¡n váº«n tÃ¬m ra cÃ¡ch gian láº­n. Báº¡n váº½ má»™t bá»©c tranh hoáº¡t hÃ¬nh nhá» minh há»a vÄƒn báº£n báº¡n Ä‘ang cÃ³ (query) vÃ  Ä‘Æ°a nÃ³ cho hÃ ng xÃ³m cá»§a báº¡n. Há» cá»‘ gáº¯ng tÃ¬m ra Ä‘oáº¡n nÃ o phÃ¹ há»£p vá»›i mÃ´ táº£ Ä‘Ã³ (key), váº½ má»™t bá»©c tranh hoáº¡t hÃ¬nh mÃ´ táº£ tá»« trong Ä‘oáº¡n Ä‘Ã³ (value) vÃ  chuyá»ƒn láº¡i cho báº¡n. Vá»›i há»‡ thá»‘ng nÃ y, báº¡n Ä‘Ã£ vÆ°á»£t qua ká»³ thi.

## 2. Meet the Transformers

- NhÆ° báº¡n Ä‘Ã£ tháº¥y trong blog nÃ y, cÃ³ ba kiáº¿n trÃºc chÃ­nh cho cÃ¡c mÃ´ hÃ¬nh transformers: encoder, decoder vÃ  encoder-decoder. ThÃ nh cÃ´ng ban Ä‘áº§u cá»§a cÃ¡c mÃ´ hÃ¬nh transformer sá»›m Ä‘Ã£ kÃ­ch hoáº¡t má»™t "vá»¥ ná»• Cambrian" trong sá»± phÃ¡t triá»ƒn mÃ´ hÃ¬nh, khi cÃ¡c nhÃ  nghiÃªn cá»©u xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh trÃªn cÃ¡c bá»™ dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c vÃ  tÃ­nh cháº¥t khÃ¡c nhau, sá»­ dá»¥ng cÃ¡c má»¥c tiÃªu tiá»n huáº¥n luyá»‡n má»›i, vÃ  Ä‘iá»u chá»‰nh kiáº¿n trÃºc Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t hÆ¡n ná»¯a. Máº·c dÃ¹ "vÆ°á»n thÃº" cÃ¡c mÃ´ hÃ¬nh váº«n Ä‘ang phÃ¡t triá»ƒn nhanh chÃ³ng, chÃºng váº«n cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh ba loáº¡i chÃ­nh nÃ y.

- Trong pháº§n nÃ y, tÃ´i sáº½ cung cáº¥p má»™t cÃ¡i nhÃ¬n tá»•ng quan ngáº¯n gá»n vá» cÃ¡c mÃ´ hÃ¬nh transformer quan trá»ng nháº¥t trong má»—i loáº¡i. HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch xem xÃ©t cÃ¢y pháº£ há»‡ cá»§a transformer.

### The Transformer Tree of Life

- Theo thá»i gian, má»—i trong ba kiáº¿n trÃºc chÃ­nh Ä‘Ã£ tráº£i qua má»™t quÃ¡ trÃ¬nh tiáº¿n hÃ³a riÃªng cá»§a nÃ³. Äiá»u nÃ y Ä‘Æ°á»£c minh há»a trong HÃ¬nh 3-8, nÆ¡i thá»ƒ hiá»‡n má»™t sá»‘ mÃ´ hÃ¬nh ná»•i báº­t nháº¥t vÃ  cÃ¡c háº­u duá»‡ cá»§a chÃºng.

![anh](./image/297.png)

- Vá»›i hÆ¡n 50 kiáº¿n trÃºc khÃ¡c nhau bao gá»“m trong ğŸ¤— Transformers, cÃ¢y pháº£ há»‡ nÃ y khÃ´ng thá»ƒ nÃ o cung cáº¥p má»™t cÃ¡i nhÃ¬n tá»•ng quan Ä‘áº§y Ä‘á»§ vá» táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh hiá»‡n cÃ³: nÃ³ chá»‰ lÃ m ná»•i báº­t má»™t vÃ i cá»™t má»‘c kiáº¿n trÃºc quan trá»ng. ChÃºng ta Ä‘Ã£ tháº£o luáº­n chi tiáº¿t vá» kiáº¿n trÃºc Transformer ban Ä‘áº§u trong blog nÃ y, vÃ¬ váº­y hÃ£y cÃ¹ng xem xÃ©t ká»¹ hÆ¡n má»™t sá»‘ "háº­u duá»‡" chÃ­nh, báº¯t Ä‘áº§u vá»›i nhÃ¡nh encoder.

### The Encoder Branch

- MÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng encoder Ä‘áº§u tiÃªn dá»±a trÃªn kiáº¿n trÃºc Transformer lÃ  BERT. Khi Ä‘Æ°á»£c cÃ´ng bá»‘, nÃ³ Ä‘Ã£ vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh tiÃªn tiáº¿n nháº¥t trÃªn chuáº©n Ä‘Ã¡nh giÃ¡ [GLUE](https://arxiv.org/abs/1804.07461) phá»• biáº¿n, má»™t thÆ°á»›c Ä‘o kháº£ nÄƒng hiá»ƒu natural language understanding (NLU) trÃªn nhiá»u tÃ¡c vá»¥ cÃ³ má»©c Ä‘á»™ khÃ³ khÃ¡c nhau. Sau Ä‘Ã³, má»¥c tiÃªu tiá»n huáº¥n luyá»‡n vÃ  kiáº¿n trÃºc cá»§a BERT Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t hÆ¡n ná»¯a. CÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng encoder váº«n thá»‘ng trá»‹ trong nghiÃªn cá»©u vÃ  cÃ´ng nghiá»‡p vá» cÃ¡c tÃ¡c vá»¥ NLU, cháº³ng háº¡n nhÆ° xá»­ lÃ½ vÄƒn báº£n, NER, question answering. HÃ£y xem qua mÃ´ hÃ¬nh BERT vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³:

1. **BERT**:
- [BERT](https://arxiv.org/abs/1810.04805) Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n vá»›i hai má»¥c tiÃªu: dá»± Ä‘oÃ¡n cÃ¡c tá»« bá»‹ che trong vÄƒn báº£n vÃ  xÃ¡c Ä‘á»‹nh xem má»™t Ä‘oáº¡n vÄƒn báº£n cÃ³ kháº£ nÄƒng tiáº¿p theo sau má»™t Ä‘oáº¡n khÃ¡c hay khÃ´ng. Nhiá»‡m vá»¥ Ä‘áº§u tiÃªn Ä‘Æ°á»£c gá»i lÃ  mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ cÃ³ che (masked language modeling - MLM), vÃ  nhiá»‡m vá»¥ sau lÃ  dá»± Ä‘oÃ¡n cÃ¢u tiáº¿p theo (next sentence prediction - NSP).

2. **DistilBERT**:
- Máº·c dÃ¹ BERT mang láº¡i káº¿t quáº£ tuyá»‡t vá»i, kÃ­ch thÆ°á»›c cá»§a nÃ³ cÃ³ thá»ƒ gÃ¢y khÃ³ khÄƒn khi triá»ƒn khai trong cÃ¡c mÃ´i trÆ°á»ng yÃªu cáº§u Ä‘á»™ trá»… tháº¥p. Báº±ng cÃ¡ch sá»­ dá»¥ng má»™t ká»¹ thuáº­t gá»i lÃ  triá»ƒn khai tri thá»©c (knowledge distillation) trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n, [DistilBERT](https://arxiv.org/abs/1910.01108) Ä‘áº¡t Ä‘Æ°á»£c 97% hiá»‡u suáº¥t cá»§a BERT, Ä‘á»“ng thá»i sá»­ dá»¥ng Ã­t hÆ¡n 40% bá»™ nhá»› vÃ  nhanh hÆ¡n 60%. 

3. **RoBERTa:**
- Má»™t nghiÃªn cá»©u sau khi phÃ¡t hÃ nh BERT cho tháº¥y hiá»‡u suáº¥t cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n báº±ng cÃ¡ch sá»­a Ä‘á»•i phÆ°Æ¡ng phÃ¡p tiá»n huáº¥n luyá»‡n. [RoBERTa](https://arxiv.org/abs/1907.11692) Ä‘Æ°á»£c huáº¥n luyá»‡n trong thá»i gian dÃ i hÆ¡n, vá»›i cÃ¡c batch lá»›n hÆ¡n vÃ  nhiá»u dá»¯ liá»‡u huáº¥n luyá»‡n hÆ¡n, Ä‘á»“ng thá»i loáº¡i bá» nhiá»‡m vá»¥ NSP. Nhá»¯ng thay Ä‘á»•i nÃ y Ä‘Ã£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a nÃ³ so vá»›i mÃ´ hÃ¬nh BERT gá»‘c.

4. **XLM:**
- Nhiá»u má»¥c tiÃªu tiá»n huáº¥n luyá»‡n Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c khÃ¡m phÃ¡ trong cÃ´ng trÃ¬nh vá» mÃ´ hÃ¬nh ngÃ´n ngá»¯ xuyÃªn ngÃ´n ngá»¯ ([XLM](https://arxiv.org/abs/1901.07291)), bao gá»“m mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ tá»± há»“i quy tá»« cÃ¡c mÃ´ hÃ¬nh giá»‘ng GPT vÃ  MLM tá»« BERT. NgoÃ i ra, cÃ¡c tÃ¡c giáº£ cá»§a bÃ i bÃ¡o vá» tiá»n huáº¥n luyá»‡n XLM Ä‘Ã£ giá»›i thiá»‡u mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ dá»‹ch (translation language modeling - TLM), má»™t má»Ÿ rá»™ng cá»§a MLM cho Ä‘áº§u vÃ o Ä‘a ngÃ´n ngá»¯. Thá»­ nghiá»‡m vá»›i cÃ¡c nhiá»‡m vá»¥ tiá»n huáº¥n luyá»‡n nÃ y, há» Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tiÃªn tiáº¿n nháº¥t trÃªn nhiá»u chuáº©n Ä‘Ã¡nh giÃ¡ NLU Ä‘a ngÃ´n ngá»¯ cÅ©ng nhÆ° cÃ¡c nhiá»‡m vá»¥ dá»‹ch thuáº­t.

5. **XLM-RoBERTa**
- Tiáº¿p ná»‘i cÃ´ng trÃ¬nh cá»§a XLM vÃ  RoBERTa, mÃ´ hÃ¬nh [XLM-RoBERTa](https://arxiv.org/abs/1911.02116) (hoáº·c XLM-R) Ä‘Æ°a tiá»n huáº¥n luyá»‡n Ä‘a ngÃ´n ngá»¯ lÃªn má»™t cáº¥p Ä‘á»™ cao hÆ¡n báº±ng cÃ¡ch má»Ÿ rá»™ng quy mÃ´ dá»¯ liá»‡u huáº¥n luyá»‡n má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ. Sá»­ dá»¥ng táº­p dá»¯ liá»‡u Common Crawl corpus, cÃ¡c nhÃ  phÃ¡t triá»ƒn Ä‘Ã£ táº¡o ra má»™t táº­p dá»¯ liá»‡u vá»›i 2,5 terabyte vÄƒn báº£n; sau Ä‘Ã³ há» huáº¥n luyá»‡n má»™t bá»™ mÃ£ hÃ³a vá»›i MLM trÃªn táº­p dá»¯ liá»‡u nÃ y.

6. **ALBERT:**
- MÃ´ hÃ¬nh [ALBERT](https://arxiv.org/abs/1909.11942) Ä‘Ã£ giá»›i thiá»‡u ba thay Ä‘á»•i Ä‘á»ƒ lÃ m cho kiáº¿n trÃºc bá»™ mÃ£ hÃ³a hiá»‡u quáº£ hÆ¡n.
- Äáº§u tiÃªn, nÃ³ tÃ¡ch biá»‡t kÃ­ch thÆ°á»›c embedding cá»§a token khá»i kÃ­ch thÆ°á»›c cá»§a lá»›p áº©n, cho phÃ©p kÃ­ch thÆ°á»›c embedding nhá» hÆ¡n vÃ  nhá» Ä‘Ã³ tiáº¿t kiá»‡m tham sá»‘, Ä‘áº·c biá»‡t khi tá»« vá»±ng trá»Ÿ nÃªn lá»›n.
- Thá»© hai, táº¥t cáº£ cÃ¡c lá»›p chia sáº» cÃ¹ng tham sá»‘, Ä‘iá»u nÃ y giáº£m sá»‘ lÆ°á»£ng tham sá»‘ hiá»‡u quáº£ cáº§n thiáº¿t xuá»‘ng thÃªm ná»¯a.
- Cuá»‘i cÃ¹ng, má»¥c tiÃªu NSP (Next Sentence Prediction) Ä‘Æ°á»£c thay tháº¿ báº±ng dá»± Ä‘oÃ¡n thá»© tá»± cÃ¢u: mÃ´ hÃ¬nh cáº§n dá»± Ä‘oÃ¡n liá»‡u thá»© tá»± cá»§a hai cÃ¢u liÃªn tiáº¿p cÃ³ bá»‹ hoÃ¡n Ä‘á»•i hay khÃ´ng, thay vÃ¬ dá»± Ä‘oÃ¡n liá»‡u chÃºng cÃ³ thuá»™c vá» cÃ¹ng má»™t ngá»¯ cáº£nh hay khÃ´ng.
- Nhá»¯ng thay Ä‘á»•i nÃ y lÃ m cho viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n vá»›i Ã­t tham sá»‘ hÆ¡n trá»Ÿ nÃªn kháº£ thi vÃ  Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u nÄƒng vÆ°á»£t trá»™i trÃªn cÃ¡c tÃ¡c vá»¥ NLU (Hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn).

7. **ELECTRA:**
- Má»™t háº¡n cháº¿ cá»§a má»¥c tiÃªu tiá»n huáº¥n luyá»‡n MLM (Masked Language Model) tiÃªu chuáº©n lÃ  á»Ÿ má»—i bÆ°á»›c huáº¥n luyá»‡n, chá»‰ cÃ¡c Ä‘áº¡i diá»‡n cá»§a cÃ¡c token bá»‹ che Ä‘i Ä‘Æ°á»£c cáº­p nháº­t, trong khi cÃ¡c token Ä‘áº§u vÃ o khÃ¡c thÃ¬ khÃ´ng. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, [ELECTRA](https://arxiv.org/abs/2003.10555) sá»­ dá»¥ng cÃ¡ch tiáº¿p cáº­n hai mÃ´ hÃ¬nh:
- MÃ´ hÃ¬nh Ä‘áº§u tiÃªn (thÆ°á»ng lÃ  nhá») hoáº¡t Ä‘á»™ng nhÆ° má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ che vÃ  dá»± Ä‘oÃ¡n cÃ¡c token bá»‹ che.
- MÃ´ hÃ¬nh thá»© hai, Ä‘Æ°á»£c gá»i lÃ  bá»™ phÃ¢n biá»‡t (discriminator), Ä‘Æ°á»£c giao nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n token nÃ o trong Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh Ä‘áº§u tiÃªn lÃ  bá»‹ che ban Ä‘áº§u.
- Do Ä‘Ã³, bá»™ phÃ¢n biá»‡t cáº§n thá»±c hiá»‡n phÃ¢n loáº¡i nhá»‹ phÃ¢n cho má»—i token, Ä‘iá»u nÃ y lÃ m cho viá»‡c huáº¥n luyá»‡n hiá»‡u quáº£ hÆ¡n gáº¥p 30 láº§n. Trong cÃ¡c tÃ¡c vá»¥ downstream, bá»™ phÃ¢n biá»‡t Ä‘Æ°á»£c tinh chá»‰nh giá»‘ng nhÆ° má»™t mÃ´ hÃ¬nh BERT tiÃªu chuáº©n.

8. **DeBERTa:**
- MÃ´ hÃ¬nh [DeBERTa](https://arxiv.org/abs/2006.03654) giá»›i thiá»‡u hai thay Ä‘á»•i kiáº¿n trÃºc:
- Äáº§u tiÃªn, má»—i token Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng hai vector: má»™t cho ná»™i dung, cÃ¡i cÃ²n láº¡i cho vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i. Báº±ng cÃ¡ch tÃ¡ch ná»™i dung cá»§a token khá»i vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a nÃ³, cÃ¡c lá»›p self-attention cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a tá»‘t hÆ¡n sá»± phá»¥ thuá»™c cá»§a cÃ¡c token gáº§n nhau.
- Máº·t khÃ¡c, vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i cá»§a má»™t tá»« cÅ©ng ráº¥t quan trá»ng, Ä‘áº·c biá»‡t lÃ  Ä‘á»‘i vá»›i viá»‡c giáº£i mÃ£. VÃ¬ lÃ½ do nÃ y, má»™t embedding vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i Ä‘Æ°á»£c thÃªm vÃ o ngay trÆ°á»›c lá»›p softmax cá»§a lá»›p giáº£i mÃ£ token.
- DeBERTa lÃ  mÃ´ hÃ¬nh Ä‘áº§u tiÃªn (Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t táº­p há»£p mÃ´ hÃ¬nh) Ä‘Ã¡nh báº¡i con ngÆ°á»i trÃªn chá»‰ sá»‘ [SuperGLUE](https://arxiv.org/abs/1905.00537) benchmark, má»™t phiÃªn báº£n khÃ³ hÆ¡n cá»§a GLUE.
  
- Giá» chÃºnng ta hÃ£y xem qua cÃ¡c mÃ´ hÃ¬nh ná»•i báº­t á»Ÿ nhÃ¡nh decoder.

### The Decoder Branch

- Tiáº¿n bá»™ trong cÃ¡c mÃ´ hÃ¬nh transformer decoder Ä‘Ã£ Ä‘Æ°á»£c dáº«n Ä‘áº§u pháº§n lá»›n bá»Ÿi OpenAI. Nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘áº·c biá»‡t xuáº¥t sáº¯c trong viá»‡c dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t chuá»—i vÃ  thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c tÃ¡c vá»¥ sinh vÄƒn báº£n. Sá»± tiáº¿n bá»™ cá»§a chÃºng Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi viá»‡c sá»­ dá»¥ng cÃ¡c táº­p dá»¯ liá»‡u lá»›n hÆ¡n vÃ  má»Ÿ rá»™ng quy mÃ´ cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lÃªn kÃ­ch thÆ°á»›c ngÃ y cÃ ng lá»›n hÆ¡n. HÃ£y cÃ¹ng xem sá»± phÃ¡t triá»ƒn cá»§a nhá»¯ng mÃ´ hÃ¬nh sinh vÄƒn báº£n thÃº vá»‹ nÃ y:

1. **GPT:**
- Viá»‡c giá»›i thiá»‡u [GPT](https://openai.com/index/language-unsupervised/) Ä‘Ã£ káº¿t há»£p hai Ã½ tÆ°á»Ÿng chÃ­nh trong NLP: kiáº¿n trÃºc transformer decoder má»›i vÃ  hiá»‡u quáº£, cÃ¹ng vá»›i há»c chuyá»ƒn giao. Trong cáº¥u hÃ¬nh nÃ y, mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo dá»±a trÃªn cÃ¡c tá»« trÆ°á»›c Ä‘Ã³. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u BookCorpus vÃ  Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tuyá»‡t vá»i trong cÃ¡c tÃ¡c vá»¥ downstream nhÆ° phÃ¢n loáº¡i.

2. **GPT-2:**
- ÄÆ°á»£c truyá»n cáº£m há»©ng tá»« thÃ nh cÃ´ng cá»§a cÃ¡ch tiáº¿p cáº­n tiá»n huáº¥n luyá»‡n Ä‘Æ¡n giáº£n vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng, mÃ´ hÃ¬nh ban Ä‘áº§u vÃ  táº­p dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Ã£ Ä‘Æ°á»£c nÃ¢ng cáº¥p Ä‘á»ƒ táº¡o ra [GPT-2](https://openai.com/index/better-language-models/). MÃ´ hÃ¬nh nÃ y cÃ³ kháº£ nÄƒng táº¡o ra cÃ¡c chuá»—i vÄƒn báº£n dÃ i, máº¡ch láº¡c. Do lo ngáº¡i vá» kháº£ nÄƒng láº¡m dá»¥ng, mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t hÃ nh theo tá»«ng giai Ä‘oáº¡n, báº¯t Ä‘áº§u vá»›i cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n trÆ°á»›c vÃ  mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ sau Ä‘Ã³.

3. **CTRL**
- CÃ¡c mÃ´ hÃ¬nh nhÆ° GPT-2 cÃ³ thá»ƒ tiáº¿p tá»¥c má»™t chuá»—i Ä‘áº§u vÃ o (cÃ²n gá»i lÃ  prompt). Tuy nhiÃªn, ngÆ°á»i dÃ¹ng cÃ³ ráº¥t Ã­t quyá»n kiá»ƒm soÃ¡t phong cÃ¡ch cá»§a vÄƒn báº£n Ä‘Æ°á»£c táº¡o ra. MÃ´ hÃ¬nh Conditional Transformer Language ([CTRL](https://arxiv.org/abs/1909.05858)) giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch thÃªm cÃ¡c "token kiá»ƒm soÃ¡t" vÃ o Ä‘áº§u chuá»—i. Nhá»¯ng token nÃ y cho phÃ©p kiá»ƒm soÃ¡t phong cÃ¡ch cá»§a vÄƒn báº£n Ä‘Æ°á»£c táº¡o ra, tá»« Ä‘Ã³ cho phÃ©p táº¡o ra vÄƒn báº£n Ä‘a dáº¡ng.

4. **GPT-3:**
- Sau thÃ nh cÃ´ng cá»§a viá»‡c má»Ÿ rá»™ng GPT lÃªn GPT-2, má»™t phÃ¢n tÃ­ch sÃ¢u sáº¯c vá» hÃ nh vi cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ á»Ÿ cÃ¡c quy mÃ´ khÃ¡c nhau Ä‘Ã£ tiáº¿t lá»™ ráº±ng cÃ³ nhá»¯ng quy luáº­t Ä‘Æ¡n giáº£n Ä‘iá»u chá»‰nh má»‘i quan há»‡ giá»¯a tÃ i nguyÃªn tÃ­nh toÃ¡n, kÃ­ch thÆ°á»›c táº­p dá»¯ liá»‡u, kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯. ÄÆ°á»£c truyá»n cáº£m há»©ng tá»« nhá»¯ng hiá»ƒu biáº¿t nÃ y, GPT-2 Ä‘Ã£ Ä‘Æ°á»£c má»Ÿ rá»™ng lÃªn 100 láº§n Ä‘á»ƒ táº¡o ra [GPT-3](https://arxiv.org/abs/2005.14165) vá»›i 175 tá»· tham sá»‘.
- NgoÃ i kháº£ nÄƒng táº¡o ra cÃ¡c Ä‘oáº¡n vÄƒn báº£n cá»±c ká»³ thá»±c táº¿, mÃ´ hÃ¬nh nÃ y cÃ²n thá»ƒ hiá»‡n kháº£ nÄƒng há»c few-shot (few-shot learning): vá»›i má»™t vÃ i vÃ­ dá»¥ cá»§a má»™t tÃ¡c vá»¥ má»›i nhÆ° dá»‹ch vÄƒn báº£n thÃ nh mÃ£, mÃ´ hÃ¬nh cÃ³ thá»ƒ thá»±c hiá»‡n nhiá»‡m vá»¥ trÃªn cÃ¡c vÃ­ dá»¥ má»›i. OpenAI chÆ°a cÃ´ng khai mÃ£ nguá»“n cá»§a mÃ´ hÃ¬nh nÃ y nhÆ°ng cung cáº¥p giao diá»‡n thÃ´ng qua API OpenAI.

5. **GPT-Neo/GPT-J-6B:**
- [GPT-Neo](https://zenodo.org/records/5297715) vÃ  [GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax) lÃ  cÃ¡c mÃ´ hÃ¬nh tÆ°Æ¡ng tá»± GPT Ä‘Æ°á»£c huáº¥n luyá»‡n bá»Ÿi EleutherAI, má»™t nhÃ³m cÃ¡c nhÃ  nghiÃªn cá»©u nháº±m tÃ¡i táº¡o vÃ  phÃ¡t hÃ nh cÃ¡c mÃ´ hÃ¬nh quy mÃ´ GPT-3. CÃ¡c mÃ´ hÃ¬nh hiá»‡n táº¡i lÃ  cÃ¡c biáº¿n thá»ƒ nhá» hÆ¡n cá»§a mÃ´ hÃ¬nh 175 tá»· tham sá»‘ Ä‘áº§y Ä‘á»§, vá»›i 1.3, 2.7 vÃ  6 tá»· tham sá»‘, vÃ  cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh GPT-3 nhá» hÆ¡n do OpenAI cung cáº¥p.

- NhÃ¡nh cuá»‘i cÃ¹ng trong "cÃ¢y tiáº¿n hÃ³a" cá»§a transformer lÃ  cÃ¡c mÃ´ hÃ¬nh encoder-decoder. HÃ£y cÃ¹ng xem qua.


### The Encoder-Decoder Branch

- Máº·c dÃ¹ viá»‡c xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng bá»™ mÃ£ hÃ³a (encoder) hoáº·c bá»™ giáº£i mÃ£ (decoder) Ä‘Ã£ trá»Ÿ nÃªn phá»• biáº¿n, nhÆ°ng váº«n cÃ³ má»™t sá»‘ biáº¿n thá»ƒ encoder-decoder cá»§a kiáº¿n trÃºc Transformer cÃ³ á»©ng dá»¥ng má»›i trong cáº£ lÄ©nh vá»±c NLU (Hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn) vÃ  NLG (Sinh ngÃ´n ngá»¯ tá»± nhiÃªn):

1. **T5:**
- MÃ´ hÃ¬nh [T5](https://arxiv.org/abs/1910.10683) thá»‘ng nháº¥t cáº£ NLU vÃ  NLG báº±ng cÃ¡ch chuyá»ƒn Ä‘á»•i chÃºng thÃ nh cÃ¡c tÃ¡c vá»¥ text-to-text. Táº¥t cáº£ cÃ¡c tÃ¡c vá»¥ Ä‘á»u Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng dÆ°á»›i dáº¡ng tÃ¡c vá»¥ seq-to-seq, nÆ¡i kiáº¿n trÃºc encoder-decoder lÃ  tá»± nhiÃªn. VÃ­ dá»¥, Ä‘á»‘i vá»›i cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n, Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  vÄƒn báº£n Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘áº§u vÃ o cá»§a bá»™ mÃ£ hÃ³a vÃ  bá»™ giáº£i mÃ£ pháº£i táº¡o ra nhÃ£n dÆ°á»›i dáº¡ng vÄƒn báº£n thÃ´ng thÆ°á»ng thay vÃ¬ má»™t lá»›p. Kiáº¿n trÃºc T5 sá»­ dá»¥ng kiáº¿n trÃºc Transformer gá»‘c. Vá»›i táº­p dá»¯ liá»‡u lá»›n C4, mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n báº±ng cÃ¡ch mÃ´ phá»ng ngÃ´n ngá»¯ bá»‹ che cÅ©ng nhÆ° chuáº©n SuperGLUE Ä‘á»ƒ giáº£i quyáº¿t táº¥t cáº£ cÃ¡c tÃ¡c vá»¥ text-to-text. ÄÃ¢y lÃ  mÃ´ hÃ¬nh lá»›n nháº¥t (11 tá»‰ tham sá»‘) Ä‘áº¡t state-of-the-art trÃªn hiá»u benchmarks.

2. **BART:**
- [BART](https://arxiv.org/abs/1910.13461) káº¿t há»£p cÃ¡c quy trÃ¬nh tiá»n huáº¥n luyá»‡n cá»§a BERT vÃ  GPT trong kiáº¿n trÃºc encoder-decoder. CÃ¡c chuá»—i Ä‘áº§u vÃ o tráº£i qua má»™t sá»‘ phÃ©p biáº¿n Ä‘á»•i cÃ³ thá»ƒ, tá»« che dáº¥u Ä‘Æ¡n giáº£n Ä‘áº¿n hoÃ¡n vá»‹ chuá»—i, xÃ³a token vÃ  xoay tÃ i liá»‡u. CÃ¡c Ä‘áº§u vÃ o Ä‘Ã£ Ä‘Æ°á»£c biáº¿n Ä‘á»•i nÃ y Ä‘Æ°á»£c truyá»n qua bá»™ encoder, vÃ  bá»™ decoder pháº£i tÃ¡i táº¡o láº¡i vÄƒn báº£n gá»‘c. Äiá»u nÃ y lÃ m cho mÃ´ hÃ¬nh linh hoáº¡t hÆ¡n vÃ¬ cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ cho cáº£ cÃ¡c nhiá»‡m vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLU) vÃ  táº¡o ngÃ´n ngá»¯ tá»± nhiÃªn (NLG), Ä‘áº¡t hiá»‡u suáº¥t hÃ ng Ä‘áº§u á»Ÿ cáº£ hai loáº¡i nhiá»‡m vá»¥.

2. **M2M-100:**
- ThÃ´ng thÆ°á»ng, má»™t mÃ´ hÃ¬nh dá»‹ch Ä‘Æ°á»£c xÃ¢y dá»±ng cho má»™t cáº·p ngÃ´n ngá»¯ vÃ  má»™t chiá»u dá»‹ch nháº¥t Ä‘á»‹nh. Táº¥t nhiÃªn, Ä‘iá»u nÃ y khÃ´ng má»Ÿ rá»™ng Ä‘Æ°á»£c sang nhiá»u ngÃ´n ngá»¯, vÃ  ngoÃ i ra, cÃ³ thá»ƒ cÃ³ kiáº¿n thá»©c chung giá»¯a cÃ¡c cáº·p ngÃ´n ngá»¯ cÃ³ thá»ƒ Ä‘Æ°á»£c táº­n dá»¥ng Ä‘á»ƒ dá»‹ch giá»¯a cÃ¡c ngÃ´n ngá»¯ hiáº¿m. [M2M-100](https://arxiv.org/abs/2010.11125) lÃ  mÃ´ hÃ¬nh dá»‹ch Ä‘áº§u tiÃªn cÃ³ thá»ƒ dá»‹ch giá»¯a báº¥t ká»³ cáº·p ngÃ´n ngá»¯ nÃ o trong sá»‘ 100 ngÃ´n ngá»¯. Äiá»u nÃ y cho phÃ©p dá»‹ch cháº¥t lÆ°á»£ng cao giá»¯a cÃ¡c ngÃ´n ngá»¯ hiáº¿m vÃ  Ã­t Ä‘Æ°á»£c Ä‘áº¡i diá»‡n. MÃ´ hÃ¬nh sá»­ dá»¥ng tiá»n tá»‘ token (tÆ°Æ¡ng tá»± nhÆ° token Ä‘áº·c biá»‡t [CLS]) Ä‘á»ƒ chá»‰ Ä‘á»‹nh ngÃ´n ngá»¯ nguá»“n vÃ  ngÃ´n ngá»¯ Ä‘Ã­ch.

3. **BigBird:**
- Má»™t háº¡n cháº¿ chÃ­nh cá»§a cÃ¡c mÃ´ hÃ¬nh transformer lÃ  kÃ­ch thÆ°á»›c ngá»¯ cáº£nh tá»‘i Ä‘a, do yÃªu cáº§u bá»™ nhá»› báº­c hai cá»§a cÆ¡ cháº¿ attention. [BigBird](https://arxiv.org/abs/2007.14062) giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng má»™t dáº¡ng attention thÆ°a, cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng tuyáº¿n tÃ­nh. Äiá»u nÃ y cho phÃ©p má»Ÿ rá»™ng máº¡nh máº½ kÃ­ch thÆ°á»›c ngá»¯ cáº£nh tá»« 512 token trong háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh BERT lÃªn 4.096 token trong BigBird. Äiá»u nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c trÆ°á»ng há»£p cáº§n báº£o tá»“n cÃ¡c phá»¥ thuá»™c dÃ i háº¡n, cháº³ng háº¡n nhÆ° tÃ³m táº¯t vÄƒn báº£n.

- CÃ¡c pretrained checkpoints cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á» cáº­p trong pháº§n nÃ y cÃ³ sáºµn trÃªn Hugging Face Hub vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ phÃ¹ há»£p vá»›i trÆ°á»ng há»£p sá»­ dá»¥ng cá»§a báº¡n vá»›i ğŸ¤— Transformers, nhÆ° Ä‘Ã£ mÃ´ táº£ trong blog trÆ°á»›c.

## 3. Tá»•ng Káº¿t

- Transformers lÃ  má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron máº¡nh máº½, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± báº±ng cÃ¡ch táº­n dá»¥ng cÆ¡ cháº¿ Attention, giÃºp mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a Ä‘áº§u vÃ o. Vá»›i cáº¥u trÃºc dá»±a trÃªn Encoder-Decoder, Transformers Ä‘Ã£ thay tháº¿ cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° RNN vÃ  LSTM trong nhiá»u á»©ng dá»¥ng, Ä‘áº·c biá»‡t lÃ  xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) vÃ  thá»‹ giÃ¡c mÃ¡y tÃ­nh (CV). TÃ­nh song song hÃ³a cao vÃ  kháº£ nÄƒng má»Ÿ rá»™ng vÆ°á»£t trá»™i khiáº¿n Transformers trá»Ÿ thÃ nh ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh AI tiÃªn tiáº¿n nhÆ° GPT vÃ  BERT.