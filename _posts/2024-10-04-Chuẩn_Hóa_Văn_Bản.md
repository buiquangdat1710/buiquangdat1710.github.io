---
title: "Chu·∫©n H√≥a VƒÉn B·∫£n (Text Normalization)"
date: 2024-10-04 00:00:00  + 0800
categories: [NLP]
tags: [text normalization]
---
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>

**Chu·∫©n h√≥a vƒÉn b·∫£n (Text Normalization)** l√† qu√° tr√¨nh x·ª≠ l√Ω v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n, lo·∫°i b·ªè c√°c y·∫øu t·ªë kh√¥ng c·∫ßn thi·∫øt, v√† chu·∫©n b·ªã cho c√°c b∆∞·ªõc x·ª≠ l√Ω ti·∫øp theo trong c√°c b√†i to√°n ng√¥n ng·ªØ t·ª± nhi√™n (NLP). Qu√° tr√¨nh n√†y th∆∞·ªùng bao g·ªìm c√°c b∆∞·ªõc nh∆∞ chuy·ªÉn ƒë·ªïi ch·ªØ hoa th√†nh ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè d·∫•u c√¢u, chu·∫©n h√≥a kho·∫£ng tr·∫Øng, v√† thay th·∫ø c√°c t·ª´ vi·∫øt t·∫Øt ho·∫∑c t·ª´ ƒë·ªìng nghƒ©a. Chu·∫©n h√≥a vƒÉn b·∫£n gi√∫p c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa c√°c m√¥ h√¨nh NLP b·∫±ng c√°ch gi·∫£m thi·ªÉu s·ª± ƒëa d·∫°ng kh√¥ng c·∫ßn thi·∫øt trong d·ªØ li·ªáu.

## 1. Chu·∫©n h√≥a vƒÉn b·∫£n.

- Chu·∫©n h√≥a vƒÉn b·∫£n l√† m·ªôt chu·ªói vi·ªác chuy·ªÉn vƒÉn b·∫£n sang d·∫°ng chu·∫©n, thu·∫≠n ti·ªán ƒë·ªÉ s·ª≠ d·ª•ng trong c√°c b√†i to√°n kh√°c nhau.
- C√°c quy tr√¨nh c√≥ th·ªÉ c√≥ trong c√¥ng vi·ªác chu·∫©n h√≥a vƒÉn b·∫£n:
  - **T√°ch c√¢u (sentence segmentation)**: Chia vƒÉn b·∫£n th√†nh c√°c c√¢u.
  - **T√°ch token (Tokenization)**: Chia vƒÉn b·∫£n th√†nh c√°c token.
  - **Lemmatization**: ƒê∆∞a v·ªÅ d·∫°ng t·ª´ g·ªëc.
  - **Stemming**: C·∫Øt h·∫≠u t·ªë t·ª´. √çt ƒë∆∞·ª£c s·ª≠ d·ª•ng h∆°n Lemmatization.
  - **L·ªçc stop words**: L·ªçc nh·ªØng t·ª´ hay xu·∫•t hi·ªán v√† √≠t ng·ªØ nghƒ©a nh∆∞ "the", "is", "at", "which", v√† "on". Th∆∞·ªùng l√† c√°c gi·ªõi t·ª´, l∆∞u √Ω c√≥ nh·ªØng c√¢u m√† gi·ªõi t·ª´ ƒë·ªÉ ch·ªâ v·ªã tr√≠ quan tr·ªçng th√¨ kh√¥ng ƒë∆∞·ª£c l·ªçc.
  - **S·ª≠a sai t·ª´ (Word Corection)**: Sai th·ª© t·ª± ch·ªØ trong t·ª´ Ti·∫øng Anh ho·∫∑c sai d·∫•u trong Ti·∫øng Vi·ªát.

- Chu·∫©n h√≥a vƒÉn b·∫£n kh√°c nhau gi·ªØa c√°c b√†i to√°n kh√°c nhau. V√≠ d·ª• nh∆∞ b√†i to√°n sinh t·ª´ (Text Generation) th√¨ chu·∫©n h√≥a s·∫Ω gi·ªØ njieuef token nh·∫•t c√≥ th·ªÉ, ƒë∆∞a vƒÉn b·∫£n v·ªÅ chung m·ªôt format, nh∆∞ l√πi ƒë·∫ßu d√≤ng, vi·∫øt hoa ƒë·∫ßu c√¢u. M·ªôt v√≠ d·ª• kh√°c l√† b√†i to√°n ph√¢n lo·∫°i c·∫£m x√∫c (Sentiment Classification) th√¨ chu·∫©n h√≥a s·∫Ω lo·∫°i b·ªè nh·ªØng stop-words nh∆∞ a, to,... v√† gi·ªØ l·∫°i c√°c bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c nh∆∞ :), :D, =)).

## 2. Lemmatization.
- Lemmatization l√† vi·ªác x√°c ƒë·ªãnh t·ª´ g·ªëc c·ªßa c√°c t·ª´, v√≠ d·ª± nh∆∞ c√°c t·ª´ say, said, saying c√≥ t·ª´ g·ªëc l√† say. Ta s·∫Ω n√≥i qua v·ªÅ ∆∞u ƒëi·ªÉm v√† nh∆∞·ª£c ƒëi·ªÉm c·ªßa Lemmatization.
- ∆Øu ƒëi·ªÉm:
  - **T√¨m ki·∫øm t·ªët h∆°n**: Khi ng∆∞·ªùi d√πng t√¨m ki·∫øm vƒÉn b·∫£n t·ª´ sing, thu·∫≠t to√°n c√≥ th·ªÉ c√πng t√¨m ki·∫øm th√™m t·ª´ sang, sung.
  - **Ph√¢n lo·∫°i t·ªët h∆°n**: Chu·∫©n h√≥a v·ªÅ t·ª´ g·ªëc gi√∫p thu h·∫πp kh√¥ng gian ph√¢n t√≠ch v√† t·∫°o ra ƒë·ªô ch√≠nh x√°c cao h∆°n.
- Nh∆∞·ª£c ƒëi·ªÉm:
  - **ƒê√°nh m·∫•t th√¥ng tin ng·ªØ ph√°p**: N·∫øu b·ªô d·ªØ li·ªáu c√≥ s·ª± m·∫≠p m·ªù l·ªõn th√¨ vi·ªác x·ª≠ l√Ω n√†y s·∫Ω ƒë√°nh m·∫•t th√¥ng tin ng·ªØ ph√°p l√†m gi·∫£m ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh.
  - Gi·ªù ta s·∫Ω l·∫≠p tr√¨nh Lemmatization s·ª≠ d·ª•ng c√°c th∆∞ vi·ªán kh√°c nhau, ƒë·∫ßu ti√™n h√£y th·ª≠ l·∫≠p tr√¨nh v·ªõi th∆∞ vi·ªán nltk (Natural Language Toolkit).

```python
import nltk
nltk.download('wordnet')  # T·∫£i v·ªÅ b·ªô t·ª´ ƒëi·ªÉn WordNet, c·∫ßn thi·∫øt cho lemmatization
from nltk.stem import WordNetLemmatizer  # Import l·ªõp WordNetLemmatizer t·ª´ th∆∞ vi·ªán nltk

# T·∫°o ƒë·ªëi t∆∞·ª£ng WordNetLemmatizer
wnl = WordNetLemmatizer()

# V√≠ d·ª• v·ªÅ lemmatization (chuy·ªÉn t·ª´ v·ªÅ d·∫°ng g·ªëc) cho t·ª´ng t·ª´
list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',  # Danh s√°ch t·ª´ c·∫ßn lemmatize
         'driving', 'died', 'tried', 'feet']

# V√≤ng l·∫∑p qua t·ª´ng t·ª´ trong list1 ƒë·ªÉ lemmatize
for words in list1:
    print(words + " ---> " + wnl.lemmatize(words))  # In ra t·ª´ g·ªëc c·ªßa t·ª´ng t·ª´

```

> kites ---> kite

> babies ---> baby

>dogs ---> dog

>flying ---> flying

>smiling ---> smiling

>driving ---> driving

>died ---> died

>tried ---> tried

>feet ---> foot

- Gi·ªù ta h√£y th·ª≠ s·ª≠ d·ª•ng th∆∞ vi·ªán textblob:

```python
from textblob import TextBlob, Word  # Import th∆∞ vi·ªán TextBlob v√† l·ªõp Word

my_word = 'cats'  # T·ª´ c·∫ßn lemmatize (d·∫°ng s·ªë nhi·ªÅu)

# T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng Word
w = Word(my_word)

print(w.lemmatize())  # Lemmatize t·ª´ 'cats', k·∫øt qu·∫£ l√† 'cat'
#> cat

sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'  
# C√¢u v√≠ d·ª• ch·ª©a nhi·ªÅu t·ª´ d·∫°ng s·ªë nhi·ªÅu v√† ƒë·ªông t·ª´

s = TextBlob(sentence)  # T·∫°o ƒë·ªëi t∆∞·ª£ng TextBlob t·ª´ c√¢u ƒë√£ cho

# Lemmatize t·ª´ng t·ª´ trong c√¢u v√† k·∫øt h·ª£p l·∫°i th√†nh chu·ªói ho√†n ch·ªânh
lemmatized_sentence = " ".join([w.lemmatize() for w in s.words])

print(lemmatized_sentence)  # In ra c√¢u ƒë√£ ƒë∆∞·ª£c lemmatize

```
> cat

> the bat saw the cat with stripe hanging upside down by their foot

- C√≥ v·∫ª th∆∞ vi·ªán textblob ch·ªâ chuy·ªÉn c√°c t·ª´ s·ªë nhi·ªÅu v·ªÅ d·∫°ng s·ªë √≠t. Gi·ªù ch√∫ng ta h√£y th·ª≠ l·∫≠p tr√¨nh v·ªõi th∆∞ vi·ªán Spacy:

```python
import spacy  # Import th∆∞ vi·ªán spaCy
nlp = spacy.load('en_core_web_sm')  # T·∫£i m√¥ h√¨nh ng√¥n ng·ªØ ti·∫øng Anh 'en_core_web_sm'

# T·∫°o ƒë·ªëi t∆∞·ª£ng Doc t·ª´ chu·ªói ƒë·∫ßu v√†o
doc = nlp(u'the bats saw the cats with best stripes hanging upside down by their feet')

# T·∫°o danh s√°ch c√°c token t·ª´ chu·ªói ƒë√£ cho
tokens = []
for token in doc:
    tokens.append(token)  # Th√™m t·ª´ng token v√†o danh s√°ch

print(tokens)
#> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]
# In ra danh s√°ch c√°c token (t·ª´) ƒë√£ t√°ch t·ª´ chu·ªói ƒë·∫ßu v√†o

# Lemmatize t·ª´ng token trong c√¢u v√† n·ªëi l·∫°i th√†nh c√¢u ho√†n ch·ªânh
lemmatized_sentence = " ".join([token.lemma_ for token in doc])

print(lemmatized_sentence)
```

> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]

> the bat see the cat with good stripe hang upside down by their foot

- C√≥ th·ªÉ th·∫•y th∆∞ vi·ªán Spacy ƒë√£ chuy·ªÉn ƒë∆∞·ª£c c√°c t·ª´ s·ªë nhi·ªÅu v·ªÅ d·∫°ng s·ªë √≠t, c√†ng t·ª´ ·ªü th√¨ qu√° kh·ª© v·ªÅ hi·ªán t·∫°i, t·ª´ so s√°nh nh·∫•t v·ªÅ d·∫°ng g·ªëc.

- Gi·ªù ch√∫ng ta h√£y th·ª≠ so s√°nh vi·ªác s·ª≠ d·ª•ng Lemmatization v√† kh√¥ng s·ª≠ d·ª•ng Lemmatization v·ªõi b√†i to√°n ph√¢n lo·∫°i c·∫£m x√∫c b√¨nh lu·∫≠n phim. Tr∆∞·ªõc ti√™n h√£y import c√°c th·ª© vi·ªán c·∫ßn thi·∫øt:

```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
import pandas as pd
```
- Ti·∫øp theo ta s·∫Ω t·∫£i b·ªô d·ªØ li·ªáu c√°c b√¨nh lu·∫≠n v·ªÅ phim:

`wget https://storage.googleapis.com/protonx-cloud-storage/datasets/IMDB%20Dataset.csv`

- Ti·∫øp theo, ta s·∫Ω m·ªü d·ªØ li·ªáu d∆∞·ªõi d·∫°ng pandas:

```python
df = pd.read_csv('/content/IMDB Dataset.csv')
print(df.iloc()[0]['review'])
```

> One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side

- Gi·ªù ch√∫ng ta h√£y l·∫≠p tr√¨nh h√†m x·ª≠ l√Ω Lemmatization v√† Stop Words:

```python
lemmatizer = WordNetLemmatizer()  # T·∫°o ƒë·ªëi t∆∞·ª£ng lemmatizer t·ª´ WordNetLemmatizer ƒë·ªÉ lemmatize c√°c t·ª´
stop_words = set(stopwords.words('english'))  # T·∫°o danh s√°ch c√°c stop words (t·ª´ d·ª´ng) ti·∫øng Anh

def preprocess_text(text):
    # Tokenize vƒÉn b·∫£n (t√°ch vƒÉn b·∫£n th√†nh c√°c t·ª´ ri√™ng l·∫ª)
    tokens = nltk.word_tokenize(text)
    # Lemmatize c√°c t·ª´ v√† lo·∫°i b·ªè c√°c stop words
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
    # K·∫øt h·ª£p l·∫°i c√°c t·ª´ ƒë√£ lemmatize th√†nh m·ªôt chu·ªói
    return ' '.join(lemmatized_tokens)

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho t·ª´ng ƒë√°nh gi√° (review) trong c·ªôt 'review' c·ªßa DataFrame
df['processed_review_with_lemmatization'] = df['review'].apply(preprocess_text)
```

- Gi·ªù h√£y l·∫≠p tr√¨nh m·ªôt h√†m ch·ªâ x·ª≠ l√Ω Stop Words:

```python
def preprocess_text_no_lemmatization(text):
    # Tokenize the text
    tokens = nltk.word_tokenize(text)
    # Lemmatize and remove stop words
    clean_tokens = [token for token in tokens if token not in stop_words]
    # Re-join tokens into a string
    return ' '.join(clean_tokens)
df['processed_review_no_lemmatization'] = df['review'].apply(preprocess_text_no_lemmatization)
```

- Gi·ªù h√£y t·∫°o m·ªôt model ƒë∆°n gi·∫£n ƒë·ªÉ train v·ªõi h√†m x·ª≠ l√Ω Lemmatization v√† xem k·∫øt qu·∫£:

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from sklearn.model_selection import train_test_split

# Assuming df['processed_review'] and df['sentiment'] are your data
X = df['processed_review_with_lemmatization']
y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert sentiment to binary

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize text
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# Convert text to sequences and pad them to ensure uniform length
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequences_train, maxlen=200, truncating='post', padding='post')
padded_test = pad_sequences(sequences_test, maxlen=200, truncating='post', padding='post')

# Define the model
model_with_lemma = Sequential([
    Embedding(input_dim=5000, output_dim=16, input_length=200),
    Bidirectional(LSTM(64)),
    Dense(24, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model_with_lemma.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model_with_lemma.fit(padded_train, y_train, epochs=10, validation_data=(padded_test, y_test))

# Evaluate the model
loss, accuracy = model_with_lemma.evaluate(padded_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

```

> Loss: 0.4142063558101654, Accuracy: 0.8694000244140625

- Gi·ªù h√£y th·ª≠ train v·ªõi m√¥ h√¨nh kh√¥ng s·ª≠ d·ª•ng Lemmatization:

```python
# Assuming df['processed_review'] and df['sentiment'] are your data
X = df['processed_review_no_lemmatization']
y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert sentiment to binary

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize text
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# Convert text to sequences and pad them to ensure uniform length
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequences_train, maxlen=200, truncating='post', padding='post')
padded_test = pad_sequences(sequences_test, maxlen=200, truncating='post', padding='post')

# Define the model
model_with_no_lemma = Sequential([
    Embedding(input_dim=5000, output_dim=16, input_length=200),
    Bidirectional(LSTM(64)),
    Dense(24, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model_with_no_lemma.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model_with_no_lemma.fit(padded_train, y_train, epochs=10, validation_data=(padded_test, y_test))

# Evaluate the model
loss, accuracy = model_with_no_lemma.evaluate(padded_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')
```

> Loss: 0.4729436933994293, Accuracy: 0.8672999739646912

- K·∫øt lu·∫≠n: Vi·ªác s·ª≠ d·ª•ng Lemmatization c√≥ k·∫øt qu·∫£ t·ªët tr√™n t·ª´ng b√†i to√°n kh√°c nhau. Vi·ªác s·ª≠ d·ª•ng c√≥ th·ªÉ l√†m m·∫•t ƒëi th√¥ng tin v·ªÅ ng·ªØ ph√°p
- C√°c b·∫°n c√≥ th·ªÉ xem 9 c√°ch l·∫≠p tr√¨nh Lemmatization t·∫°i ƒë√¢y: [Lemmatization](https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/).

## 3. Chia c√¢u (Sentence Segmentation).

- Sentence Segmentation l√† vi·ªác chia vƒÉn b·∫£n th√†nh c√¢u, vi·ªác chia n√†y c√≥ th·ªÉ d·ª±a v√†o d·∫•u ch·∫•m (.), h·ªèi ch·∫•m (?), ch·∫•m than (!). V·∫•n ƒë·ªÅ kh√≥ khƒÉn x·∫£y ra nh∆∞ t·ª´ vi·∫øt t·∫Øt trong Ti·∫øng Anh s·ª≠ d·ª•ng d·∫•u ch·∫•m. V√≠ d·ª• nh∆∞ Mr. hay Mrs.
- Gi·ªù ch√∫ng ta h√£y th·ª≠ l·∫≠p tr√¨nh Sentence Segmentation b·∫±ng c√°c th∆∞ vi·ªán kh√°c nhau. ƒê·∫ßu ti√™n l√† th∆∞ vi·ªán nltk:

```python
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
s = '''Good muffins cost $3.88\nin New York.  Please buy me two of them.\n\nThanks! Who'''
print(sent_tokenize(s))
```

>['Good muffins cost $3.88\nin New York.',

>'Please buy me two of them.',

>'Thanks!',

>'Who']

- Ti·∫øp theo, th·ª≠ l·∫≠p tr√¨nh b·∫±ng th∆∞ vi·ªán Spacy:

```python
import spacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")

# The text you want to segment into sentences
text = "This is a sentence. This is another one."

# Process the text
doc = nlp(text)

# Iterate over the sentences
for sent in doc.sents:
    print(sent.text)
```

> This is a sentence.
> This is another one.

- Ti·∫øp theo,ta s·∫Ω s·ª≠ d·ª•ng th∆∞ vi·ªán underthesea ƒë·ªÉ x·ª≠ l√Ω Ti·∫øng Vi·ªát. Tr∆∞·ªõc ti√™n ta c·∫ßn ph·∫£i t·∫£i th∆∞ vi·ªán underthesea:

`pip install underthesea`

```python
from underthesea import sent_tokenize
text = 'B·∫°n l√† h∆∞·ªõng n·ªôi hay h∆∞·ªõng ngo·∫°i? T√¥i h∆∞·ªõng lung tung. Mr. and Mrs'
sent_tokenize(text)
```

> ['B·∫°n l√† h∆∞·ªõng n·ªôi hay h∆∞·ªõng ngo·∫°i?', 'T√¥i h∆∞·ªõng lung tung.', 'Mr. and Mrs']

## 4. L·ªçc Stop Words.

- ƒê·∫ßu ti√™n, ta s·∫Ω l·∫≠ tr√¨nh l·ªçc Stop Words b·∫±ng th∆∞ vi·ªán nltk:

```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```
- Ti·∫øp theo ta s·∫Ω kh·ªüi t·∫°o c√°c t·ª´ stop words ti·∫øng anh v√† chia token c√¢u ƒë·∫ßu v√†o, ta s·∫Ω ch·ªâ l·∫•y c√°c token kh√¥ng ph·∫£i l√† stop words:

```python
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Sample text
text = "This is an example sentence. However, it contains stop words!"
stop_words = set(stopwords.words('english'))


word_tokens = word_tokenize(text)


filtered_text = [word for word in word_tokens if not word in stop_words]

print("Original text:", text)
print("Filtered text:", " ".join(filtered_text))
```

> Original text: This is an example sentence. However, it contains stop words!

> Filtered text: This example sentence . However , contains stop words !
- Ti·∫øp theo, ta s·∫Ω s·ª≠ d·ª•ng th∆∞ vi·ªán Spacy:

```python
import spacy

# Load the language model
nlp = spacy.load("en_core_web_sm")

text = "This is an example sentence... However, it contains stop words!"
doc = nlp(text)

filtered_text = [token.text for token in doc if not token.is_stop]

print("Original text:", text)
print("Filtered text:", " ".join(filtered_text))
```

> Original text: This is an example sentence... However, it contains stop words!

> Filtered text: example sentence ... , contains stop words !

- Oke, gi·ªù ta h√£y th·ª≠ hu·∫•n luy·ªán gi·ªØa hai m√¥ h√¨nh, m·ªôt m√¥ h√¨nh s·ª≠ d·ª•ng l·ªçc stop words v√† m·ªôt m√¥ h√¨nh kh√¥ng s·ª≠ d·ª•ng:

```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
import pandas as pd
```

- T·∫£i b·ªô d·ªØ li·ªáu ƒë√°nh gi√° phim gi·ªëng ph·∫ßn tr√™n:

`wget https://storage.googleapis.com/protonx-cloud-storage/datasets/IMDB%20Dataset.csv`

- Load d·ªØ li·ªáu d∆∞·ªõi d·∫°ng pandas:
  
```python
df = pd.read_csv('/content/IMDB Dataset.csv')
```

- L·∫≠p tr√¨nh hai h√†m x·ª≠ l√Ω, m·ªôt h√†m l·ªçc stop words v√† m·ªôt h√†m kh√¥ng l√†m g√¨:

```python
stop_words = set(stopwords.words('english'))

def preprocess_text_with_stopwords_filtering(text):
    # Tokenize the text
    tokens = nltk.word_tokenize(text)
    # Lemmatize and remove stop words
    lemmatized_tokens = [token for token in tokens if token not in stop_words]
    # Re-join tokens into a string
    return ' '.join(lemmatized_tokens)

def preprocess_text_with_no_stopwords_filtering(text):
    # Tokenize the text
    tokens = nltk.word_tokenize(text)
    # Lemmatize and remove stop words
    lemmatized_tokens = [token for token in tokens]
    # Re-join tokens into a string
    return ' '.join(lemmatized_tokens)


# Apply preprocessing to each review
df['processed_review_with_stopwords_filtering'] = df['review'].apply(preprocess_text_with_stopwords_filtering)
df['processed_review_with_no_stopwords_filtering'] = df['review'].apply(preprocess_text_with_no_stopwords_filtering)
```

- Th·ª≠ nghi·ªám hu·∫•n luy·ªán m√¥ h√¨nh s·ª≠ d·ª•ng Stopwords Filtering:

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from sklearn.model_selection import train_test_split

# Assuming df['processed_review'] and df['sentiment'] are your data
X = df['processed_review_with_stopwords_filtering']
y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert sentiment to binary




# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize text
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# Convert text to sequences and pad them to ensure uniform length
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequences_train, maxlen=200, truncating='post', padding='post')
padded_test = pad_sequences(sequences_test, maxlen=200, truncating='post', padding='post')
# Define the model
model_with_stopwords_filtering = Sequential([
    Embedding(input_dim=5000, output_dim=16, input_length=200),
    Bidirectional(LSTM(64)),
    Dense(24, activation='relu'),
    Dense(1, activation='sigmoid')
])


# Compile the model
model_with_stopwords_filtering.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model_with_stopwords_filtering.fit(padded_train, y_train, epochs=10, validation_data=(padded_test, y_test))

# Evaluate the model
loss, accuracy = model_with_stopwords_filtering.evaluate(padded_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

```

> Loss: 0.3831138610839844, Accuracy: 0.8629000186920166



- Th·ª≠ nghi·ªám hu·∫•n luy·ªán m√¥ h√¨nh kh√¥ng s·ª≠ d·ª•ng Stopwords Filtering:

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from sklearn.model_selection import train_test_split

# Assuming df['processed_review'] and df['sentiment'] are your data
X = df['processed_review_with_no_stopwords_filtering']
y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert sentiment to binary

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize text
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# Convert text to sequences and pad them to ensure uniform length
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequences_train, maxlen=200, truncating='post', padding='post')
padded_test = pad_sequences(sequences_test, maxlen=200, truncating='post', padding='post')

# Define the model
model_with_no_stopwords_filtering = Sequential([
    Embedding(input_dim=5000, output_dim=16, input_length=200),
    Bidirectional(LSTM(64)),
    Dense(24, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model_with_no_stopwords_filtering.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model_with_no_stopwords_filtering.fit(padded_train, y_train, epochs=10, validation_data=(padded_test, y_test))

# Evaluate the model
loss, accuracy = model_with_no_stopwords_filtering.evaluate(padded_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

```

> Loss: 0.4349353015422821, Accuracy: 0.8571000099182129

- K·∫øt lu·∫≠n trong tr∆∞·ªùng h·ª£p n√†y s·ª≠ d·ª•ng Stopwords Filtering cho k·∫øt qu·∫£ t·ªët h∆°n.

## 5. S·ª≠a T·ª´.
- S·ª≠a t·ª´ c≈©ng l√† m·ªôt b√†i to√°n quan tr·ªçng trong chu·∫©n h√≥a vƒÉn b·∫£n hay ƒë·ªÅ xu·∫•t tin nh·∫Øn. V√≠ d·ª• nh∆∞ t·ª´ happpy c√≥ th·ªÉ s·ª≠a l·∫°i th√†nh happy, t·ª´ azmaing c√≥ th·ªÉ s·ª≠a l·∫°i th√†nh amazing, t·ª´ intelliengt c√≥ th·ªÉ s·ª≠a l·∫°i th√†nh intelligent. Th·ª±c ra ph·∫ßn n√†y m√¨nh th·∫•y thi√™n v·ªÅ thu·∫≠t to√°n h∆°n l√† AI v√¨ ta s·∫Ω t√≠nh kho·∫£ng c√°ch c·ªßa t·ª´ c·∫ßn s·ª≠a v·ªõi c√°c t·ª´ trong t·ª´ ƒëi·ªÉn, t·ª´ n√†o m√† kho·∫£ng c√°ch g·∫ßn nh·∫•t v·ªõi t·ª´ c·∫ßn s·ª≠a th√¨ l√† t·ª´ ƒë√∫ng. Gi·ªù m√¨nh s·∫Ω tr√¨nh b√†y hai c√°ch t√≠nh kho·∫£ng c√°ch ch√≠nh l√†: **Kho·∫£ng c√°ch JCard** v√† kho·∫£ng c√°ch **Edit Distance**.

### Kho·∫£ng c√°ch JCard.
- Tr∆∞·ªõc ti√™n ta s·∫Ω n√≥i v·ªÅ **h·ªá s·ªë Jaccard**, ƒë√¢y l√† h·ªá s·ªë ƒë·ªÉ ƒëo m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c t·∫≠p m·∫´u h·ªØu h·∫°n v√† ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† k√≠ch th∆∞·ªõc c·ªßa giao ƒëi·ªÉm chia cho k√≠ch th∆∞·ªõc c·ªßa h·ª£p c√°c t·∫≠p m·∫´u:

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
$$

- Theo c√¥ng th·ª©c th√¨ $0 \leq J(A,B) \leq 1$. N·∫øu giao ƒëi·ªÉm A v√† B r·ªóng th√¨ $J(A,B) = 0$. **Kho·∫£ng c√°ch Jaccard** l√† kho·∫£ng c√°ch ƒëo s·ª± kh√°c bi·ªát gi·ªØa c√°c t·∫≠p m·∫´u, thu ƒë∆∞·ª£c b·∫±ng c√°ch tr·ª´ h·ªá s·ªë Jaccard kh·ªèi 1:

$$
d_J(A, B) = 1 - J(A, B) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|}
$$

- $d_J(A, B)$ c√†ng nh·ªè th√¨ s·ª± kh√°c bi·ªát gi·ªØa hai t·∫≠p h·ª£p A, B c√†ng nh·ªè, ng∆∞·ª£c l·∫°i, $d_J(A, B)$ c√†ng l·ªõn th√¨ s·ª± kh√°c bi·ªát gi·ªØa hai t·∫≠p h·ª£p A,B c√†ng l·ªõn.

- Hai t·∫≠p A,B c√≥ th·ªÉ ƒë·ªãnh nghƒ©a l√† s·ªë k√≠ t·ª± ri√™ng bi·ªát trong m·ªôt t·ª´, ho·∫∑c l√† c√°c c·∫∑p 2 k√≠ t·ª± li√™n ti·∫øp trong m·ªôt t·ª´,... C√°i n√†y l√† do b·∫°n t·ª± ƒë·ªãnh nghƒ©a.
- Gi·ªù h√£y th·ª≠ ƒë·ªãnh nghƒ©a A,B l√† c√°c c·∫∑p 2 k√≠ t·ª± li√™n ti·∫øp trong m·ªôt t·ª´, ta s·∫Ω ph·∫ßn t√≠ch hai t·ª´ l√† hello v√† elhlo. T·ª´ hello ƒë∆∞·ª£c ph√¢n t√≠ch th√†nh t·∫≠p h·ª£p A = [he,el,ll,lo], t·ª´ elhlo ƒë∆∞·ª£c ph√¢n t√≠ch th√†nh t·∫≠p h·ª£p B = [el,lh,hl,lo]. Hai t·∫≠p h·ª£p giao nhau s·∫Ω ra 2 ph·∫ßn t·ª≠:

$$
|A \cap B| = 2
$$

- Hai t·∫≠p h·ª£p n√†y h·ª£p nhau s·∫Ω th√†nh 6 ph·∫ßn t·ª≠:

$$
|A \cup B| = 2
$$

- Kho·∫£ng c√°ch Jcard:

$$
d_J(A, B) = 1 - J(A, B) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|} = \frac{2}{3}
$$

- Gi·ªù ta s·∫Ω th·ª≠ l·∫≠p tr√¨nh kho·∫£ng c√°ch JCard d·ª±a v√†o t·ª´ng k√≠ t·ª±:

```python
def jaccard_distance(str1, str2):
    # Convert the strings into sets of characters
    set1 = set(str1)
    set2 = set(str2)

    # Calculate intersection and union
    intersection = set1.intersection(set2)
    union = set1.union(set2)

    # Calculate Jaccard distance
    distance = 1 - (len(intersection) / len(union))

    return distance

# Example usage
str1 = "hello"
str2 = "yellow"
distance = jaccard_distance(str1, str2)
print(f"Jaccard Distance: {distance}")

```

> Jaccard Distance: 0.5

- Th√™m m·ªôt v√≠ d·ª• n·ªØa:

```python
str3 = "azmaing"
str4 = "amazing"
distance = jaccard_distance(str3, str4)
print(f"Jaccard Distance: {distance}")
```

> Jaccard Distance: 0.0

- Gi·ªù h√£y th·ª≠ l·∫≠p tr√¨nh b·∫±ng c√°ch chia ra c√°c c·ª•m k√≠ t·ª±:

```python
def jaccard_distance_ngrams(str1, str2, n=2):
    # Function to generate n-grams from a given string
    def generate_ngrams(s, n):
        ngrams = set()
        for i in range(len(s) - n + 1):
            ngrams.add(s[i:i+n])
        return ngrams

    # Generate n-grams for both strings
    ngrams1 = generate_ngrams(str1, n)
    ngrams2 = generate_ngrams(str2, n)

    # Calculate intersection and union of n-grams sets
    intersection = ngrams1.intersection(ngrams2)
    union = ngrams1.union(ngrams2)

    # Calculate Jaccard distance
    distance = 1 - (len(intersection) / len(union))

    return distance

# Calculate Jaccard distance for 'hello' and 'hlleo' using 2-grams
str1 = "hello"
str2 = "hlleo"
distance = jaccard_distance_ngrams(str1, str2, 2)
distance

```

> 0.8571428571428572

- H√£y l·∫•y th√™m m·ªôt v√≠ d·ª• n·ªØa:

```python
str1 = "hello"
str2 = "elhlo"
distance = jaccard_distance_ngrams(str1, str2, 2)
distance
```

> 0.6666666666666667

- Ti·∫øp theo ta s·∫Ω ƒë·∫øn v·ªõi m·ªôt kho·∫£ng c√°ch n·ªØa l√† **Edit Distance**.

### Edit Distance

- **Edit Distance** hay c√≤n g·ªçi l√† **Levenshtein distance** ch·ªâ s·ª± ƒë√¥ l∆∞·ªùng t·ªïng s·ªë ph√©p nh·ªè nh·∫•t c·∫ßn th·ª±c hi·ªán ƒë·ªÉ bi·∫øn chu·ªói A th√†nh chu·ªói B. C√°c ph√©p bi·∫øn bao g·ªìm ba ph√©p: Ph√©p ch√®n (Insertion), Ph√©p x√≥a (Deletion), Ph√©p thay th·∫ø (Substitution).
- V√≠ d·ª± nh∆∞ t·ª´ kitten v√† t·ª´ sitting c√≥ Edit distance b·∫±ng 3 v√¨ c·∫ßn √≠t nh·∫•t 3 ph√©p ƒë·ªÉ chuy·ªÉn kitten th√†nh sitting. ƒê·∫ßu ti√™n thay k√≠ t·ª± k th√†nh s, sau ƒë√≥ thay k√≠ t·ª± e b·∫±ng i, th√™m k√≠ t·ª± g ·ªü cu·ªëi.

- Ch√∫ng ta s·∫Ω n√≥i qua v·ªÅ quy tr√¨nh s·ª≠a t·ª´. ƒê·∫ßu ti√™n ch√∫ng ta c·∫ßn l·∫∑p qua c√°c t·ª´ trong t·ª´ ƒëi·ªÉn v√† t√≠nh kho·∫£ng c√°ch c·ªßa c√°c t·ª´ n√†y v·ªõi t·ª´ m·ª•c ti√™u. T·ª´ n√†o trong t·ª´ ƒëi·ªÉn c√≥ kho·∫£ng c√°ch ng·∫Øn nh·∫•t th√¨ ƒë·ªÅ xu·∫•t t·ª´ ƒë√≥ l√† t·ª´ thay th·∫ø. C√≥ th·ªÉ t·ªëi ∆∞u h√≥a th·ªùi gian t√¨m ki·∫øm b·∫±ng c√°ch ch·ªâ l·∫∑p qua c√°c t·ª´ c√≥ ƒë·ªô d√†i b·∫±ng t·ª´ m·ª•c ti√™u, hay ch·ªâ l·∫∑p qua c√°c t·ª´ c√≥ c√°c k√≠ t·ª± gi·ªëng v·ªõi t·ª´ m·ª•c ti√™u,...
- Ch√∫ng ta c√≥ th·ªÉ l·∫≠p tr√¨nh Edit Distance b·∫±ng quy ho·∫°ch ƒë·ªông:

```python
def edit_distance(str1, str2):
    # Create a table to store results of subproblems
    dp = [[0 for x in range(len(str2) + 1)] for x in range(len(str1) + 1)]

    # Fill dp[][] in bottom up manner
    for i in range(len(str1) + 1):
        for j in range(len(str2) + 1):

            # If first string is empty, insert all characters of second string
            if i == 0:
                dp[i][j] = j

            # If second string is empty, remove all characters of the first string
            elif j == 0:
                dp[i][j] = i

            # If last characters are the same, ignore the last char
            # and recur for remaining string
            elif str1[i-1] == str2[j-1]:
                dp[i][j] = dp[i-1][j-1]

            # If last character is different, consider all possibilities
            # and find minimum
            else:
                dp[i][j] = 1 + min(dp[i][j-1],        # Insert
                                   dp[i-1][j],        # Remove
                                   dp[i-1][j-1])      # Replace

    return dp[len(str1)][len(str2)]

# Example usage
str1 = "hello"
str2 = "elhlo"
print(f"Edit Distance between '{str1}' and '{str2}':", edit_distance(str1, str2))

```

## 6. Gi·ªõi thi·ªáu th∆∞ vi·ªán Underthesea.

- üåä B·ªô c√¥ng c·ª• NLP ti·∫øng Vi·ªát - Underthesea l√† m·ªôt b·ªô c√¥ng c·ª• b·∫±ng ng√¥n ng·ªØ Python m√£ ngu·ªìn m·ªü bao g·ªìm c√°c m√¥-ƒëun, b·ªô d·ªØ li·ªáu v√† h∆∞·ªõng d·∫´n h·ªó tr·ª£ nghi√™n c·ª©u v√† ph√°t tri·ªÉn trong lƒ©nh v·ª±c X·ª≠ l√Ω Ng√¥n ng·ªØ T·ª± nhi√™n ti·∫øng Vi·ªát. Th∆∞ vi·ªán c√≥ c√°c API d·ªÖ s·ª≠ d·ª•ng ƒë·ªÉ nhanh ch√≥ng √°p d·ª•ng c√°c m√¥ h√¨nh NLP ti·ªÅn hu·∫•n luy·ªán cho vƒÉn b·∫£n ti·∫øng Vi·ªát c·ªßa b·∫°n, nh∆∞ ph√¢n ƒëo·∫°n t·ª´ (Word segmentation), g√°n ph·∫ßn lo·∫°i t·ª´ (PoS), nh·∫≠n di·ªán th·ª±c th·ªÉ ƒë·∫∑t t√™n (Named entity recognition - NER), ph√¢n lo·∫°i vƒÉn b·∫£n (Text Classification) v√† ph√¢n t√≠ch ph·ª• thu·ªôc (Dependency Parsing). Chi ti·∫øt th∆∞ vi·ªán [t·∫°i ƒë√¢y](https://github.com/undertheseanlp/underthesea).

- Gi·ªù h√£y th·ª≠ l·∫≠p tr√¨nh d√πng th∆∞ vi·ªán underthesea, ƒë·∫ßu ti√™n ta c·∫ßn ph·∫£i t·∫£i th∆∞ vi·ªán:

`pip install underthesea`

- **T√°ch vƒÉn b·∫£n th√†nh c√¢u**:

```python
from underthesea import sent_tokenize
text = 'Taylor cho bi·∫øt l√∫c ƒë·∫ßu c√¥ c·∫£m th·∫•y ng·∫°i v·ªõi c√¥ b·∫°n th√¢n Amanda nh∆∞ng r·ªìi m·ªçi th·ª© tr√¥i qua nhanh ch√≥ng. Amanda c≈©ng tho·∫£i m√°i v·ªõi m·ªëi quan h·ªá n√†y.'

print(sent_tokenize(text))
```
 
> [
  "Taylor cho bi·∫øt l√∫c ƒë·∫ßu c√¥ c·∫£m th·∫•y ng·∫°i v·ªõi c√¥ b·∫°n th√¢n Amanda nh∆∞ng r·ªìi m·ªçi th·ª© tr√¥i qua nhanh ch√≥ng.",
  "Amanda c≈©ng tho·∫£i m√°i v·ªõi m·ªëi quan h·ªá n√†y."
]

- **Chu·∫©n h√≥a vƒÉn b·∫£n**:

```python
from underthesea import text_normalize
text_normalize("√ê·∫£m ba·ªè ch·∫•t l·ª±∆°ng ph√≤ng th√≠ ngh·ªã√™m ho√° h·ªçc")
```
> "ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng ph√≤ng th√≠ nghi·ªám h√≥a h·ªçc"

- **T√°ch c√¢u th√†nh t·ª´**:

```python
from underthesea import word_tokenize
text = "Ch√†ng trai 9X Qu·∫£ng Tr·ªã kh·ªüi nghi·ªáp t·ª´ n·∫•m s√≤"

word_tokenize(text)

word_tokenize(sentence, format="text")

text = "Vi·ªán Nghi√™n C·ª©u chi·∫øn l∆∞·ª£c qu·ªëc gia v·ªÅ h·ªçc m√°y"
fixed_words = ["Vi·ªán Nghi√™n C·ª©u", "h·ªçc m√°y"]
word_tokenize(text, fixed_words=fixed_words)
```

> ["Ch√†ng trai", "9X", "Qu·∫£ng Tr·ªã", "kh·ªüi nghi·ªáp", "t·ª´", "n·∫•m", "s√≤"]

> "Ch√†ng_trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤"

> "Vi·ªán_Nghi√™n_C·ª©u chi·∫øn_l∆∞·ª£c qu·ªëc_gia v·ªÅ h·ªçc_m√°y"

- **POS Tagging - ƒê√°nh nh√£n t·ª´**:

```python
from underthesea import pos_tag
pos_tag('Ch·ª£ th·ªãt ch√≥ n·ªïi ti·∫øng ·ªü S√†i G√≤n b·ªã truy qu√©t')
```

> [('Ch·ª£', 'N'),

>('th·ªãt', 'N'),

>('ch√≥', 'N'),

>('n·ªïi ti·∫øng', 'A'),

>('·ªü', 'E'),

>('S√†i G√≤n', 'Np'),

>('b·ªã', 'V'),

>('truy qu√©t', 'V')]

- Ch·ª£ l√† danh t·ª´, S√†i g√≤n l√† danh t·ª´ ri√™ng, truy qu√©t l√† ƒë·ªông t·ª´.
- **Nh·∫≠n di·ªán t√™n th·ª±c th·ªÉ**:
```python
from underthesea import ner
text = 'Ch∆∞a ti·∫øt l·ªô l·ªãch tr√¨nh t·ªõi Vi·ªát Nam c·ªßa T·ªïng th·ªëng M·ªπ Donald Trump'
ner(text)
```

> [('Ch∆∞a', 'R', 'O', 'O'),

>('ti·∫øt l·ªô', 'V', 'B-VP', 'O'),

>('l·ªãch tr√¨nh', 'V', 'B-VP', 'O'),

>('t·ªõi', 'E', 'B-PP', 'O'),

>('Vi·ªát Nam', 'Np', 'B-NP', 'B-LOC'),

>('c·ªßa', 'E', 'B-PP', 'O'),

>('T·ªïng th·ªëng', 'N', 'B-NP', 'O'),

>('M·ªπ', 'Np', 'B-NP', 'B-LOC'),

>('Donald', 'Np', 'B-NP', 'B-PER'),

>('Trump', 'Np', 'B-NP', 'I-PER')]

- **Ph√¢n lo·∫°i ch·ªß ƒë·ªÅ c√¢u**:

```python
from underthesea import classify

classify('HLV ƒë·∫ßu ti√™n ·ªü Premier League b·ªã sa th·∫£i sau 4 v√≤ng ƒë·∫•u')
classify('H·ªôi ƒë·ªìng t∆∞ v·∫•n kinh doanh Asean vinh danh gi·∫£i th∆∞·ªüng qu·ªëc t·∫ø')
classify('L√£i su·∫•t t·ª´ BIDV r·∫•t ∆∞u ƒë√£i', domain='bank')
```

> ['The thao']

> ['Kinh doanh']

> ['INTEREST_RATE']

- **Ph√¢n t√≠ch c·∫£m x√∫c c√¢u**:

```python
from underthesea import sentiment
sentiment('h√†ng k√©m ch·∫•t lg,chƒÉn ƒë·∫Øp l√™n d√≠nh l√¥ng l√° kh·∫Øp ng∆∞·ªùi. th·∫•t v·ªçng')
sentiment('S·∫£n ph·∫©m h∆°i nh·ªè so v·ªõi t∆∞·ªüng t∆∞·ª£ng nh∆∞ng ch·∫•t l∆∞·ª£ng t·ªët, ƒë√≥ng g√≥i c·∫©n th·∫≠n.')
sentiment('ƒêky qua ƒë∆∞·ªùng link ·ªü b√†i vi·∫øt n√†y t·ª´ th·ª© 6 m√† gi·ªù ch∆∞a th·∫•y ai lhe h·∫øt', domain='bank')
sentiment('Xem l·∫°i v·∫´n th·∫•y x√∫c ƒë·ªông v√† t·ª± h√†o v·ªÅ BIDV c·ªßa m√¨nh', domain='bank')
```

> 'negative'

> 'positive'

> ['CUSTOMER_SUPPORT#negative']

> ['TRADEMARK#positive']

## 7. Gi·ªõi thi·ªáu th∆∞ vi·ªán Spacy.

- SpaCy l√† m·ªôt th∆∞ vi·ªán x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n m√£ ngu·ªìn m·ªü cho Python. N√≥ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ th·ª±c hi·ªán x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n. SpaCy cung c·∫•p c√°c m√¥ h√¨nh ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc cho nhi·ªÅu ng√¥n ng·ªØ v√† n√≥ n·ªïi ti·∫øng v·ªõi t·ªëc ƒë·ªô v√† hi·ªáu su·∫•t c·ªßa m√¨nh khi x·ª≠ l√Ω l∆∞·ª£ng vƒÉn b·∫£n l·ªõn. 
- **T√°ch token (Tokenization)**: SpaCy c√≥ th·ªÉ ph√¢n ƒëo·∫°n vƒÉn b·∫£n th√†nh t·ª´ ho·∫∑c ƒë∆°n v·ªã con t·ª´, gi√∫p d·ªÖ d√†ng ph√¢n t√≠ch v√† x·ª≠ l√Ω c√°c y·∫øu t·ªë ng√¥n ng·ªØ.

- **POS-Tagging**: G√°n c√°c lo·∫°i (v√≠ d·ª•: danh t·ª´, ƒë·ªông t·ª´, t√≠nh t·ª´) cho m·ªói t·ª´ trong m·ªôt c√¢u.

- **Nh·∫≠n d·∫°ng th·ª±c th·ªÉ t√™n th·ª±c th·ªÉ ( Name Entity Recognition - NER)**: SpaCy c√≥ th·ªÉ x√°c ƒë·ªãnh v√† ph√¢n lo·∫°i c√°c th·ª±c th·ªÉ nh∆∞ ng∆∞·ªùi, t·ªï ch·ª©c, ƒë·ªãa ƒëi·ªÉm, ng√†y th√°ng v√† nhi·ªÅu h∆°n n·ªØa trong m·ªôt ƒëo·∫°n vƒÉn b·∫£n cho tr∆∞·ªõc.

- **Ph√¢n t√≠ch ph·ª• thu·ªôc (Dependency Parsing)**: N√≥ ph√¢n t√≠ch c·∫•u tr√∫c ng·ªØ ph√°p c·ªßa m·ªôt c√¢u, x√°c ƒë·ªãnh m·ªëi quan h·ªá gi·ªØa c√°c t·ª´ v√† ph·ª• thu·ªôc ng·ªØ ph√°p c·ªßa ch√∫ng.


- **Lemmatization**: SpaCy c√≥ th·ªÉ gi·∫£m c√°c t·ª´ v·ªÅ g·ªëc v√≠ d·ª• running t·ª´ g·ªëc l√† run , ƒëi·ªÅu n√†y h·ªØu √≠ch cho c√°c nhi·ªám v·ª• nh∆∞ chu·∫©n h√≥a vƒÉn b·∫£n.


- **Vector h√≥a t·ª´**: SpaCy cung c·∫•p c√°c vector t·ª´ ƒë√£ ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c nhi·ªám v·ª• nh∆∞ t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng v√† ph√¢n c·ª•m t√†i li·ªáu.


- **T√πy ch·ªânh**: Ng∆∞·ªùi d√πng c√≥ th·ªÉ ƒë√†o t·∫°o c√°c m√¥ h√¨nh ri√™ng c·ªßa h·ªç b·∫±ng c√°ch s·ª≠ d·ª•ng SpaCy cho c√°c lƒ©nh v·ª±c ho·∫∑c ng√¥n ng·ªØ c·ª• th·ªÉ.

- Gi·ªù ch√∫ng ta s·∫Ω l·∫≠p tr√¨nh th∆∞ vi·ªán Spacy, ƒë·∫ßu ti√™n c·∫ßn ph·∫£i t·∫£i th∆∞ vi·ªán xu·ªëng:

`pip install spacy`

- T·∫£i m√¥ h√¨nh t√°ch t·ª´ Ti·∫øng anh:

`python -m spacy download en_core_web_sm`

- Gi·ªù h√£y l·∫≠p tr√¨nh t√°ch t·ª´:

```python
import spacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")

# Text to be tokenized
text = "SpaCy is a powerful natural language processing library for Python."

# Process the text with spaCy
doc = nlp(text)

# Access the tokens
tokens = [token.text for token in doc]

# Print the tokens
print(tokens)
```

> ['SpaCy', 'is', 'a', 'powerful', 'natural', 'language', 'processing', 'library', 'for', 'Python', '.']

- Ngo√†i ra Spacy c√≤n h·ªó tr·ª£ s·ª≠ d·ª•ng b·ªô t√°ch token c·ªßa Bert. Tr∆∞·ªõc ti√™n ch√∫ng ta s·∫Ω c√†i ƒë·∫∑t th∆∞ vi·ªán tokenizers:

`pip install tokenizers`

- T·∫£i t·ª´ ƒëi·ªÉn c·ªßa Bert b·∫±ng ƒëo·∫°n code sau:

```python
import requests

url = 'https://raw.githubusercontent.com/microsoft/SDNet/master/bert_vocab_files/bert-base-uncased-vocab.txt'
response = requests.get(url)

# L∆∞u file xu·ªëng
with open('bert-base-uncased-vocab.txt', 'w') as file:
    file.write(response.text)

```

- Nh∆∞ b·∫°n ƒë√£ bi·∫øt Bert s·ª≠ d·ª•ng thu·∫≠t to√°n WordPiece ƒë·ªÉ t√°ch t·ª´. Xem chi ti·∫øt v·ªÅ thu·∫≠t to√°n n√†y [t·∫°i ƒë√¢y](https://buiquangdat1710.github.io/posts/C%C3%A1c_K%C4%A9_Thu%E1%BA%ADt_T%C3%A1ch_T%E1%BB%AB_Ph%E1%BA%A7n_2/).
- Gi·ªù h√£y l·∫≠p tr√¨nh ƒë·ªÉ t√°ch token t·ª´ m√¥ h√¨nh Bert:
  
```python
from tokenizers import BertWordPieceTokenizer
from spacy.tokens import Doc
import spacy

class BertTokenizer:
    def __init__(self, vocab, vocab_file, lowercase=True):
        self.vocab = vocab
        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)

    def __call__(self, text):
        tokens = self._tokenizer.encode(text)
        words = []
        spaces = []
        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):
            words.append(text)
            if i < len(tokens.tokens) - 1:
                # If next start != current end we assume a space in between
                next_start, next_end = tokens.offsets[i + 1]
                spaces.append(next_start > end)
            else:
                spaces.append(True)
        return Doc(self.vocab, words=words, spaces=spaces)

nlp = spacy.blank("en")
nlp.tokenizer = BertTokenizer(nlp.vocab, "bert-base-uncased-vocab.txt")
doc = nlp("Justin Drew Bieber is a Canadian singer, songwriter, and actor.")
print([token.text for token in doc])
```

> ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer', ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']

### Ki·∫øn tr√∫c Spacy.

![·∫¢nh Ki·∫øn tr√∫c c·ªßa Spacy](https://spacy.io/images/architecture.svg)

- Ki·∫øn tr√∫c l√µi c·ªßa Spacy bao g·ªìm c√°c th√†nh ph·∫ßn sau:
  - **L·ªõp Language**: L·ªõp n√†y s·ª≠ d·ª•ng ƒë·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n v√† chuy·ªÉn th√†nh ƒë·ªëi t∆∞·ª£ng Doc. Bi·∫øn s·ª≠ d·ª•ng l√† nlp.
  - **L·ªõp Vocab**: Vector c·ªßa t·ª´ v√† c√°c thu·ªôc t√≠nh t·ª´ v·ª±ng n·∫±m b√™n trong Vocab
  - **L·ªõp Doc**: ƒê·ªëi t∆∞·ª£ng Doc ch·ª©a chu·ªói tokens v√† nh√£n t∆∞∆°ng ·ª©ng.

- L·ªõp **Token** l√† l·ªõp c∆° b·∫£n nh·∫•t c·ªßa Spacy. B·∫°n c√≥ th·ªÉ xem c√°c h√†m chi ti·∫øt t·∫°i ƒë√¢y: [https://spacy.io/api/token](https://spacy.io/api/token)
- C√°ch chuy·ªÉn vƒÉn b·∫£n th√†nh chu·ªói c√°c Token:

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Give it back! He pleaded.")
```

- B·∫°n ho√†n to√†n c√≥ th·ªÉ truy c·∫≠p v√†o vector c·ªßa token th√¥ng qua thu·ªôc t√≠nh sau:

```python
print(doc.vector)
```

- L·ªõp **Doc** l√† l·ªõp t·∫°o ra nh·ªØng ƒë·ªëi t∆∞·ª£ng ch·ª©a chu·ªói c√°c token.

### X√¢y d·ª±ng t·ª´ ƒëi·ªÉn t·ª´ c√°c vƒÉn b·∫£n.

![·∫¢nh](https://spacy.io/images/vocab_stringstore.svg)

- T·ª´ nhi·ªÅu vƒÉn b·∫£n ta x√¢y d·ª±ng ƒë∆∞·ª£c t·ª´ ƒëi·ªÉn th√¥ng qua vi·ªác t·∫°o ra √°nh x·∫° gi·ªØa c√°c token v·ªõi s·ªë ho·∫∑c gi√° tr·ªã bƒÉm hash. V√≠ d·ª• trong tr∆∞·ªùng h·ª£p n√†y:
  - Token I c√≥ gi√° tr·ªã 46904.
  - Token love c√≥ gi√° tr·ªã 37020.

- D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n code v√≠ d·ª•:

```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("I love coffee")
print(doc.vocab.strings["coffee"])  
print(doc.vocab.strings[3197928453018144401])  

```
> 3197928453018144401

> coffee

- Trong tr∆∞·ªùng h·ª£p n√†y hash c·ªßa token "coffee" l√† 3197928453018144401. Hi·ªÉn th·ªã t·ª´ ƒë√£ ƒë∆∞·ª£c ƒë∆∞a v√†o t·ª´ ƒëi·ªÉn:

```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("I love coffee")
for word in doc:
    lexeme = doc.vocab[word.text]
    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,
            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)
```
> I 4690420944186131903 X I I True False True en

> love 3702023516439754181 xxxx l ove True False False en

> coffee 3197928453018144401 xxxx c fee True False False en

- ƒê∆°n v·ªã l∆∞u tr·ªØ trong Vocab d∆∞·ªõi d·∫°ng t·ª´ v·ª±ng (Lexeme). M·ªôt t·ª´ v·ª±ng c√≥ c√°c thu·ªôc t√≠nh sau:
  - **VƒÉn b·∫£n:** VƒÉn b·∫£n g·ªëc c·ªßa t·ª´ v·ª±ng.
  - **Orth:** Gi√° tr·ªã bƒÉm c·ªßa t·ª´ v·ª±ng.
  - **Shape:** H√¨nh d·∫°ng t·ª´ ng·ªØ tr·ª´u t∆∞·ª£ng c·ªßa t·ª´ v·ª±ng.
  - **Prefix:** Theo m·∫∑c ƒë·ªãnh, l√† ch·ªØ c√°i ƒë·∫ßu ti√™n c·ªßa chu·ªói t·ª´.
  - **Suffix:** Theo m·∫∑c ƒë·ªãnh, l√† ba ch·ªØ c√°i cu·ªëi c√πng c·ªßa chu·ªói t·ª´.
  - **is alpha:** Li·ªáu t·ª´ v·ª±ng c√≥ bao g·ªìm c√°c k√Ω t·ª± ch·ªØ c√°i kh√¥ng?
  - **is digit:** Li·ªáu t·ª´ v·ª±ng c√≥ bao g·ªìm c√°c ch·ªØ s·ªë kh√¥ng?

- Gi√° tr·ªã bƒÉm c·ªßa m·ªôt token kh√¥ng th·ªÉ decode ƒë·ªÉ ra ƒë∆∞·ª£c t·ª´ ƒë√≥ v√† lu√¥n ph·∫£i c√≥ t·ª´ ƒëi·ªÉn ƒë·ªÉ √°nh x·∫° ng∆∞·ª£c l·∫°i.

```python
import spacy
from spacy.tokens import Doc
from spacy.vocab import Vocab

nlp = spacy.load("en_core_web_sm")
doc = nlp("I love coffee")  # Original Doc
print(doc.vocab.strings["coffee"])  # 3197928453018144401
print(doc.vocab.strings[3197928453018144401])  # 'coffee' üëç

empty_doc = Doc(Vocab())  # New Doc with empty Vocab
# empty_doc.vocab.strings[3197928453018144401] will raise an error :(

empty_doc.vocab.strings.add("coffee")  # Add "coffee" and generate hash
print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' üëç

new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab
print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' üëç
```

> 3197928453018144401

> coffee

> coffee

> coffee

### Thi·∫øt k·∫ø Pipeline.

![Anh](https://spacy.io/images/pipeline.svg)

- C√°c th√†nh ph·∫ßn trong pipeline b·∫°n c√≥ th·ªÉ t√πy bi·∫øn: 
  - **Tagger**: part-of-speech tags.
  - **Parser**: Th√™m ph·ª• thu·ªôc.
  - **Ner**: Ph√°t hi·ªán v√† ƒë√°nh nh√£n c√°c th·ª±c th·ªÉ.
  - **Lemmatizer**: ƒê∆∞a v·ªÅ d·∫°ng g·ªëc.
  - **Textcat**: ƒê√°nh nh√£n vƒÉn b·∫£ncustom: B·ªï sung c√°c thu·ªôc t√≠nh, h√†m t√πy ch·ªânh v√†o trong pipeline.

## 8. M√¥ h√¨nh ƒë·ªÅ xu·∫•t b·ªüi Kiss v√† Strunk.

### T·ª´ vi·∫øt t·∫Øt - Abbreviation.
- C√≥ ba y·∫øu t·ªë ch√≠nh c·ªßa t·ª´ vi·∫øt t·∫Øt:
  - T·ª´ vi·∫øt t·∫Øt s·∫Ω tr√¥ng d∆∞·ªõi d·∫°ng c√°c k√Ω t·ª´ ƒëi li·ªÅn v·ªõi d·∫•u ch·∫•m. V√≠ d·ª• Mrs. hay Mr. hay Dr.
  - T·ª´ vi·∫øt t·∫Øt c√≥ xu h∆∞·ªõng ng·∫Øn. T·ª´ c√†ng d√†i th√¨ kh·∫£ nƒÉng l√† t·ª´ vi·∫øt t·∫Øt c√†ng th·∫•p.
  - D·∫•u ch·∫•m xu·∫•t hi·ªán nhi·ªÅu ·ªü gi·ªØa t·ª´ vi·∫øt t·∫Øt.
- H·ªá th·ªëng ph√°t hi·ªán d·∫•u ch·∫•m c·ªßa Punkt ph√°t hi·ªán ƒë∆∞·ª£c 99.38% t·ª´ vi·∫øt t·∫Øt. Tuy nhi√™n nh·ªØng t·ª´ vi·∫øt t·∫Øt n√†y ch∆∞a bao g·ªìm:
  - Ch·ªØ vi·∫øt t·∫Øt t√™n v√≠ d·ª• John F Kennedy - J.F.K ho·∫∑c Dr. J. Smith ho·∫∑c J.K. Rowling. 
  - S·ªë th·ª© t·ª± (Ordinal Numbers) nh∆∞ 1. 2.

- V·ªõi vi·ªác k·∫øt lu·∫≠n ƒë∆∞·ª£c t·ª´ vi·∫øt t·∫Øt th√¨ t·∫•t c·∫£ c√°c d·∫•u ch·∫•m ƒë·ª©ng sau nh·ªØng t·ª´ kh√¥ng ph·∫£i t·ª´ vi·∫øt t·∫Øt c√≥ th·ªÉ coi l√† k·∫øt th√∫c c√¢u (End of a Sentence). Tuy nhi√™n c√≥ tr∆∞·ªùng h·ª£p d·∫•u ch·∫•m v·ª´a l√† k·∫øt th√∫c t·ª´ vi·∫øt t·∫Øt c≈©ng nh∆∞ l√† k·∫øt th√∫c c√¢u.

### Pipeline ƒë·ªÉ ph√°t hi·ªán d·∫•u ch·∫•m c√¢u.
- B√†i b√°o g·ªëc: [https://aclanthology.org/J06-4003.pdf](https://aclanthology.org/J06-4003.pdf)

![·∫¢nh](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/e6235cf0-a9eb-11ee-881b-717bb9489abc-Screen_Shot_2024_01_03_at_10.55.01_500x448.png)

#### 1. Kh√¢u ƒë·∫ßu ti√™n - Type-based classification stage.

- Ph√°t hi·ªán t·ª´ vi·∫øt t·∫Øt th√¥ng qua:
  - Collocational bond: T·ª´ vi·∫øt t·∫Øt hay ƒëi v·ªõi d·∫•u ch·∫•m ƒë·∫±ng sau.
  - Length: Chi·ªÅu d√†i c·ªßa t·ª´ vi·∫øt t·∫Øt.
  - Internal Periods: T·ª´ vi·∫øt t·∫Øt c√≥ d·∫•u ch·∫•m c√¢u b√™n trong.
  - Occurrences without a final period: T·ª´ vi·∫øt t·∫Øt kh√¥ng c√≥ d·∫•u ch·∫•m k·∫øt th√∫c.
- Sau b∆∞·ªõc n√†y c√°c t·ª´ s·∫Ω ƒë∆∞·ª£c ph√¢n lo·∫°i th√†nh:
  - `<A>`: T·ª´ vi·∫øt t·∫Øt.
  - `<E>`: D·∫•u 3 ch·∫•m (...).
  - `<S>`: D·∫•u ch·∫•m m√† kh√¥ng ƒëi sau t·ª´ vi·∫øt t·∫Øt th√¨ quy th√†nh k·∫øt th√∫c c√¢u.

#### 2. Kh√¢u th·ª© hai - Token-based classification stage.

- Ph√°t hi·ªán t·ª´ vi·∫øt t·∫Øt ·ªü cu·ªëi c√¢u: `<A>` th√†nh `<A><S>`
- Ph√°t hi·ªán d·∫•u ba ch·∫•m ·ªü cu·ªëi c√¢u: `<E>` th√†nh `<E> <S>`
  - C√°ch ph√°t hi·ªán: s·ª≠ d·ª•ng Regex: `[^\W\d]\.$`
- Ph√°t hi·ªán k√Ω t·ª± ƒë·∫ßu c√¢u: `<S>` th√†nh `<A>` v√† `<A>` th√†nh `<A>`. V√≠ d·ª• A. hay B.
  - C√°ch ph√°t hi·ªán: s·ª≠ d·ª•ng Regex: `[^\W\d]\.$`
- Ph√°t hi·ªán s·ªë th·ª© t·ª±: `<S>` Th√†nh `<A>`. V√≠ d·ª• 1. hay 1000. hay -1.
  - C√°ch ph√°t hi·ªán: s·ª≠ d·ª•ng Regex: `^-?[\.,]?\d[\d,\.-]*\.?$`

#### 3. Kh√¢u th·ª© ba.

- K·∫øt lu·∫≠t t·ª´ ƒë√≥ c√≥ ph·∫£i t·ª´ vi·∫øt t·∫Øt hay kh√¥ng v·ªõi c√°c nh√£n. `<A>`, `<E>`, `<S>`, `<A><S>`, `<E><S>`

### Dunning log-likelihood.
- C√¥ng th·ª©c Dunning log-likelihood ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ so s√°nh t·∫ßn su·∫•t c·ªßa m·ªôt ƒë·∫∑c tr∆∞ng trong hai ng·ªØ li·ªáu kh√°c nhau nh·∫±m xem li·ªáu s·ª± kh√°c bi·ªát c√≥ mang √Ω nghƒ©a th·ªëng k√™ hay kh√¥ng. C√¥ng th·ª©c n√†y d·ª±a tr√™n t·ª∑ l·ªá kh·∫£ nƒÉng (likelihood ratio), so s√°nh kh·∫£ nƒÉng c·ªßa d·ªØ li·ªáu d∆∞·ªõi hai gi·∫£ thuy·∫øt kh√°c nhau. D∆∞·ªõi ƒë√¢y l√† c√¥ng th·ª©c:

$$
G^2 = 2 \sum_{i=1}^{k} O_i \cdot \ln \left( \frac{O_i}{E_i} \right)
$$

- Trong ƒë√≥:

    - $G^2$ l√† th·ªëng k√™ t·ª∑ l·ªá kh·∫£ nƒÉng log (log-likelihood ratio statistic).
    - $O_i$ l√† t·∫ßn su·∫•t quan s√°t ƒë∆∞·ª£c c·ªßa m·ªôt s·ª± ki·ªán c·ª• th·ªÉ trong d·ªØ li·ªáu.
    - $E_i$ l√† t·∫ßn su·∫•t k·ª≥ v·ªçng c·ªßa s·ª± ki·ªán d∆∞·ªõi gi·∫£ thuy·∫øt kh√¥ng (null hypothesis).
    - T·ªïng ƒë∆∞·ª£c t√≠nh qua t·∫•t c·∫£ c√°c s·ª± ki·ªán (ho·∫∑c h·∫°ng m·ª•c) ƒëang ƒë∆∞·ª£c xem x√©t.
- V·ªõi tr∆∞·ªùng h·ª£p c√≥ hai s·ª± ki·ªán:

$$
G^2 = 2 \left[ O_1 \cdot \ln \left( \frac{O_1}{E_1} \right) + O_2 \cdot \ln \left( \frac{O_2}{E_2} \right) \right]
$$

- Trong ng·ªØ c·∫£nh n√†y:

  - $O_1$ v√† $O_2$ l√† t·∫ßn su·∫•t quan s√°t trong m·ªói t·∫≠p ng·ªØ li·ªáu.
  - $E_1$ v√† $E_2$ l√† t·∫ßn su·∫•t k·ª≥ v·ªçng d∆∞·ªõi gi·∫£ thuy·∫øt kh√¥ng, gi·∫£ ƒë·ªãnh r·∫±ng t·∫ßn su·∫•t t·ªïng th·ªÉ c·ªßa ƒë·∫∑c tr∆∞ng l√† nh∆∞ nhau trong c·∫£ hai t·∫≠p ng·ªØ li·ªáu.
  
- C√°c t·∫ßn su·∫•t k·ª≥ v·ªçng $E_i$ th∆∞·ªùng ƒë∆∞·ª£c t√≠nh to√°n d·ª±a tr√™n t·∫ßn su·∫•t t·ªïng th·ªÉ c·ªßa s·ª± ki·ªán v√† k√≠ch th∆∞·ªõc c·ªßa m·ªói t·∫≠p ng·ªØ li·ªáu. Gi√° tr·ªã c·ªßa $G^2$, c√†ng l·ªõn th√¨ c√†ng √≠t kh·∫£ nƒÉng l√† s·ª± ph√¢n b·ªë quan s√°t ƒë∆∞·ª£c l√† do ng·∫´u nhi√™n, cho th·∫•y r·∫±ng s·ª± kh√°c bi·ªát trong t·∫ßn su·∫•t gi·ªØa hai t·∫≠p ng·ªØ li·ªáu l√† c√≥ √Ω nghƒ©a th·ªëng k√™.


- ƒê·ªëi v·ªõi m·ªôt k·ªãch b·∫£n ƒëi·ªÉn h√¨nh, khi b·∫°n so s√°nh t·∫ßn su·∫•t c·ªßa m·ªôt t·ª´ ho·∫∑c ƒë·∫∑c tr∆∞ng trong hai t·∫≠p d·ªØ li·ªáu (g·ªçi l√† T·∫≠p d·ªØ li·ªáu 1 v√† T·∫≠p d·ªØ li·ªáu 2), b·∫°n s·∫Ω t√≠nh $E_i$ nh∆∞ sau:
  - **T·ªïng s·ªë l·∫ßn x·∫£y ra s·ª± ki·ªán trong c·∫£ hai t·∫≠p d·ªØ li·ªáu**: ƒê·∫ßu ti√™n, t√¨m t·ªïng s·ªë l·∫ßn x·∫£y ra s·ª± ki·ªán (v√≠ d·ª•: m·ªôt t·ª´ c·ª• th·ªÉ) trong c·∫£ hai t·∫≠p d·ªØ li·ªáu ƒë√£ k·∫øt h·ª£p. G·ªçi s·ªë ƒë·∫øm n√†y l√† $C$.
  - K√≠ch th∆∞·ªõc t·ªïng c·ªßa t·ª´ng t·∫≠p d·ªØ li·ªáu: X√°c ƒë·ªãnh k√≠ch th∆∞·ªõc t·ªïng c·ªßa m·ªói t·∫≠p d·ªØ li·ªáu. G·ªçi k√≠ch th∆∞·ªõc c·ªßa T·∫≠p d·ªØ li·ªáu 1 l√† $N_1$ v√† T·∫≠p d·ªØ li·ªáu 2 l√† $N_2$.
  - K√≠ch th∆∞·ªõc t·ªïng c·ªßa c·∫£ hai t·∫≠p d·ªØ li·ªáu: T√≠nh t·ªïng k√≠ch th∆∞·ªõc c·ªßa c·∫£ hai t·∫≠p d·ªØ li·ªáu, $N = N_1 + N_2$.
  - T√≠nh t·∫ßn su·∫•t k·ª≥ v·ªçng cho m·ªói t·∫≠p d·ªØ li·ªáu: T·∫ßn su·∫•t k·ª≥ v·ªçng c·ªßa s·ª± ki·ªán trong m·ªói t·∫≠p d·ªØ li·ªáu d∆∞·ªõi gi·∫£ thuy·∫øt kh√¥ng c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh b·∫±ng c√¥ng th·ª©c:
  - 
  $$
  E_i = \frac{C*N_i}{N}
  $$ 
  


- C√¥ng th·ª©c Dunning log-likelihood n√†y ƒë∆∞·ª£c d√πng ƒë·ªÉ x√°c ƒë·ªãnh xem s·ª± kh√°c bi·ªát trong t·∫ßn su·∫•t c·ªßa m·ªôt s·ª± ki·ªán c·ª• th·ªÉ trong hai t·∫≠p d·ªØ li·ªáu c√≥ mang √Ω nghƒ©a th·ªëng k√™ hay kh√¥ng. C·ª• th·ªÉ, n√≥ gi√∫p so s√°nh s·ª± ph√¢n b·ªë c·ªßa m·ªôt ƒë·∫∑c tr∆∞ng (v√≠ d·ª•: m·ªôt t·ª´ ho·∫∑c m·ªôt s·ª± ki·ªán) gi·ªØa hai t·∫≠p d·ªØ li·ªáu kh√°c nhau, v√† cho th·∫•y s·ª± kh√°c bi·ªát ƒë√≥ c√≥ th·ªÉ ch·ªâ l√† do ng·∫´u nhi√™n ho·∫∑c th·ª±c s·ª± c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ.

- C√¥ng th·ª©c n√†y h·ªØu √≠ch khi b·∫°n mu·ªën ki·ªÉm tra xem m·ªôt t·ª´ ho·∫∑c s·ª± ki·ªán c√≥ xu·∫•t hi·ªán kh√°c nhau v·ªÅ t·∫ßn su·∫•t trong hai t·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n kh√°c nhau (v√≠ d·ª•: so s√°nh t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªôt t·ª´ trong ng·ªØ li·ªáu c·ªßa b√°o ch√≠ v√† ng·ªØ li·ªáu c·ªßa m·∫°ng x√£ h·ªôi).
- Gi·∫£ s·ª≠ ch√∫ng ta mu·ªën so s√°nh t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ "AI" trong hai t·∫≠p vƒÉn b·∫£n kh√°c nhau: m·ªôt t·∫≠p vƒÉn b·∫£n v·ªÅ c√¥ng ngh·ªá v√† m·ªôt t·∫≠p vƒÉn b·∫£n v·ªÅ vƒÉn h·ªçc.
- **Ng·ªØ li·ªáu v·ªÅ c√¥ng ngh·ªá**: T·ª´ "AI" xu·∫•t hi·ªán 300 l·∫ßn tr√™n t·ªïng s·ªë 10.000 t·ª´.
- **Ng·ªØ li·ªáu v·ªÅ vƒÉn h·ªçc**: T·ª´ "AI" xu·∫•t hi·ªán 20 l·∫ßn tr√™n t·ªïng s·ªë 8.000 t·ª´.

#### B∆∞·ªõc 1: T√≠nh t·∫ßn su·∫•t quan s√°t ƒë∆∞·ª£c.

- $O_1$ ‚Äã(s·ªë l·∫ßn xu·∫•t hi·ªán t·ª´ "AI" trong ng·ªØ li·ªáu c√¥ng ngh·ªá) = 300.
- $O_2$ s·ªë l·∫ßn xu·∫•t hi·ªán t·ª´ "AI" trong ng·ªØ li·ªáu vƒÉn h·ªçc) = 20.

#### B∆∞·ªõc 2:  T√≠nh t·∫ßn su·∫•t k·ª≥ v·ªçng.

- D∆∞·ªõi gi·∫£ thuy·∫øt kh√¥ng, ta gi·∫£ ƒë·ªãnh t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ "AI" s·∫Ω t∆∞∆°ng t·ª± trong c·∫£ hai ng·ªØ li·ªáu. T·ªïng s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa "AI" trong c·∫£ hai ng·ªØ li·ªáu l√†:
  - T·ªïng s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ "AI": $300+20=320$.
  - T·ªïng s·ªë t·ª´ trong c·∫£ hai ng·ªØ li·ªáu: $10000 + 8000 = 18000$.
- T·∫ßn su·∫•t k·ª≥ v·ªçng c·ªßa t·ª´ "AI" trong ng·ªØ li·ªáu c√¥ng ngh·ªá v√† vƒÉn h·ªçc s·∫Ω l·∫ßn l∆∞·ª£t l√†:
  - $E_1$ (t·∫ßn su·∫•t k·ª≥ v·ªçng c·ªßa "AI" trong ng·ªØ li·ªáu c√¥ng ngh·ªá) = $\frac{320*10000}{18000} \approx 177.78$.
  - $E_2$ t·∫ßn su·∫•t k·ª≥ v·ªçng c·ªßa "AI" trong ng·ªØ li·ªáu vƒÉn h·ªçc) = $\frac{320*8000}{18000} \approx 142.22$.

#### B∆∞·ªõc 3: T√≠nh $G^2$

- S·ª≠ d·ª•ng c√¥ng th·ª©c Dunning log-likelihood cho hai s·ª± ki·ªán:

$$
G^2 = 2 \left[ O_1 \cdot \ln \left( \frac{O_1}{E_1} \right) + O_2 \cdot \ln \left( \frac{O_2}{E_2} \right) \right]
$$

- Thay gi√° tr·ªã v√†o:

$$
G^2 = 2 \left[ 300 \cdot \ln \left( \frac{300}{177.78} \right) + 20 \cdot \ln \left( \frac{20}{1422.22} \right) \right] = 235.44
$$

- Gi√° tr·ªã $G^2 = 235.44$ cho th·∫•y r·∫±ng s·ª± kh√°c bi·ªát v·ªÅ t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ "AI" gi·ªØa ng·ªØ li·ªáu c√¥ng ngh·ªá v√† vƒÉn h·ªçc r·∫•t c√≥ √Ω nghƒ©a th·ªëng k√™. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† t·∫ßn su·∫•t c·ªßa t·ª´ "AI" trong hai t·∫≠p d·ªØ li·ªáu kh√°c nhau kh√¥ng ph·∫£i l√† do ng·∫´u nhi√™n, m√† do s·ª± kh√°c bi·ªát n·ªôi t·∫°i gi·ªØa c√°c ch·ªß ƒë·ªÅ c·ªßa hai t·∫≠p vƒÉn b·∫£n.
- T·ª´ "AI" xu·∫•t hi·ªán th∆∞·ªùng xuy√™n h∆°n r·∫•t nhi·ªÅu trong ng·ªØ li·ªáu c√¥ng ngh·ªá so v·ªõi ng·ªØ li·ªáu vƒÉn h·ªçc, v√† s·ª± kh√°c bi·ªát n√†y l√† c√≥ √Ω nghƒ©a th·ªëng k√™.
- **$G^2$ c√†ng cao**, s·ª± kh√°c bi·ªát gi·ªØa t·∫ßn su·∫•t quan s√°t ƒë∆∞·ª£c v√† t·∫ßn su·∫•t k·ª≥ v·ªçng c√†ng l·ªõn. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ gi·ªØa hai t·∫≠p ng·ªØ li·ªáu v·ªÅ t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa ƒë·∫∑c tr∆∞ng ƒëang ƒë∆∞·ª£c so s√°nh (v√≠ d·ª•: m·ªôt t·ª´ xu·∫•t hi·ªán nhi·ªÅu h∆°n trong m·ªôt t·∫≠p so v·ªõi t·∫≠p kia). S·ª± kh√°c bi·ªát n√†y c√≥ th·ªÉ kh√¥ng ph·∫£i ng·∫´u nhi√™n m√† do n·ªôi t·∫°i c·ªßa hai ng·ªØ li·ªáu (v√≠ d·ª•, do n·ªôi dung ch·ªß ƒë·ªÅ c·ªßa hai ng·ªØ li·ªáu kh√°c nhau).
- **$G^2$ c√†ng th·∫•p**, s·ª± kh√°c bi·ªát gi·ªØa t·∫ßn su·∫•t quan s√°t ƒë∆∞·ª£c v√† t·∫ßn su·∫•t k·ª≥ v·ªçng c√†ng nh·ªè, cho th·∫•y r·∫±ng t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa ƒë·∫∑c tr∆∞ng trong c·∫£ hai ng·ªØ li·ªáu c√≥ xu h∆∞·ªõng t∆∞∆°ng ƒë·ªìng. ƒêi·ªÅu n√†y ng·ª• √Ω r·∫±ng s·ª± kh√°c bi·ªát (n·∫øu c√≥) c√≥ th·ªÉ ch·ªâ l√† do ng·∫´u nhi√™n, v√† kh√¥ng c√≥ d·∫•u hi·ªáu r√µ r√†ng v·ªÅ s·ª± kh√°c bi·ªát gi·ªØa hai t·∫≠p ng·ªØ li·ªáu.


### Likelihood l√† l√µi.

- C·∫£ kh√¢u m·ªôt v√† hai ƒë·ªÅu s·ª≠ d·ª•ng kh·∫£ nƒÉng (Likelihood) ƒë·ªÉ x√°c ƒë·ªãnh m·ªëi quan h·ªá gi·ªØa c√°c t·ª´ vi·∫øt t·∫Øt v√† d·∫•u ch·∫•m c·ªßa n√≥ c≈©ng nh∆∞ m·ªëi quan h·ªá c·ªßa d·∫•u ch·∫•m k·∫øt th√∫c c√¢u v√† nh·ªØng t·ª´ ƒëi sau n√≥ v√† nh·ªØng t·ª´ xung quanh m·ªôt d·∫•u ch·∫•m. Ch√∫ √Ω t·ª´ w trong b√†i vi·∫øt n√†y l√† **t·ª´ vi·∫øt t·∫Øt**.

#### 1. S·ª≠ d·ª•ng Likelihood trong kh√¢u 1.

- Log-likelihood ƒë·ªÅ xu·∫•t b·ªüi Dunning nƒÉm 1993 ki·ªÉm tra li·ªáu x√°c su·∫•t c·ªßa m·ªôt t·ª´ c√≥ ph·ª• thu·ªôc v·ªõi vi·ªác v·ªõi lo·∫°i t·ª´ ·ªü ph√≠a tr∆∞·ªõc kh√¥ng.

- **Gi·∫£ thuy·∫øt Null ($H_0$)**:

$$
P( \cdot | w) = p = P( \cdot | \neg w)
$$

- Gi·∫£ thuy·∫øt thay th·∫ø kh·∫≥ng ƒë·ªãnh r·∫±ng x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m ph·ª• thu·ªôc v√†o t·ª´ ƒë·ª©ng tr∆∞·ªõc. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu t·ª´ tr∆∞·ªõc thay ƒë·ªïi, x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m c≈©ng thay ƒë·ªïi.
- V√≠ d·ª•: Gi·∫£ s·ª≠ ch√∫ng ta ƒëang ki·ªÉm tra xem d·∫•u ch·∫•m c√≥ ph·ª• thu·ªôc v√†o t·ª´ "h·ªçc" trong c√¢u "T√¥i ƒëang h·ªçc." hay kh√¥ng. Gi·∫£ thuy·∫øt Null ($H_0$) n√≥i r·∫±ng x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m trong c√¢u n√†y s·∫Ω kh√¥ng thay ƒë·ªïi n·∫øu ch√∫ng ta thay th·∫ø t·ª´ "h·ªçc" b·∫±ng t·ª´ kh√°c nh∆∞ "ch∆°i" (v√≠ d·ª•: "T√¥i ƒëang ch∆°i."). T·ª©c l√†, x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m s·∫Ω gi·ªëng nhau d√π t·ª´ tr∆∞·ªõc n√≥ l√† "h·ªçc" hay "ch∆°i".

- **Gi·∫£ thuy·∫øt Thay th·∫ø ($H_1$)**:

$$
P( \cdot | w) = p_1 \neq P( \cdot | \neg w) = p_2
$$

- Gi·∫£ thuy·∫øt thay th·∫ø kh·∫≥ng ƒë·ªãnh r·∫±ng x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m ph·ª• thu·ªôc v√†o t·ª´ ƒë·ª©ng tr∆∞·ªõc. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu t·ª´ tr∆∞·ªõc thay ƒë·ªïi, x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m c≈©ng thay ƒë·ªïi.

- Ti·∫øp t·ª•c v√≠ d·ª• tr∆∞·ªõc, gi·∫£ thuy·∫øt thay th·∫ø ($H_1$) cho r·∫±ng x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m trong c√¢u "T√¥i ƒëang h·ªçc." kh√°c v·ªõi x√°c su·∫•t trong c√¢u "T√¥i ƒëang ch∆°i.". T·ª´ "h·ªçc" v√† "ch∆°i" c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác c√≥ d·∫•u ch·∫•m hay kh√¥ng, v√¨ v·∫≠y $P(.\|h·ªçc) \neq P(.\|ch∆°i)$.

- **T·ª∑ s·ªë Log-likelihood ($\lambda$)**:

$$
\log \lambda = -2 \log \frac{P_{\text{binom}}(H_0)}{P_{\text{binom}}(H_1)}
$$

- ƒê√¢y l√† t·ª∑ s·ªë log-likelihood ƒë∆∞·ª£c d√πng ƒë·ªÉ so s√°nh hai x√°c su·∫•t d·ª±a tr√™n gi·∫£ thuy·∫øt Null v√† gi·∫£ thuy·∫øt thay th·∫ø. N·∫øu gi√° tr·ªã c·ªßa t·ª∑ s·ªë log-likelihood l·ªõn, ƒëi·ªÅu n√†y ch·ªâ ra r·∫±ng gi·∫£ thuy·∫øt thay th·∫ø c√≥ kh·∫£ nƒÉng ƒë√∫ng h∆°n gi·∫£ thuy·∫øt Null.

- $P_{\text{binom}}(H_0)$ l√† x√°c su·∫•t x·∫£y ra hi·ªán t∆∞·ª£ng (v√≠ d·ª•: c√≥ d·∫•u ch·∫•m) d·ª±a tr√™n gi·∫£ thuy·∫øt Null.
- $P_{\text{binom}}(H_1)$  l√† x√°c su·∫•t x·∫£y ra hi·ªán t∆∞·ª£ng d·ª±a tr√™n gi·∫£ thuy·∫øt thay th·∫ø. T·ª∑ s·ªë n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√°nh gi√° li·ªáu gi·∫£ thuy·∫øt Null c√≥ kh·∫£ nƒÉng ƒë√∫ng hay kh√¥ng. N·∫øu $\lambda$ l·ªõn, th√¨ gi·∫£ thuy·∫øt thay th·∫ø s·∫Ω c√≥ nhi·ªÅu kh·∫£ nƒÉng ƒë√∫ng h∆°n.
- V√≠ d·ª•: Gi·∫£ s·ª≠ ch√∫ng ta c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu g·ªìm 100 c√¢u, trong ƒë√≥ 50 c√¢u c√≥ d·∫•u ch·∫•m khi t·ª´ ƒë·ª©ng tr∆∞·ªõc l√† "h·ªçc", v√† 20 c√¢u c√≥ d·∫•u ch·∫•m khi t·ª´ ƒë·ª©ng tr∆∞·ªõc l√† "ch∆°i". Ch√∫ng ta c√≥ th·ªÉ t√≠nh to√°n t·ª∑ s·ªë log-likelihood ƒë·ªÉ x√°c ƒë·ªãnh xem d·∫•u ch·∫•m c√≥ ph·ª• thu·ªôc v√†o t·ª´ ƒë·ª©ng tr∆∞·ªõc hay kh√¥ng.

- **Gi·∫£ thuy·∫øt Null ƒë√£ ƒëi·ªÅu ch·ªânh (MLE)**:

$$
P( \cdot | w) = P_{\text{MLE}}( \cdot ) = \frac{C( \cdot )}{N}
$$

- C√¥ng th·ª©c n√†y s·ª≠ d·ª•ng ∆∞·ªõc l∆∞·ª£ng h·ª£p l√Ω t·ªëi ƒëa (Maximum Likelihood Estimation - MLE) ƒë·ªÉ t√≠nh x√°c su·∫•t c·ªßa d·∫•u ch·∫•m
- $C(\cdot)$ l√† s·ªë l·∫ßn xu·∫•t hi·ªán d·∫•u ch·∫•m trong t·∫≠p d·ªØ li·ªáu.
- $N$ l√† t·ªïng s·ªë c√¢u trong t·∫≠p d·ªØ li·ªáu. C√¥ng th·ª©c n√†y gi·∫£ ƒë·ªãnh r·∫±ng x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m l√† m·ªôt gi√° tr·ªã c·ªë ƒë·ªãnh ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa n√≥ trong d·ªØ li·ªáu.
  
- **Gi·∫£ thuy·∫øt Thay th·∫ø ƒë√£ ƒëi·ªÅu ch·ªânh**:

$$
P( \cdot | w) = 0.99
$$

- ƒêi·ªÅu n√†y cho th·∫•y r·∫±ng gi·∫£ thuy·∫øt thay th·∫ø kh·∫≥ng ƒë·ªãnh x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m g·∫ßn nh∆∞ ch·∫Øc ch·∫Øn (99%) n·∫øu t·ª´ tr∆∞·ªõc l√† t·ª´ ƒë·∫∑c bi·ªát n√†o ƒë√≥. Vi·ªác c√†i ƒë·∫∑t gi√° tr·ªã 0.99 thay v√¨ 1, ch√∫ng ta cho r·∫±ng m·ªôt t·ª´ vi·∫øt t·∫Øt thi tho·∫£ng s·∫Ω kh√¥ng c√≥ d·∫•u ch·∫•m ƒëi k√®m trong ng·ªØ li·ªáu.
- V√≠ d·ª•: N·∫øu ch√∫ng ta cho r·∫±ng trong 100 c√¢u, 99 c√¢u c√≥ d·∫•u ch·∫•m khi t·ª´ tr∆∞·ªõc l√† "h·ªçc", th√¨ x√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m trong tr∆∞·ªùng h·ª£p t·ª´ tr∆∞·ªõc l√† "h·ªçc" g·∫ßn nh∆∞ ch·∫Øc ch·∫Øn:

$$
P( \cdot | h·ªçc) = 0.99
$$

- C√°c c√¥ng th·ª©c likelihood gi√∫p x√°c ƒë·ªãnh m·ªëi quan h·ªá gi·ªØa t·ª´ ƒë·ª©ng tr∆∞·ªõc v√† d·∫•u c√¢u, cho ph√©p ta ƒë√°nh gi√° xem s·ª± xu·∫•t hi·ªán c·ªßa d·∫•u c√¢u c√≥ ph·ª• thu·ªôc v√†o t·ª´ tr∆∞·ªõc hay kh√¥ng. Vi·ªác t√≠nh to√°n log-likelihood s·∫Ω gi√∫p ƒë∆∞a ra k·∫øt lu·∫≠n v·ªÅ m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng c·ªßa c√°c t·ª´ ƒë·ªëi v·ªõi d·∫•u c√¢u trong m·ªôt t·∫≠p d·ªØ li·ªáu ng√¥n ng·ªØ t·ª± nhi√™n.
- C√°ch l·∫≠p tr√¨nh nh∆∞ sau:

```python
def _dunning_log_likelihood(count_a, count_b, count_ab, N):
        """
        A function that calculates the modified Dunning log-likelihood
        ratio scores for abbreviation candidates.  The details of how
        this works is available in the paper.
        """
        p1 = count_b / N
        p2 = 0.99

        null_hypo = count_ab * math.log(p1 + 1e-8) + (count_a - count_ab) * math.log(
            1.0 - p1 + 1e-8
        )
        alt_hypo = count_ab * math.log(p2) + (count_a - count_ab) * math.log(1.0 - p2)

        likelihood = null_hypo - alt_hypo

        return -2.0 * likelihood
```

- Trong tr∆∞·ªùng h·ª£p n√†y:
  - count_a s·∫Ω l√† s·ªë l·∫ßn t·ª´ n√†y xu·∫•t hi·ªán v·ªõi d·∫•u ch·∫•m c√¢u ·ªü cu·ªëi v√† s·ªë l·∫ßn kh√¥ng xu·∫•t hi·ªán d·∫•u ch·∫•m ·ªü cu·ªëi.
  - count_b s·∫Ω l√† s·ªë l∆∞·ª£ng d·∫•u ch·∫•m.
  - count_ab s·∫Ω l√† s·ªë l·∫ßn t·ª´ n√†y xu·∫•t hi·ªán v·ªõi d·∫•u ch·∫•m ·ªü cu·ªëi.
  - count_a - count_ab = s·ªë l·∫ßn t·ª´ n√†y xu·∫•t hi·ªán v·ªõi d·∫•u ch·∫•m kh√¥ng ·ªü cu·ªëi.
  - N l√† s·ªë l∆∞·ª£ng t·∫•t c·∫£ c√°c tokens trong dataset.
  - p1: X√°c su·∫•t xu·∫•t hi·ªán d·∫•u ch·∫•m ·ªü cu·ªëi.
  - p2: X√°c su·∫•t kh√¥ng xu·∫•t hi·ªán d·∫•u ch·∫•m ·ªü cu·ªëi.

- L·∫≠p tr√¨nh Gi·∫£ thuy·∫øt null:
  - S·ª≠ d·ª•ng ph√¢n ph·ªëi nh·ªã th·ª©c v·ªõi count_a l·∫ßn x·∫£y ra:
    -  T·ª´ ƒëi sau d·∫•u ch·∫•m ·ªü cu·ªëi v·ªõi x√°c su·∫•t p1 x·∫£y ra count_ab l·∫ßn v·ªõi x√°c su·∫•t p1.
    -  T·ª´ ƒëi sau kh√¥ng c√≥ d·∫•u ch·∫•m ·ªü cu·ªëi x·∫£y ra count_a - count_ab l·∫ßn v·ªõi x√°c su·∫•t 1 - p1.

```python
null_hypo = count_ab * math.log(p1 + 1e-8) + (count_a - count_ab) * math.log(1.0 - p1 + 1e-8)
```

- L·∫≠p tr√¨nh Gi·∫£ thuy·∫øt thay th·∫ø (T∆∞∆°ng t·ª± gi·∫£ thuy·∫øt null):

```python
alt_hypo = count_ab * math.log(p2) + (count_a - count_ab) * math.log(1.0 - p2)
```

- T√≠nh gi√° tr·ªã likelihood:

$$
\log \lambda = -2 \log \frac{P_{\text{binom}}(H_0)}{P_{\text{binom}}(H_A)}
\log \lambda = 2 * (\log P_{\text{binom}}(H_A) - \log P_{\text{binom}}(H_0))
$$ 

```python
likelihood = -2.0 * (null_hypo - alt_hypo)
```

#### 2. S·ª≠ d·ª•ng Likelihood trong Kh√¢u 2.

- V·ªõi m·ªôt c·∫∑p t·ª´ w1 v√† w2 xung quanh m·ªôt d·∫•u ch·∫•m v√† ch√∫ng ta ki·ªÉm tra r·∫±ng li·ªáu c√≥ vi·ªác xu·∫•t hi·ªán th∆∞·ªùng xuy√™n gi·ªØa ch√∫ng hay kh√¥ng.

- Gi·∫£ thuy·∫øt Null: 

$$
P( \cdot | w) = p = P( \cdot | \neg w)
$$

- Gi·∫£ thuy·∫øt thay th·∫ø:

$$
P( \cdot | w) = p_1 \neq P( \cdot | \neg w) = p_2
$$

- Gi√° tr·ªã $\log \lambda$  s·∫Ω l·ªõn n·∫øu x√°c su·∫•t $p_1$ v√† $p_2$ kh√°c nhau nhi·ªÅu tuy nhi√™n ta s·∫Ω quan t√¢m t·ªõi tr∆∞·ªùng h·ª£p $p_1$ l·ªõn h∆°n nhi·ªÅu $p_2$ hay $p_1$ >> $p_2$ v·ªõi √Ω nghƒ©a $w_2$ xu·∫•t hi·ªán sau w1 nhi·ªÅu h∆°n k·ª≥ v·ªçng.
- V√¨ tr∆∞·ªùng h·ª£p $p_1$ << $p_2$ hi·∫øm n√™n ta s·∫Ω coi l√† $p_1$ >> $p_2$.

- C√¥ng th·ª©c s·ª≠ d·ª•ng:

$$
\log \lambda = 0 if \frac{C(w_2)}{N} = \frac{C(w_1,w_2)}{C(w_1)}
$$

- V·∫ø ph·∫£i l√† $w_2$ hay ƒëi sau $w_1$.
- V·∫ø tr√°i l√† $w_2$ ƒëi m·ªôt m√¨nh.
- N·∫øu hai v·∫ø c·ªßa ph∆∞∆°ng tr√¨nh n√†y kh√°c nhau, gi√° tr·ªã $\log \lambda$ s·∫Ω l·ªõn h∆°n 0. Gi√° tr·ªã $\log \lambda$ l·ªõn c√≥ nghƒ©a r·∫±ng $w_1$ v√† $w_2$ hay ƒëi v·ªõi nhau hay v·∫ø ph·∫£i l·ªõn h∆°n v·∫ø tr√°i c·ªßa c√¥ng th·ª©c tr√™n. N·∫øu v·∫ø tr√°i l·ªõn h∆°n v·∫ø ph·∫£i, gi√° tr·ªã $\log \lambda$  v·∫´n l·ªõn h∆°n 0 tuy nhi√™n ph·∫£n √°nh w2 kh√¥ng xu·∫•t hi·ªán th∆∞·ªùng xuy√™n sau w1 nh∆∞ k·ª≥ v·ªçng.

```python
    def _col_log_likelihood(count_a, count_b, count_ab, N):
        """
        A function that will just compute log-likelihood estimate, in
        the original paper it's described in algorithm 6 and 7.

        This *should* be the original Dunning log-likelihood values,
        unlike the previous log_l function where it used modified
        Dunning log-likelihood values
        """
        # count_a: S·ªë l∆∞·ª£ng w1 xu·∫•t hi·ªán
        # count_b: S·ªë l∆∞·ª£ng w2 xu·∫•t hi·ªán
        # count_ab: S·ªë l∆∞·ª£ng w1w2
        # N - count_a: S·ªë l∆∞·ª£ng kh√¥ng ph·∫£i w1
        # N - count_a - count_b + count_ab: Kh√¥ng c√≥ w1 v√† w2. C√°ch vi·∫øt kh√°c: N - count_a - (count_b - count_ab)
        # count_b - count_ab: w2 ƒë·ª©ng m·ªôt m√¨nh kh√¥ng ƒëi v·ªõi w1
        # count_a - count_ab: w1 ƒë·ª©ng m·ªôt m√¨nh kh√¥ng ƒëi v·ªõi w2
        # p: X√°c su·∫•t w2 ƒë·ª©ng m·ªôt m√¨nh, s·ª≠ d·ª•ng cho null hypothesis
        p = count_b / N

        # p1: P(w2|w1)
        p1 = count_ab / count_a
        try:
            # p2: P(w2|¬¨w1)
            p2 = (count_b - count_ab) / (N - count_a)
        except ZeroDivisionError:
            p2 = 1

        try:
            # Binomial null hypothesis p. P(w2|w1)
            summand1 = count_ab * math.log(p) + (count_a - count_ab) * math.log(1.0 - p)
        except ValueError:
            summand1 = 0

        try:
            # Binomial null hypothesis p. P(w2|¬¨w1)
            summand2 = (count_b - count_ab) * math.log(p) + (
                N - count_a - count_b + count_ab
            ) * math.log(1.0 - p)
        except ValueError:
            summand2 = 0

        if count_a == count_ab or p1 <= 0 or p1 >= 1:
            summand3 = 0
        else:
            # Binomial alternative hypothesis p1: P(w2|w1)
            summand3 = count_ab * math.log(p1) + (count_a - count_ab) * math.log(
                1.0 - p1
            )

        if count_b == count_ab or p2 <= 0 or p2 >= 1:
            summand4 = 0
        else:
            # Binomial alternative hypothesis p2 P(w2|¬¨w1)
            summand4 = (count_b - count_ab) * math.log(p2) + (
                N - count_a - count_b + count_ab
            ) * math.log(1.0 - p2)

        likelihood = summand1 + summand2 - summand3 - summand4

        return -2.0 * likelihood

```

### Type-based Classification
- C√≥ ba y·∫øu t·ªë x√°c ƒë·ªãnh m·ªôt t·ª´ vi·∫øt t·∫Øt:
  - Hay ƒëi c√πng v·ªõi d·∫•u ch·∫•m ·ªü cu·ªëi.
  - Ng·∫Øn.
  - R·∫•t nhi·ªÅu t·ª´ vi·∫øt t·∫Øt ch·ª©a d·∫•u ch·∫•m b√™n trong.

![Anh](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/7b6fa1e0-aa16-11ee-9bef-cda9df06a9a6-Screen_Shot_2024_01_03_at_15.59.53_900x434.png)

- C√°c t·ª´ vi·∫øt nghi√™ng kh√¥ng ph·∫£i t·ª´ vi·∫øt t·∫Øt. Tuy nhi√™n t·ª´ *ounces* r·∫•t hay xu·∫•t hi·ªán ƒëi k√®m d·∫•u ch·∫•m ƒë·∫±ng sau (4 l·∫ßn) m√† kh√¥ng bao gi·ªù xu·∫•t hi·ªán n·∫øu kh√¥ng c√≥ d·∫•u ch·∫•m ƒëi k√®m. Cho n√™n c√°ch l√†m ch·ªâ ƒë∆°n gi·∫£n d√πng 3 ti√™u ch√≠ n√†y ch∆∞a ƒë·ªß ƒë·ªÉ ƒë√°nh gi√° t·ª´ *ounces* l√† m·ªôt t·ª´ vi·∫øt t·∫Øt. Chi·ªÅu d√†i c·ªßa t·ª´ vi·∫øt t·∫Øt ng·∫Øn h∆°n so v·ªõi t·ª´ kh√¥ng vi·∫øt t·∫Øt. Chi·ªÅu d√†i c·ªßa t·ª´ vi·∫øt t·∫Øt ng·∫Øn h∆°n so v·ªõi t·ª´ kh√¥ng vi·∫øt t·∫Øt.

#### M√¥ h√¨nh h√≥a c√°c y·∫øu t·ªë.

- Chi·ªÅu d√†i c·ªßa t·ª´ s·∫Ω lo·∫°i b·ªè s·ªë l∆∞·ª£ng c√°c d·∫•u ch·∫•m trong t·ª´. V√≠ d·ª•:

$$
length(u.s.a.) = 3
$$

- Tr∆∞·ªùng h·ª£p n√†y chi·ªÅu d√†i s·∫Ω b·∫±ng 3 v√¨ ƒë√£ lo·∫°i b·ªè s·ªë l∆∞·ª£ng d·∫•u ch·∫•m. T√≠nh ch·∫•t t·ª´ vi·∫øt t·∫Øt ng·∫Øn d·ª±a tr√™n s·ªë l∆∞·ª£ng k√Ω t·ª± kh√¥ng bao g·ªìm d·∫•u ch·∫•m.
- Gi√° tr·ªã m√¥ ph·ªèng ƒë∆∞·ª£c l·ª±a ch·ªçn d∆∞·ªõi d·∫°ng h√†m m≈© v√¨ n√≥ ph·∫£n √°nh ch√≠nh x√°c kh·∫£ nƒÉng l√† t·ª´ vi·∫øt t·∫Øt khi chi·ªÅu d√†i gi·∫£m ƒëi.

$$
F_{\text{length}(w)} = \frac{1}{e^{\text{length(w)}}}
$$

- Bi·ªÉu ƒë·ªì th·ªÉ hi·ªán m·ªëi quan h·ªá gi·ªØa chi·ªÅu d√†i c·ªßa t·ª´ v√† kh·∫£ nƒÉng t·ª´ ƒë√≥ l√† t·ª´ vi·∫øt t·∫Øt, m·ªëi quan h·ªá d∆∞·ªõi d·∫°ng h√†m m≈©. C√†ng ng·∫Øn th√¨ kh·∫£ nƒÉng l√† t·ª´ vi·∫øt t·∫Øt c√†ng cao.

![Anh](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/ebc8e1b0-aab4-11ee-9077-df5dcf1c4350-Screen_Shot_2024_01_04_at_10.54.05.png)

- H√†m ƒë·∫øm s·ªë l∆∞·ª£ng d·∫•u ch·∫•m trong t·ª´. T·ª´ c√†ng nhi·ªÅu d·∫•u ch·∫•m b√™n trong th√¨ c√≥ kh·∫£ nƒÉng c√†ng cao l√† t·ª´ vi·∫øt t·∫Øt:

![anh](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/20e42850-aab5-11ee-9bef-cda9df06a9a6-Screen_Shot_2024_01_04_at_10.55.34.png)

- T·ªïng k·∫øt c√°c y·∫øu t·ªë v√† ƒë∆∞a ra k·∫øt lu·∫≠n:

![Anh](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/9b5952a0-ac44-11ee-9bef-cda9df06a9a6-Screen_Shot_2024_01_06_at_10.35.02.png)

## 9. T√†i li·ªáu tham kh·∫£o.
- [1] [Chu·∫©n h√≥a vƒÉn b·∫£n - ProtonX](https://protonx.coursemind.io/courses/66b0895e02b79700126975cd/topics/66badf0a58f9530012731ad6?activeAId=66badf0a58f9530012731ae6)