---
title: "Hello Transformers"
date: 2025-01-17 00:00:00  + 0800
categories: [Natural Language Processing with Transformers Book]
tags: [transfomers]
---
---


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>


- VÃ o nÄƒm 2017, cÃ¡c nhÃ  nghiÃªn cá»©u táº¡i Google Ä‘Ã£ xuáº¥t báº£n má»™t bÃ i bÃ¡o Ä‘á» xuáº¥t má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron má»›i Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a trÃ¬nh tá»± ([paper](https://arxiv.org/pdf/1706.03762)). ÄÆ°á»£c Ä‘áº·t tÃªn lÃ  `Transformer`, kiáº¿n trÃºc nÃ y vÆ°á»£t trá»™i hÆ¡n cÃ¡c Recurrent Neural Wetworks (RNN) trong cÃ¡c tÃ¡c vá»¥ dá»‹ch mÃ¡y, cáº£ vá» cháº¥t lÆ°á»£ng dá»‹ch thuáº­t vÃ  chi phÃ­ Ä‘Ã o táº¡o.
- CÃ¹ng lÃºc Ä‘Ã³, cÃ³ má»™t phÆ°Æ¡ng phÃ¡p há»c chuyá»ƒn tiáº¿p hiá»‡u quáº£ (transfer learning) cÃ³ tÃªn lÃ  `ULMFiT` ([paper](https://arxiv.org/abs/1801.06146)) cho tháº¥y ráº±ng viá»‡c Ä‘Ã o táº¡o Long Short-Term Memory (LSTM) Networks trÃªn má»™t kho dá»¯ liá»‡u ráº¥t lá»›n vÃ  Ä‘a dáº¡ng cÃ³ thá»ƒ táº¡o ra má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i vÄƒn báº£n Ä‘áº¡t ngÆ°á»¡ng state-of-the-art (SOTA) vá»›i Ã­t dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n sáºµn.
- Nhá»¯ng tiáº¿n bá»™ nÃ y lÃ  cháº¥t xÃºc tÃ¡c cho hai trong sá»‘ nhá»¯ng transformers tiáº¿ng nháº¥t hiá»‡n nay:  `Generative Pretrained Transformer` ([GPT](https://openai.com/index/language-unsupervised/)) vÃ  `Bidirectional Encoder Representations from Transformers` ([BERT](https://arxiv.org/abs/1810.04805)). Báº±ng cÃ¡ch káº¿t há»£p kiáº¿n trÃºc Transformer vá»›i há»c khÃ´ng giÃ¡m sÃ¡t, cÃ¡c mÃ´ hÃ¬nh nÃ y khÃ´ng cáº§n pháº£i train tá»« nhá»¯ng task cá»¥ thá»ƒ tá»« Ä‘áº§u vÃ  phÃ¡ vá»¡ háº§u háº¿t má»i benchmark trong NLP vá»›i khoáº£ng cÃ¡ch Ä‘Ã¡ng ká»ƒ. Ká»ƒ tá»« khi phÃ¡t hÃ nh GPT vÃ  BERT, rÃ¢t nhiá»u cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn kiáº¿n trÃºc transformer Ä‘Ã£ xuáº¥t hiá»‡n; dÃ²ng thá»i gian cá»§a cÃ¡c kiáº¿n trÃºc ná»•i báº­t nháº¥t Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 1-1.

![anh](./image/273.png)

- NhÆ°ng cÃ³ váº» chÃºng ta Ä‘ang Ä‘i hÆ¡i nhanh. Äá»ƒ hiá»ƒu Ä‘iá»u gÃ¬ má»›i láº¡ vá» transformer, trÆ°á»›c tiÃªn chÃºng ta cáº§n pháº£i tÃ¬m hiá»ƒu:
1. The encoder-decoder framework.
2. CÆ¡ cháº¿ chÃº Ã½.
3. Há»c chuyá»ƒn tiáº¿p (Transfer learning).

- Trong blog nÃ y, chÃºng ta sáº½ giá»›i thiá»‡u cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi lÃ m ná»n táº£ng cho sá»± phá»• biáº¿n cá»§a transformer, xem xÃ©t má»™t sá»‘ task mÃ  chÃºng vÆ°á»£t trá»™i vÃ  káº¿t thÃºc báº±ng cÃ¡ch xem xÃ©t há»‡ sinh thÃ¡i cá»§a Hugging Face gá»“m cÃ¡c cÃ´ng cá»¥ vÃ  thÆ° viá»‡n.

- HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch khÃ¡m phÃ¡ The encoder-decoder framework vÃ  cÃ¡c kiáº¿n trÃºc cá»§a transformer.

## 1. The Encoder-Decoder Framework

- TrÆ°á»›c khi cÃ³ transformer, cÃ¡c kiáº¿n trÃºc láº·p láº¡i nhÆ° LSTM lÃ  SOTA trong NLP. CÃ¡c kiáº¿n trÃºc nÃ y chá»©a má»™t vÃ²ng pháº£n há»“i (feedback loop) trong cÃ¡c káº¿t ná»‘i máº¡ng cho phÃ©p thÃ´ng tin lan truyá»n tá»« bÆ°á»›c nÃ y sang bÆ°á»›c khÃ¡c, khiáº¿n chÃºng trá»Ÿ nÃªn lÃ½ tÆ°á»Ÿng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a dá»¯ liá»‡u tuáº§n tá»± nhÆ° vÄƒn báº£n. NhÆ° minh há»a á»Ÿ phÃ­a bÃªn trÃ¡i cá»§a HÃ¬nh 1-2, má»™t RNN nháº­n Ä‘Æ°á»£c má»™t sá»‘ Ä‘áº§u vÃ o (cÃ³ thá»ƒ lÃ  má»™t tá»« hoáº·c kÃ½ tá»±), input Ä‘i qua máº¡ng vÃ  xuáº¥t ra má»™t vectÆ¡ Ä‘Æ°á»£c gá»i lÃ  tráº¡ng thÃ¡i áº©n (hidden state). Äá»“ng thá»i, mÃ´ hÃ¬nh cung cáº¥p má»™t sá»‘ thÃ´ng tin trá»Ÿ láº¡i chÃ­nh nÃ³ thÃ´ng qua vÃ²ng pháº£n há»“i (feedback loop), sau Ä‘Ã³ nÃ³ cÃ³ thá»ƒ sá»­ dá»¥ng trong bÆ°á»›c tiáº¿p theo. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c nhÃ¬n tháº¥y rÃµ rÃ ng hÆ¡n náº¿u chÃºng ta "má»Ÿ cuá»™n" (unroll) vÃ²ng láº·p nhÆ° thá»ƒ hiá»‡n á»Ÿ phÃ­a bÃªn pháº£i cá»§a HÃ¬nh 1-2: RNN chuyá»ƒn thÃ´ng tin vá» tráº¡ng thÃ¡i cá»§a nÃ³ á»Ÿ má»—i bÆ°á»›c sang hoáº¡t Ä‘á»™ng tiáº¿p theo trong trÃ¬nh tá»±. Äiá»u nÃ y cho phÃ©p RNN theo dÃµi thÃ´ng tin tá»« cÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã³ vÃ  sá»­ dá»¥ng nÃ³ cho cÃ¡c dá»± Ä‘oÃ¡n Ä‘áº§u ra cá»§a nÃ³.

![anh](./image/274.png)

- CÃ¡c kiáº¿n trÃºc nÃ y Ä‘Ã£ (vÃ  tiáº¿p tá»¥c) Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho cÃ¡c tÃ¡c vá»¥ NLP, xá»­ lÃ½ giá»ng nÃ³i vÃ  chuá»—i thá»i gian. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y má»™t giáº£i thÃ­ch tuyá»‡t vá»i vá» kháº£ nÄƒng cá»§a chÃºng trong bÃ i Ä‘Äƒng trÃªn blog cá»§a Andrej Karpathy, [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) (cÃ³ thá»ƒ trong tÆ°Æ¡ng lai tÃ´i sáº½ dá»‹ch láº¡i blog nÃ y).
- Má»™t lÄ©nh vá»±c mÃ  RNN Ä‘Ã³ng má»™t vai trÃ² quan trá»ng lÃ  trong viá»‡c phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng dá»‹ch mÃ¡y, trong Ä‘Ã³ má»¥c tiÃªu lÃ  Ã¡nh xáº¡ má»™t chuá»—i cÃ¡c tá»« trong ngÃ´n ngá»¯ nÃ y sang ngÃ´n ngá»¯ khÃ¡c. Loáº¡i nhiá»‡m vá»¥ nÃ y thÆ°á»ng Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng bá»™ mÃ£ hÃ³a-giáº£i mÃ£ (encoder-decoder) hoáº·c kiáº¿n trÃºc trÃ¬nh tá»± Ä‘áº¿n trÃ¬nh tá»± ([sequence-to-sequence](https://arxiv.org/abs/1409.3215)), ráº¥t phÃ¹ há»£p cho cÃ¡c tÃ¬nh huá»‘ng mÃ  Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra Ä‘á»u lÃ  chuá»—i cÃ³ Ä‘á»™ dÃ i tÃ¹y Ã½. CÃ´ng viá»‡c cá»§a bá»™ mÃ£ hÃ³a lÃ  mÃ£ hÃ³a thÃ´ng tin tá»« chuá»—i Ä‘áº§u vÃ o thÃ nh má»™t biá»ƒu diá»…n sá»‘ thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  tráº¡ng thÃ¡i áº©n cuá»‘i cÃ¹ng (last hidden state). Tráº¡ng thÃ¡i nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n bá»™ giáº£i mÃ£, táº¡o ra trÃ¬nh tá»± Ä‘áº§u ra.
- NÃ³i chung, cÃ¡c thÃ nh pháº§n cá»§a bá»™ mÃ£ hÃ³a vÃ  bá»™ giáº£i mÃ£ cÃ³ thá»ƒ lÃ  báº¥t ká»³ loáº¡i kiáº¿n trÃºc máº¡ng nÆ¡-ron nÃ o cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a cÃ¡c chuá»—i. VÃ­ dá»¥ Ä‘Æ°á»£c minh há»a cho má»™t cáº·p RNNs trong HÃ¬nh 1-3, trong Ä‘Ã³ cÃ¢u tiáº¿ng Anh "Transformers are great!" Ä‘Æ°á»£c mÃ£ hÃ³a dÆ°á»›i dáº¡ng má»™t vectÆ¡ tráº¡ng thÃ¡i áº©n (hidden state) sau Ä‘Ã³ Ä‘Æ°á»£c giáº£i mÃ£ Ä‘á»ƒ táº¡o ra báº£n dá»‹ch tiáº¿ng Äá»©c "Transformer sind grossartig!" CÃ¡c tá»« Ä‘áº§u vÃ o Ä‘Æ°á»£c cung cáº¥p tuáº§n tá»± thÃ´ng qua bá»™ mÃ£ hÃ³a vÃ  cÃ¡c tá»« Ä‘áº§u ra Ä‘Æ°á»£c táº¡o tá»«ng tá»« má»™t, tá»« trÃªn xuá»‘ng dÆ°á»›i.

![anh](./image/275.png)

- Máº·c dÃ¹ nÃ³ Ä‘Æ¡n giáº£n, má»™t Ä‘iá»ƒm yáº¿u cá»§a kiáº¿n trÃºc nÃ y lÃ  tráº¡ng thÃ¡i áº©n cuá»‘i cÃ¹ng (final hidden state) cá»§a bá»™ mÃ£ hÃ³a táº¡o ra má»™t nÃºt tháº¯t cá»• chai: nÃ³ pháº£i Ä‘áº¡i diá»‡n cho Ã½ nghÄ©a cá»§a toÃ n bá»™ chuá»—i Ä‘áº§u vÃ o vÃ¬ Ä‘Ã¢y lÃ  táº¥t cáº£ nhá»¯ng gÃ¬ bá»™ giáº£i mÃ£ cÃ³ quyá»n truy cáº­p khi táº¡o Ä‘áº§u ra. Äiá»u nÃ y Ä‘áº·c biá»‡t khÃ³ khÄƒn Ä‘á»‘i vá»›i cÃ¡c chuá»—i dÃ i, nÆ¡i thÃ´ng tin á»Ÿ Ä‘áº§u chuá»—i cÃ³ thá»ƒ bá»‹ máº¥t trong quÃ¡ trÃ¬nh nÃ©n má»i thá»© thÃ nh má»™t biá»ƒu diá»…n cá»‘ Ä‘á»‹nh duy nháº¥t.
- May máº¯n thay, cÃ³ má»™t cÃ¡ch thoÃ¡t khá»i nÃºt tháº¯t cá»• chai nÃ y báº±ng cÃ¡ch cho phÃ©p bá»™ giáº£i mÃ£ cÃ³ quyá»n truy cáº­p vÃ o táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i áº©n cá»§a bá»™ mÃ£ hÃ³a. CÆ¡ cháº¿ chung cho Ä‘iá»u nÃ y Ä‘Æ°á»£c gá»i lÃ  CÆ¡ cháº¿ chÃº Ã½ ([paper](https://arxiv.org/abs/1409.0473)) vÃ  nÃ³ lÃ  má»™t thÃ nh pháº§n quan trá»ng trong nhiá»u kiáº¿n trÃºc máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i. Hiá»ƒu Ä‘Æ°á»£c cÆ¡ cháº¿ chÃº Ã½ Ä‘Æ°á»£c phÃ¡t triá»ƒn nhÆ° tháº¿ nÃ o Ä‘á»‘i vá»›i RNN sáº½ giÃºp chÃºng ta hiá»ƒu Ä‘Æ°á»£c má»™t trong nhá»¯ng khá»‘i xÃ¢y dá»±ng chÃ­nh cá»§a kiáº¿n trÃºc Transformer. ChÃºng ta hÃ£y xem xÃ©t sÃ¢u hÆ¡n thÃ´i nÃ o.

## 2. CÆ¡ cháº¿ chÃº Ã½ (Attention Mechanisms)

- Ã tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau cÆ¡ cháº¿ chÃº Ã½ lÃ  thay vÃ¬ táº¡o ra má»™t tráº¡ng thÃ¡i áº©n duy nháº¥t cho chuá»—i Ä‘áº§u vÃ o, bá»™ mÃ£ hÃ³a xuáº¥t ra má»™t tráº¡ng thÃ¡i áº©n á»Ÿ má»—i bÆ°á»›c mÃ  bá»™ giáº£i mÃ£ cÃ³ thá»ƒ truy cáº­p. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i cÃ¹ng má»™t lÃºc sáº½ táº¡o ra má»™t Ä‘áº§u vÃ o khá»•ng lá»“ cho bá»™ giáº£i mÃ£, vÃ¬ váº­y cáº§n cÃ³ má»™t cÆ¡ cháº¿ Ä‘á»ƒ Æ°u tiÃªn sá»­ dá»¥ng tráº¡ng thÃ¡i nÃ o. ÄÃ¢y lÃ  lÃºc cÆ¡ cháº¿ chÃº Ã½ xuáº¥t hiá»‡n: nÃ³ cho phÃ©p bá»™ giáº£i mÃ£ gÃ¡n cÃ¡c trá»ng lÆ°á»£ng khÃ¡c nhau, hoáº·c "chÃº Ã½", cho tá»«ng tráº¡ng thÃ¡i bá»™ mÃ£ hÃ³a á»Ÿ má»—i bÆ°á»›c giáº£i mÃ£. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1-4:

![anh](./image/276.png)

- Báº±ng cÃ¡ch táº­p trung vÃ o mÃ£ thÃ´ng bÃ¡o Ä‘áº§u vÃ o nÃ o cÃ³ liÃªn quan nháº¥t táº¡i má»—i bÆ°á»›c giáº£i mÃ£, cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn sá»± chÃº Ã½ nÃ y cÃ³ thá»ƒ tÃ¬m hiá»ƒu sá»± liÃªn káº¿t khÃ´ng táº§m thÆ°á»ng giá»¯a cÃ¡c tá»« trong cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch vÃ  cÃ¡c tá»« trong cÃ¢u gá»‘c. VÃ­ dá»¥, HÃ¬nh 1-5 trá»±c quan hÃ³a trá»ng sá»‘ chÃº Ã½ cho mÃ´ hÃ¬nh dá»‹ch tiáº¿ng Anh sang tiáº¿ng PhÃ¡p, trong Ä‘Ã³ má»—i pixel biá»ƒu thá»‹ má»™t trá»ng lÆ°á»£ng. HÃ¬nh áº£nh cho tháº¥y cÃ¡ch bá»™ giáº£i mÃ£ cÃ³ thá»ƒ cÄƒn chá»‰nh chÃ­nh xÃ¡c cÃ¡c tá»« "zone" vÃ  "Area", máº·c dÃ¹ cÃ¡c tá»« nÃ y á»Ÿ vá»‹ trÃ­ khÃ¡c nhau trong cÃ¢u tiáº¿ng Anh vÃ  tiáº¿ng PhÃ¡p.

![anh](./image/277.png)

- Máº·c dÃ¹ cÆ¡ cháº¿ chÃº Ã½ cho phÃ©p táº¡o ra cÃ¡c báº£n dá»‹ch tá»‘t hÆ¡n, nhÆ°ng váº«n cÃ³ má»™t thiáº¿u sÃ³t lá»›n khi sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh láº·p láº¡i (nhÆ° RNN) cho bá»™ mÃ£ hÃ³a vÃ  bá»™ giáº£i mÃ£: Náº¿u sá»­ dá»¥ng RNN thÃ¬ pháº£i mÃ£ hÃ³a tuáº§n tá»± tá»«ng token cá»§a cÃ¢u Ä‘áº§u vÃ o chá»© khÃ´ng thá»ƒ mÃ£ hÃ³a cÃ¹ng má»™t lÃºc, song song táº¥t cáº£ token.
- Vá»›i transformer, má»™t kiáº¿n trÃºc mÃ´ hÃ¬nh má»›i Ä‘Ã£ Ä‘Æ°á»£c giá»›i thiá»‡u: loáº¡i bá» hoÃ n toÃ n sá»± láº·p láº¡i, vÃ  thay vÃ o Ä‘Ã³ dá»±a hoÃ n toÃ n vÃ o má»™t hÃ¬nh thá»©c chÃº Ã½ Ä‘áº·c biá»‡t Ä‘Æ°á»£c gá»i lÃ  tá»± chÃº Ã½ (self-attention). ChÃºng ta sáº½ Ä‘á» cáº­p Ä‘áº¿n self-attention vÃ o blog khÃ¡c, nhÆ°ng Ã½ tÆ°á»Ÿng cÆ¡ báº£n lÃ  cho phÃ©p sá»± chÃº Ã½ hoáº¡t Ä‘á»™ng trÃªn táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i trong cÃ¹ng má»™t lá»›p cá»§a máº¡ng tháº§n kinh. Äiá»u nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 1-6, trong Ä‘Ã³ cáº£ bá»™ mÃ£ hÃ³a vÃ  bá»™ giáº£i mÃ£ Ä‘á»u cÃ³ cÆ¡ cháº¿ tá»± chÃº Ã½ riÃªng, cÃ³ Ä‘áº§u ra Ä‘Æ°á»£c Ä‘Æ°a vÃ o cÃ¡c feed-forward neural networks (FFNN). Kiáº¿n trÃºc nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c train nhanh hÆ¡n nhiá»u so vá»›i cÃ¡c mÃ´ hÃ¬nh láº·p láº¡i (recurrent models ) vÃ  má»Ÿ Ä‘Æ°á»ng cho nhiá»u Ä‘á»™t phÃ¡ gáº§n Ä‘Ã¢y trong NLP.

![anh](./image/278.png)

- Trong bÃ i bÃ¡o gá»‘c cá»§a Transformer, mÃ´ hÃ¬nh dá»‹ch thuáº­t Ä‘Æ°á»£c Ä‘Ã o táº¡o tá»« Ä‘áº§u trÃªn má»™t kho dá»¯ liá»‡u lá»›n cÃ¡c cáº·p cÃ¢u báº±ng nhiá»u ngÃ´n ngá»¯ khÃ¡c nhau. Tuy nhiÃªn, trong nhiá»u á»©ng dá»¥ng thá»±c táº¿ cá»§a NLP, chÃºng ta khÃ´ng cÃ³ quyá»n truy cáº­p vÃ o má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u vÄƒn báº£n Ä‘Æ°á»£c gáº¯n nhÃ£n Ä‘á»ƒ Ä‘Ã o táº¡o cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng ta. Tiáº¿p theo chÃºng ta sáº½ nÃ³i vá» há»c chuyá»ƒn tiáº¿p (transfer learning)

## 3. Há»c chuyá»ƒn tiáº¿p trong NLP

- NgÃ y nay, cÃ¡ch lÃ m phá»• biáº¿n trong thá»‹ giÃ¡c mÃ¡y tÃ­nh lÃ  sá»­ dá»¥ng há»c chuyá»ƒn tiáº¿p Ä‘á»ƒ Ä‘Ã o táº¡o má»™t máº¡ng nÆ¡-ron tÃ­ch cháº­p nhÆ° ResNet trÃªn má»™t nhiá»‡m vá»¥, sau Ä‘Ã³ Ä‘iá»u chá»‰nh nÃ³ hoáº·c tinh chá»‰nh nÃ³ trÃªn má»™t nhiá»‡m vá»¥ má»›i. Äiá»u nÃ y cho phÃ©p máº¡ng sá»­ dá»¥ng kiáº¿n thá»©c há»c Ä‘Æ°á»£c tá»« nhiá»‡m vá»¥ ban Ä‘áº§u. Vá» máº·t kiáº¿n trÃºc, Ä‘iá»u nÃ y liÃªn quan Ä‘áº¿n viá»‡c tÃ¡ch mÃ´ hÃ¬nh thÃ nh hai pháº§n: má»™t body vÃ  má»™t head, trong Ä‘Ã³ head lÃ  má»™t máº¡ng lÆ°á»›i dÃ nh riÃªng cho nhiá»‡m vá»¥ cá»¥ thá»ƒ. Trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o, trá»ng sá»‘ cá»§a body há»c cÃ¡c Ä‘áº·c Ä‘iá»ƒm há»¯u Ã­ch cá»§a nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³ vÃ  cÃ¡c trá»ng sá»‘ nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh má»›i cho nhiá»‡m vá»¥ má»›i. So vá»›i há»c cÃ³ giÃ¡m sÃ¡t truyá»n thá»‘ng, cÃ¡ch tiáº¿p cáº­n nÃ y thÆ°á»ng táº¡o ra cÃ¡c mÃ´ hÃ¬nh cháº¥t lÆ°á»£ng cao cÃ³ thá»ƒ Ä‘Æ°á»£c train hiá»‡u quáº£ hÆ¡n nhiá»u trÃªn nhiá»u nhiá»‡m vá»¥ downstream vÃ  vá»›i Ã­t dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n hÆ¡n nhiá»u. So sÃ¡nh hai cÃ¡ch tiáº¿p cáº­n Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 1-7.

![anh](./image/279.png)

- Trong thá»‹ giÃ¡c mÃ¡y tÃ­nh, cÃ¡c mÃ´ hÃ¬nh Ä‘áº§u tiÃªn sáº½ Ä‘Æ°á»£c train trÃªn cÃ¡c bá»™ dá»¯ liá»‡u quy mÃ´ lá»›n nhÆ° ImageNet, chá»©a hÃ ng triá»‡u hÃ¬nh áº£nh. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  pretraining vÃ  má»¥c Ä‘Ã­ch chÃ­nh cá»§a nÃ³ lÃ  dáº¡y cho mÃ´ hÃ¬nh cÃ¡c tÃ­nh nÄƒng cÆ¡ báº£n cá»§a hÃ¬nh áº£nh, cháº³ng háº¡n nhÆ° cáº¡nh hoáº·c mÃ u sáº¯c. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c nÃ y sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh theo má»™t nhiá»‡m vá»¥ downstream nhÆ° phÃ¢n loáº¡i cÃ¡c loÃ i hoa. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh thÆ°á»ng Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c train tá»« Ä‘áº§u trÃªn cÃ¹ng má»™t lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n.

- Máº·c dÃ¹ há»c chuyá»ƒn tiáº¿p Ä‘Ã£ trá»Ÿ thÃ nh cÃ¡ch tiáº¿p cáº­n tiÃªu chuáº©n trong thá»‹ giÃ¡c mÃ¡y tÃ­nh, nhÆ°ng trong nhiá»u nÄƒm, ngÆ°á»i ta khÃ´ng rÃµ há»c chuyá»ƒn tiáº¿p tÆ°Æ¡ng tá»± cho NLP lÃ  gÃ¬. Do Ä‘Ã³, cÃ¡c á»©ng dá»¥ng NLP thÆ°á»ng yÃªu cáº§u má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao. VÃ  ngay cáº£ tháº¿, hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh NLP Ä‘Ã³ cÅ©ng khÃ´ng thá»ƒ so sÃ¡nh vá»›i nhá»¯ng gÃ¬ mÃ´ hÃ¬nh khÃ¡c Ä‘áº¡t Ä‘Æ°á»£c trong lÄ©nh vá»±c thá»‹ giÃ¡c mÃ¡y tÃ­nh.
- VÃ o nÄƒm 2017 vÃ  2018, má»™t sá»‘ nhÃ³m nghiÃªn cá»©u Ä‘Ã£ Ä‘á» xuáº¥t cÃ¡c phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n má»›i Ä‘Ã£ lÃ m cho há»c chuyá»ƒn tiáº¿p hoáº¡t Ä‘á»™ng trÃªn NLP. NÃ³ báº¯t Ä‘áº§u tá»« cÃ¡i nhÃ¬n sÃ¢u sáº¯c tá»« cÃ¡c nhÃ  nghiÃªn cá»©u táº¡i OpenAI, nhá»¯ng ngÆ°á»i Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t máº¡nh máº½ trong nhiá»‡m vá»¥ phÃ¢n loáº¡i cáº£m xÃºc báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c tÃ­nh nÄƒng Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« unsupervised pretraining ([paper](https://arxiv.org/abs/1704.01444)). Tiáº¿p theo lÃ  ULMFiT, giá»›i thiá»‡u má»™t framework chung Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c mÃ´ hÃ¬nh LSTM Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau (Má»™t cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y lÃ  ELMo (Embeddings from Language Models), cho tháº¥y LSTM Ä‘Ã o táº¡o trÆ°á»›c cÃ³ thá»ƒ táº¡o ra cÃ¡c word embeddings cháº¥t lÆ°á»£ng cao cho cÃ¡c nhiá»‡m vá»¥ downstream).
- NhÆ° minh há»a trong HÃ¬nh 1-8, ULMFiT bao gá»“m ba bÆ°á»›c chÃ­nh:
1. **Pretraining:** Má»¥c tiÃªu train ban Ä‘áº§u khÃ¡ Ä‘Æ¡n giáº£n: dá»± Ä‘oÃ¡n tá»« tiáº¿p theo dá»±a trÃªn cÃ¡c tá»« trÆ°á»›c Ä‘Ã³. Nhiá»‡m vá»¥ nÃ y Ä‘Æ°á»£c gá»i lÃ  mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ (language modeling). Sá»± hay ho cá»§a cÃ¡ch tiáº¿p cáº­n nÃ y náº±m á»Ÿ chá»— khÃ´ng cáº§n dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n vÃ  ngÆ°á»i ta cÃ³ thá»ƒ sá»­ dá»¥ng vÄƒn báº£n cÃ³ sáºµn phong phÃº tá»« cÃ¡c nguá»“n nhÆ° Wikipedia (Äiá»u nÃ y Ä‘Ãºng vá»›i tiáº¿ng Anh hÆ¡n lÃ  Ä‘á»‘i vá»›i háº§u háº¿t cÃ¡c ngÃ´n ngá»¯ trÃªn tháº¿ giá»›i, nÆ¡i cÃ³ thá»ƒ khÃ³ khÄƒn Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t kho dá»¯ liá»‡u lá»›n vÄƒn báº£n Ä‘Æ°á»£c sá»‘ hÃ³a. TÃ¬m cÃ¡ch thu háº¹p khoáº£ng cÃ¡ch nÃ y lÃ  má»™t lÄ©nh vá»±c tÃ­ch cá»±c cá»§a nghiÃªn cá»©u vÃ  hoáº¡t Ä‘á»™ng NLP).
2. **Domain adaptation:** Má»™t khi mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trÃªn má»™t kho dá»¯ liá»‡u quy mÃ´ lá»›n, bÆ°á»›c tiáº¿p theo lÃ  Ä‘iá»u chá»‰nh nÃ³ cho phÃ¹ há»£p vá»›i kho dá»¯ liá»‡u mÃ  báº¡n quan tÃ¢m (vÃ­ dá»¥: MÃ´ hÃ¬nh Ä‘Æ°á»£c train trÃªn Wikipedia vÃ  dá»¯ liá»‡u mÃ  báº¡n quan tÃ¢m lÃ  cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim trÃªn IMDb, nhÆ° trong HÃ¬nh 1-8). Giai Ä‘oáº¡n nÃ y váº«n sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯, nhÆ°ng bÃ¢y giá» mÃ´ hÃ¬nh pháº£i dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong kho dá»¯ liá»‡u mÃ  báº¡n Ä‘ang quan tÃ¢m.
3. **Fine-tuning:** Trong bÆ°á»›c nÃ y, mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c tinh chá»‰nh vá»›i má»™t lá»›p phÃ¢n loáº¡i náº±m á»Ÿ cuá»‘i mÃ´ hÃ¬nh cho nhiá»‡m vá»¥ cá»¥ thá»ƒ (vÃ­ dá»¥: phÃ¢n loáº¡i cáº£m xÃºc cá»§a cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim trong HÃ¬nh 1-8).

![anh](./image/280.png)

- Báº±ng cÃ¡ch giá»›i thiá»‡u má»™t framework kháº£ thi Ä‘á»ƒ pretraining vÃ  há»c chuyá»ƒn tiáº¿p (transfer learning) trong NLP, ULMFiT nhÆ° má»™t bá»‡ phÃ³ng lÃ m cho kiáº¿n trÃºc transformers cáº¥t cÃ¡nh. VÃ o nÄƒm 2018, hai kiáº¿n trÃºc transformers káº¿t há»£p sá»± tá»± chÃº Ã½ (self-attention) vá»›i há»c chuyá»ƒn tiáº¿p Ä‘Æ°á»£c ra máº¯t:

  - **GPT:** Chá»‰ sá»­ dá»¥ng pháº§n bá»™ giáº£i mÃ£ (decoder) cá»§a kiáº¿n trÃºc Transformer vÃ  cÃ¡ch tiáº¿p cáº­n mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ (language modeling) tÆ°Æ¡ng tá»± nhÆ° ULMFiT. GPT Ä‘Ã£ Ä‘Æ°á»£c pretrain trÃªn BookCorpus ([paper](https://arxiv.org/abs/1506.06724)) bao gá»“m 7.000 cuá»‘n sÃ¡ch chÆ°a xuáº¥t báº£n thuá»™c nhiá»u thá»ƒ loáº¡i khÃ¡c nhau bao gá»“m PhiÃªu lÆ°u, Giáº£ tÆ°á»Ÿng vÃ  LÃ£ng máº¡n.
  - **BERT:** Sá»­ dá»¥ng pháº§n bá»™ mÃ£ hÃ³a (encoder) cá»§a kiáº¿n trÃºc Transformer vÃ  má»™t hÃ¬nh thá»©c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘áº·c biá»‡t Ä‘Æ°á»£c gá»i lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ máº·t náº¡ (masked language modeling). Má»¥c tiÃªu cá»§a masked language modeling lÃ  dá»± Ä‘oÃ¡n cÃ¡c tá»« Ä‘Æ°á»£c che giáº¥u ngáº«u nhiÃªn trong má»™t vÄƒn báº£n. VÃ­ dá»¥: cho má»™t cÃ¢u nhÆ° "TÃ´i Ä‘Ã£ nhÃ¬n vÃ o [MASK] cá»§a tÃ´i vÃ  tháº¥y ráº±ng [MASK] Ä‘Ã£ muá»™n", mÃ´ hÃ¬nh cáº§n dá»± Ä‘oÃ¡n cÃ¡c tá»« cÃ³ kháº£ nÄƒng nháº¥t cho cÃ¡c tá»« Ä‘Æ°á»£c che giáº¥u Ä‘Æ°á»£c kÃ½ hiá»‡u báº±ng [MASK]. BERT Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trÃªn BookCorpus vÃ  Wikipedia tiáº¿ng Anh.

- GPT vÃ  BERT Ä‘Ã£ táº¡o ra SOTA má»›i trÃªn nhiá»u Ä‘iá»ƒm chuáº©n NLP vÃ  má»Ÿ ra ká»· nguyÃªn cá»§a Transformers.

## 4. Hugging Face Transformers: Thu háº¹p khoáº£ng cÃ¡ch

- Ãp dá»¥ng kiáº¿n trÃºc há»c mÃ¡y má»›i cho má»™t nhiá»‡m vá»¥ má»›i cÃ³ thá»ƒ lÃ  má»™t cÃ´ng viá»‡c phá»©c táº¡p vÃ  thÆ°á»ng bao gá»“m cÃ¡c bÆ°á»›c sau:
1. Triá»ƒn khai kiáº¿n trÃºc mÃ´ hÃ¬nh trong code, thÆ°á»ng dá»±a trÃªn PyTorch hoáº·c TensorFlow.
2. Load cÃ¡c pretrained weights (náº¿u cÃ³ sáºµn) tá»« server.
3. Xá»­ lÃ½ trÆ°á»›c cÃ¡c Ä‘áº§u vÃ o, chuyá»ƒn chÃºng qua mÃ´ hÃ¬nh vÃ  Ã¡p dá»¥ng má»™t sá»‘ xá»­ lÃ½ cho Ä‘áº§u ra (dÃ nh riÃªng cho má»—i nhiá»‡m vá»¥ cá»¥ thá»ƒ).
4. Triá»ƒn khai bá»™ náº¡p dá»¯ liá»‡u (dataloaders) vÃ  xÃ¡c Ä‘á»‹nh hÃ m máº¥t mÃ¡t (loss functions) vÃ  trÃ¬nh tá»‘i Æ°u hÃ³a (optimizers) Ä‘á»ƒ train mÃ´ hÃ¬nh.

- Má»—i bÆ°á»›c nÃ y yÃªu cáº§u logic tÃ¹y chá»‰nh cho tá»«ng mÃ´ hÃ¬nh vÃ  nhiá»‡m vá»¥. ThÆ°á»ng thÃ¬ (nhÆ°ng khÃ´ng pháº£i lÃºc nÃ o cÅ©ng tháº¿), khi cÃ¡c nhÃ³m nghiÃªn cá»©u xuáº¥t báº£n má»™t bÃ i bÃ¡o má»›i, há» cÅ©ng sáº½ phÃ¡t hÃ nh code cÃ¹ng vá»›i trá»ng sá»‘ mÃ´ hÃ¬nh. Tuy nhiÃªn, code nÃ y hiáº¿m khi cÃ³ thá»ƒ dÃ¹ng ngay Ä‘Æ°á»£c vÃ  thÆ°á»ng cáº§n nhiá»u ngÃ y chá»‰nh sá»­a Ä‘á»ƒ cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘Æ°á»£c.
- ÄÃ¢y chÃ­nh lÃ  lÃºc [ğŸ¤— Transformers](https://github.com/huggingface/transformers) trá»Ÿ thÃ nh cá»©u cÃ¡nh cho cÃ¡c nhÃ  thá»±c hÃ nh NLP! ThÆ° viá»‡n nÃ y cung cáº¥p má»™t giao diá»‡n tiÃªu chuáº©n hÃ³a cho nhiá»u mÃ´ hÃ¬nh transformer khÃ¡c nhau, cÃ¹ng vá»›i code vÃ  cÃ´ng cá»¥ Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c mÃ´ hÃ¬nh nÃ y cho nhá»¯ng trÆ°á»ng há»£p sá»­ dá»¥ng má»›i. Hiá»‡n táº¡i, thÆ° viá»‡n há»— trá»£ ba framework há»c sÃ¢u chÃ­nh (PyTorch, TensorFlow vÃ  JAX) vÃ  cho phÃ©p báº¡n dá»… dÃ ng chuyá»ƒn Ä‘á»•i giá»¯a chÃºng. NgoÃ i ra, nÃ³ cÃ²n cung cáº¥p cÃ¡c thÃ nh pháº§n Ä‘áº·c thÃ¹ theo nhiá»‡m vá»¥, giÃºp báº¡n dá»… dÃ ng tinh chá»‰nh cÃ¡c transformer cho cÃ¡c tÃ¡c vá»¥ downstream nhÆ° phÃ¢n loáº¡i vÄƒn báº£n (text classification), nháº­n dáº¡ng thá»±c thá»ƒ cÃ³ tÃªn (named entity recognition), vÃ  tráº£ lá»i cÃ¢u há»i (question answering). Äiá»u nÃ y giÃºp giáº£m thá»i gian cáº§n thiáº¿t Ä‘á»ƒ má»™t nhÃ  thá»±c hÃ nh train vÃ  test má»™t sá»‘ mÃ´ hÃ¬nh tá»« vÃ i tuáº§n xuá»‘ng chá»‰ cÃ²n má»™t buá»•i chiá»u!
- Báº¡n sáº½ tá»± mÃ¬nh tháº¥y Ä‘iá»u nÃ y trong pháº§n tiáº¿p theo, nÆ¡i chÃºng ta chá»‰ cáº§n vá»›i má»™t vÃ i dÃ²ng code, ğŸ¤— Transformers cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t má»™t sá»‘ á»©ng dá»¥ng NLP phá»• biáº¿n nháº¥t mÃ  báº¡n cÃ³ thá»ƒ gáº·p pháº£i trong tá»± nhiÃªn.

## 5. CÃ¡c á»©ng dá»¥ng Transformer phá»• biáº¿n

- Má»—i nhiá»‡m vá»¥ NLP Ä‘á»u báº¯t Ä‘áº§u báº±ng má»™t Ä‘oáº¡n vÄƒn báº£n, vÃ­ dá»¥ nhÆ° feedback cá»§a khÃ¡ch hÃ ng:

```python
text = """Dear Amazon, last week I ordered an Optimus Prime action figure
from your online store in Germany. Unfortunately, when I opened the package,
I discovered to my horror that I had been sent an action figure of Megatron
instead! As a lifelong enemy of the Decepticons, I hope you can understand my
dilemma. To resolve the issue, I demand an exchange of Megatron for the
Optimus Prime figure I ordered. Enclosed are copies of my records concerning
this purchase. I expect to hear from you soon. Sincerely, Bumblebee."""
```

- TÃ¹y thuá»™c vÃ o á»©ng dá»¥ng cá»§a báº¡n, vÄƒn báº£n báº¡n Ä‘ang lÃ m viá»‡c cÃ³ thá»ƒ lÃ  há»£p Ä‘á»“ng phÃ¡p lÃ½, mÃ´ táº£ sáº£n pháº©m hoáº·c má»™t cÃ¡i gÃ¬ Ä‘Ã³ hoÃ n toÃ n khÃ¡c. Trong trÆ°á»ng há»£p pháº£n há»“i cá»§a khÃ¡ch hÃ ng, báº¡n cÃ³ thá»ƒ muá»‘n biáº¿t pháº£n há»“i lÃ  tÃ­ch cá»±c hay tiÃªu cá»±c. Nhiá»‡m vá»¥ nÃ y Ä‘Æ°á»£c gá»i lÃ  phÃ¢n tÃ­ch cáº£m xÃºc (sentiment analysis) vÃ  lÃ  má»™t pháº§n cá»§a chá»§ Ä‘á» rá»™ng hÆ¡n lÃ  phÃ¢n loáº¡i vÄƒn báº£n (text classification). BÃ¢y giá», chÃºng ta hÃ£y xem nhá»¯ng gÃ¬ cáº§n thiáº¿t Ä‘á»ƒ trÃ­ch xuáº¥t cáº£m xÃºc tá»« Ä‘oáº¡n vÄƒn báº£n cá»§a chÃºng ta báº±ng cÃ¡ch sá»­ dá»¥ng ğŸ¤— Transformers.


### PhÃ¢n loáº¡i vÄƒn báº£n (Text Classification)

- ğŸ¤— Transformers cÃ³ má»™t API phÃ¢n lá»›p cho phÃ©p báº¡n tÆ°Æ¡ng tÃ¡c vá»›i thÆ° viá»‡n á»Ÿ nhiá»u má»©c Ä‘á»™ trá»«u tÆ°á»£ng khÃ¡c nhau. Trong chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ báº¯t Ä‘áº§u vá»›i `pipeline`, má»™t cÃ´ng cá»¥ trá»«u tÆ°á»£ng hÃ³a táº¥t cáº£ cÃ¡c bÆ°á»›c cáº§n thiáº¿t Ä‘á»ƒ chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ´ thÃ nh má»™t táº­p há»£p dá»± Ä‘oÃ¡n tá»« má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh (fine-tuned model).

Trong ğŸ¤— Transformers, chÃºng ta khá»Ÿi táº¡o má»™t `pipeline` báº±ng cÃ¡ch gá»i hÃ m `pipeline()` vÃ  cung cáº¥p tÃªn cá»§a tÃ¡c vá»¥ mÃ  chÃºng ta quan tÃ¢m:

```python
from transformers import pipeline
classifier = pipeline("text-classification")
```

- Láº§n Ä‘áº§u tiÃªn báº¡n cháº¡y code nÃ y, báº¡n sáº½ tháº¥y má»™t vÃ i thanh tiáº¿n trÃ¬nh xuáº¥t hiá»‡n vÃ¬ `pipeline` tá»± Ä‘á»™ng táº£i xuá»‘ng trá»ng sá»‘ mÃ´ hÃ¬nh tá»« Hub Hugging Face. Láº§n thá»© hai báº¡n khá»Ÿi táº¡o quy trÃ¬nh, thÆ° viá»‡n sáº½ nháº­n tháº¥y ráº±ng báº¡n Ä‘Ã£ táº£i xuá»‘ng trá»ng sá»‘ vÃ  sáº½ sá»­ dá»¥ng phiÃªn báº£n Ä‘Æ°á»£c lÆ°u trong bá»™ nhá»› Ä‘á»‡m Ä‘á»ƒ thay tháº¿. Theo máº·c Ä‘á»‹nh, text-classification pipeline sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ phÃ¢n tÃ­ch cáº£m xÃºc, nhÆ°ng nÃ³ cÅ©ng há»— trá»£ phÃ¢n loáº¡i Ä‘a lá»›p vÃ  Ä‘a nhÃ£n.
- BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ `pipeline` cá»§a mÃ¬nh, hÃ£y táº¡o ra má»™t sá»‘ dá»± Ä‘oÃ¡n! Má»—i `pipeline` láº¥y má»™t chuá»—i vÄƒn báº£n (hoáº·c danh sÃ¡ch cÃ¡c chuá»—i) lÃ m Ä‘áº§u vÃ o vÃ  tráº£ vá» danh sÃ¡ch cÃ¡c dá»± Ä‘oÃ¡n. Má»—i dá»± Ä‘oÃ¡n lÃ  má»™t tá»« Ä‘iá»ƒn Python, vÃ¬ váº­y chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng `Pandas` Ä‘á»ƒ hiá»ƒn thá»‹ chÃºng má»™t cÃ¡ch Ä‘áº¹p Ä‘áº½ dÆ°á»›i dáº¡ng `DataFrame`:

![anh](./image/281.png)

- Trong trÆ°á»ng há»£p nÃ y, mÃ´ hÃ¬nh ráº¥t tá»± tin ráº±ng vÄƒn báº£n cÃ³ cáº£m xÃºc tiÃªu cá»±c, Ä‘iá»u nÃ y cÃ³ Ã½ nghÄ©a khi chÃºng ta Ä‘ang giáº£i quyáº¿t khiáº¿u náº¡i tá»« má»™t khÃ¡ch hÃ ng tá»©c giáº­n! LÆ°u Ã½ ráº±ng Ä‘á»‘i vá»›i cÃ¡c tÃ¡c vá»¥ phÃ¢n tÃ­ch cáº£m xÃºc, `pipeline` chá»‰ tráº£ vá» má»™t trong hai nhÃ£n POSITIVE hoáº·c NEGATIVE, vÃ¬ nhÃ£n cÃ²n láº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c suy ra báº±ng cÃ¡ch tÃ­nh 1 - score.
- BÃ¢y giá» chÃºng ta hÃ£y xem xÃ©t má»™t nhiá»‡m vá»¥ phá»• biáº¿n khÃ¡c, xÃ¡c Ä‘á»‹nh cÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn trong vÄƒn báº£n.


### Nháº­n dáº¡nng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (Named Entity Recognition)

- Dá»± Ä‘oÃ¡n cáº£m xÃºc vá» pháº£n há»“i cá»§a khÃ¡ch hÃ ng lÃ  bÆ°á»›c khá»Ÿi Ä‘áº§u tá»‘t, nhÆ°ng báº¡n thÆ°á»ng muá»‘n biáº¿t liá»‡u pháº£n há»“i cÃ³ liÃªn quan Ä‘áº¿n má»™t máº·t hÃ ng hoáº·c dá»‹ch vá»¥ cá»¥ thá»ƒ hay khÃ´ng. Trong NLP, cÃ¡c Ä‘á»‘i tÆ°á»£ng trong tháº¿ giá»›i thá»±c nhÆ° sáº£n pháº©m, Ä‘á»‹a Ä‘iá»ƒm hoáº·c con ngÆ°á»i Ä‘Æ°á»£c gá»i lÃ  thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (named entities) vÃ  trÃ­ch xuáº¥t chÃºng tá»« vÄƒn báº£n Ä‘Æ°á»£c gá»i lÃ  nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (NER). ChÃºng tÃ´i cÃ³ thá»ƒ Ã¡p dá»¥ng NER báº±ng cÃ¡ch táº£i `pipeline` tÆ°Æ¡ng á»©ng:

```python
ner_tagger = pipeline("ner", aggregation_strategy="simple")
outputs = ner_tagger(text)
pd.DataFrame(outputs)
```

![anh](./image/282.png)

- Báº¡n cÃ³ thá»ƒ tháº¥y ráº±ng `pipeline` Ä‘Ã£ phÃ¡t hiá»‡n táº¥t cáº£ cÃ¡c thá»±c thá»ƒ vÃ  cÅ©ng gÃ¡n má»™t danh má»¥c nhÆ° `ORG` (tá»• chá»©c), `LOC` (Ä‘á»‹a Ä‘iá»ƒm) hoáº·c `PER` (ngÆ°á»i) cho má»—i thá»±c thá»ƒ Ä‘Ã³. á» Ä‘Ã¢y, chÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng tham sá»‘ `aggregation_strategy` Ä‘á»ƒ nhÃ³m cÃ¡c tá»« theo dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. VÃ­ dá»¥, thá»±c thá»ƒ â€œOptimus Primeâ€ Ä‘Æ°á»£c táº¡o thÃ nh tá»« hai tá»«, nhÆ°ng Ä‘Æ°á»£c gÃ¡n má»™t danh má»¥c duy nháº¥t: MISC (loáº¡i khÃ¡c). CÃ¡c Ä‘iá»ƒm sá»‘ cho chÃºng ta biáº¿t mÃ´ hÃ¬nh tá»± tin Ä‘áº¿n má»©c nÃ o vá» cÃ¡c thá»±c thá»ƒ mÃ  nÃ³ nháº­n diá»‡n. ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng mÃ´ hÃ¬nh Ã­t tá»± tin nháº¥t vá» â€œDecepticonsâ€ vÃ  tá»« â€œMegaâ€, cáº£ hai Ä‘á»u khÃ´ng Ä‘Æ°á»£c nhÃ³m thÃ nh má»™t thá»±c thá»ƒ duy nháº¥t.

> Tháº¥y nhá»¯ng kÃ½ hiá»‡u # trong cá»™t tá»« á»Ÿ báº£ng trÃªn khÃ´ng? ChÃºng Ä‘Æ°á»£c táº¡o ra bá»Ÿi tokenizer cá»§a mÃ´ hÃ¬nh, chia cÃ¡c tá»« thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ nhá» Ä‘Æ°á»£c gá»i lÃ  token. Báº¡n sáº½ tÃ¬m hiá»ƒu táº¥t cáº£ vá» cÃ¡i nÃ y á»Ÿ blog khÃ¡c.

- TrÃ­ch xuáº¥t táº¥t cáº£ cÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn trong má»™t vÄƒn báº£n tháº­t tuyá»‡t, nhÆ°ng Ä‘Ã´i khi chÃºng ta muá»‘n Ä‘áº·t cÃ¢u há»i vá» vÄƒn báº£n. Äáº¥y lÃ  lÃ  lÃºc chÃºng ta Ä‘i tá»›i nhiá»‡m vá»¥ ná»¯a lÃ  tráº£ lá»i cÃ¢u há»i (question answering).

### Tráº£ lá»i cÃ¢u há»i (Question Answering)


- Trong tráº£ lá»i cÃ¢u há»i, chÃºng ta cung cáº¥p cho mÃ´ hÃ¬nh má»™t Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c gá»i lÃ  ngá»¯ cáº£nh (context), cÃ¹ng vá»›i má»™t cÃ¢u há»i mÃ  chÃºng ta muá»‘n mÃ´ hÃ¬nh cÃ¢u tráº£ lá»i. Sau Ä‘Ã³, mÃ´ hÃ¬nh tráº£ vá» khoáº£ng vÄƒn báº£n tÆ°Æ¡ng á»©ng vá»›i cÃ¢u tráº£ lá»i. HÃ£y xem chÃºng ta nháº­n Ä‘Æ°á»£c gÃ¬ khi Ä‘áº·t má»™t cÃ¢u há»i cá»¥ thá»ƒ vá» pháº£n há»“i cá»§a khÃ¡ch hÃ ng:

```python
reader = pipeline("question-answering")
question = "What does the customer want ?"
outputs = reader(question = question, context = text)
pd.DataFrame([outputs])
```

![anh](./image/283.png)


- ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¹ng vá»›i cÃ¢u tráº£ lá»i, `pipeline` cÅ©ng tráº£ vá» cÃ¡c chá»‰ sá»‘ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc tÆ°Æ¡ng á»©ng cÃ¢u tráº£ lá»i (giá»‘ng nhÆ° NER). Loáº¡i Ä‘áº·c biá»‡t nÃ y Ä‘Æ°á»£c gá»i lÃ  tráº£ lá»i cÃ¢u há»i trÃ­ch xuáº¥t tá»« vÄƒn báº£n (extractive question answering) vÃ¬ cÃ¢u tráº£ lá»i Ä‘Æ°á»£c trÃ­ch xuáº¥t trá»±c tiáº¿p tá»« vÄƒn báº£n. Táº¥t nhiÃªn, Ä‘Ã¢y cÃ³ thá»ƒ lÃ  cÃ¢u tráº£ lá»i khÃ´ng tá»‘t cho cÃ¡c cÃ¢u há»i khÃ¡c nhau, tÃ´i sáº½ viáº¿t má»™t blog khÃ¡c chi tiáº¿t hÆ¡n vá» question answering mÃ  model cÃ³ thá»ƒ tá»± do tráº£ lá»i cÃ¢u há»i.

- Vá»›i cÃ¡ch tiáº¿p cáº­n nÃ y, báº¡n cÃ³ thá»ƒ Ä‘á»c vÃ  trÃ­ch xuáº¥t thÃ´ng tin liÃªn quan má»™t cÃ¡ch nhanh chÃ³ng tá»« pháº£n há»“i cá»§a khÃ¡ch hÃ ng. NhÆ°ng Ä‘iá»u gÃ¬ sáº½ xáº£y ra náº¿u báº¡n nháº­n Ä‘Æ°á»£c má»™t nÃºi nhá»¯ng lá»i phÃ n nÃ n dÃ i dÃ²ng vÃ  báº¡n khÃ´ng cÃ³ thá»i gian Ä‘á»ƒ Ä‘á»c táº¥t cáº£? HÃ£y xem liá»‡u má»™t mÃ´ hÃ¬nh tÃ³m táº¯t cÃ³ thá»ƒ giÃºp Ã­ch hay khÃ´ng !

### TÃ³m táº¯t (Summarization)

- Má»¥c tiÃªu cá»§a tÃ³m táº¯t vÄƒn báº£n lÃ  láº¥y má»™t vÄƒn báº£n dÃ i lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra má»™t phiÃªn báº£n ngáº¯n vá»›i táº¥t cáº£ cÃ¡c sá»± kiá»‡n cÃ³ liÃªn quan. ÄÃ¢y lÃ  má»™t nhiá»‡m vá»¥ phá»©c táº¡p hÆ¡n nhiá»u so vá»›i nhá»¯ng nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã¢y vÃ¬ nÃ³ yÃªu cáº§u mÃ´ hÃ¬nh táº¡o ra vÄƒn báº£n máº¡ch láº¡c. ChÃºng ta cÃ³ thá»ƒ khá»Ÿi táº¡o má»™t `pipeline` nhÆ° sau:

```python
summarizer = pipeline("summarization")
outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)
print(outputs[0]['summary_text'])
# Outpus: Bumblebee ordered an Optimus Prime action figure from your online store in
#Germany. Unfortunately, when I opened the package, I discovered to my horror
#that I had been sent an action figure of Megatron instead.
```

- Báº£n tÃ³m táº¯t nÃ y khÃ´ng quÃ¡ tá»‡! Máº·c dÃ¹ má»™t pháº§n cá»§a vÄƒn báº£n gá»‘c Ä‘Ã£ Ä‘Æ°á»£c sao chÃ©p, mÃ´ hÃ¬nh Ä‘Ã£ cÃ³ thá»ƒ náº¯m báº¯t Ä‘Æ°á»£c báº£n cháº¥t cá»§a váº¥n Ä‘á» vÃ  xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c ráº±ng "Bumblebee" (xuáº¥t hiá»‡n á»Ÿ cuá»‘i) lÃ  tÃ¡c giáº£ cá»§a khiáº¿u náº¡i. Trong vÃ­ dá»¥ nÃ y, báº¡n cÅ©ng cÃ³ thá»ƒ tháº¥y ráº±ng chÃºng ta Ä‘Ã£ chuyá»ƒn tham sá»‘ nhÆ° `max_length` vÃ  `clean_up_tokenization_spaces` vÃ o `pipeline`. Äiá»u nÃ y cho phÃ©p chÃºng ta Ä‘iá»u chá»‰nh output trong thá»i gian cháº¡y.
- NhÆ°ng Ä‘iá»u gÃ¬ sáº½ xáº£y ra khi báº¡n nháº­n Ä‘Æ°á»£c pháº£n há»“i báº±ng ngÃ´n ngá»¯ mÃ  báº¡n khÃ´ng hiá»ƒu? Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng Google Dá»‹ch hoáº·c báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng transformer cá»§a riÃªng mÃ¬nh Ä‘á»ƒ chuyá»ƒn Ä‘á»•i nÃ³ cho báº¡n !

### Dá»‹ch vÄƒn báº£n (Translation)

- Giá»‘ng nhÆ° tÃ³m táº¯t, dá»‹ch vÄƒn báº£n lÃ  má»™t nhiá»‡m vá»¥ trong Ä‘Ã³ Ä‘áº§u ra bao gá»“m vÄƒn báº£n Ä‘Æ°á»£c dá»‹ch. HÃ£y sá»­ dá»¥ng má»™t `pipeline` Ä‘á»ƒ dá»‹ch má»™t vÄƒn báº£n tiáº¿ng Anh sang tiáº¿ng Äá»©c:

```python
translator = pipeline("translation_en_to_de",
 model="Helsinki-NLP/opus-mt-en-de")
outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)
print(outputs[0]['translation_text'])

# Output: Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus
# Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket Ã¶ffnete,
# entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von 
# Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich
# hoffe, Sie kÃ¶nnen mein Dilemma verstehen. Um das Problem zu lÃ¶sen, Ich fordere
# einen Austausch von Megatron fÃ¼r die Optimus Prime Figur habe ich bestellt.
# Anbei sind Kopien meiner Aufzeichnungen Ã¼ber diesen Kauf. Ich erwarte, bald von
# Ihnen zu hÃ¶ren. Aufrichtig, Bumblebee.
```

- Má»™t láº§n ná»¯a, mÃ´ hÃ¬nh nÃ y Ä‘Ã£ táº¡o ra má»™t báº£n dá»‹ch ráº¥t tá»‘t sá»­ dá»¥ng chÃ­nh xÃ¡c cÃ¡c Ä‘áº¡i tá»« chÃ­nh thá»©c cá»§a tiáº¿ng Äá»©c, nhÆ° "Ihrem" vÃ  "Sie". á» Ä‘Ã¢y chÃºng ta cÅ©ng Ä‘Ã£ chá»‰ ra cÃ¡ch báº¡n cÃ³ thá»ƒ ghi Ä‘Ã¨ mÃ´ hÃ¬nh máº·c Ä‘á»‹nh trong `pipeline` Ä‘á»ƒ chá»n mÃ´ hÃ¬nh tá»‘t nháº¥t cho á»©ng dá»¥ng cá»§a báº¡n â€” vÃ  báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y mÃ´ hÃ¬nh cho hÃ ng nghÃ¬n cáº·p ngÃ´n ngá»¯ trÃªn Hugging Face Hub. TrÆ°á»›c khi chÃºng ta lÃ¹i láº¡i má»™t bÆ°á»›c vÃ  xem xÃ©t toÃ n bá»™ há»‡ sinh thÃ¡i Hugging Face, chÃºng ta hÃ£y xem xÃ©t má»™t á»©ng dá»¥ng cuá»‘i cÃ¹ng.


### Táº¡o vÄƒn báº£n (Text Generation)


- Giáº£ sá»­ báº¡n muá»‘n cÃ³ thá»ƒ tráº£ lá»i pháº£n há»“i nhanh hÆ¡n cho pháº£n há»“i cá»§a khÃ¡ch hÃ ng báº±ng cÃ¡ch cÃ³ quyá»n truy cáº­p vÃ o chá»©c nÄƒng tá»± Ä‘á»™ng hoÃ n thÃ nh. Vá»›i mÃ´ hÃ¬nh táº¡o vÄƒn báº£n, báº¡n cÃ³ thá»ƒ thá»±c hiá»‡n viá»‡c nÃ y nhÆ° sau:

```python
generator = pipeline("text-generation")
response = "Dear Bumblebee, I am sorry to hear that your order was mixed up."
prompt = text + "\n\nCustomer service response:\n" + response
outputs = generator(prompt, max_length=200)
print(outputs[0]['generated_text'])
# Ouput: 
# Dear Amazon, last week I ordered an Optimus Prime action figure from your online
# store in Germany. Unfortunately, when I opened the package, I discovered to my
# horror that I had been sent an action figure of Megatron instead! As a lifelong
# enemy of the Decepticons, I hope you can understand my dilemma. To resolve the
# issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.
# Enclosed are copies of my records concerning this purchase. I expect to hear
# from you soon. Sincerely, Bumblebee.
# Customer service response:
# Dear Bumblebee, I am sorry to hear that your order was mixed up. The order was
# completely mislabeled, which is very common in our online store, but I can
# appreciate it because it was my understanding from this site and our customer
# service of the previous day that your order was not made correct in our mind and
# that we are in a process of resolving this matter. We can assure you that your
# order
```

- BÃ¢y giá» báº¡n Ä‘Ã£ tháº¥y má»™t vÃ i á»©ng dá»¥ng thÃº vá»‹ cá»§a cÃ¡c mÃ´ hÃ¬nh transformers, báº¡n cÃ³ thá»ƒ tá»± há»i viá»‡c training diá»…n ra á»Ÿ Ä‘Ã¢u. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh mÃ  chÃºng ta Ä‘Ã£ sá»­ dá»¥ng trong blog nÃ y Ä‘á»u cÃ³ sáºµn cÃ´ng khai vÃ  Ä‘Ã£ Ä‘Æ°á»£c fine-tuned cho nhiá»‡m vá»¥ cá»¥ thá»ƒ. Tuy nhiÃªn, nÃ³i chung, báº¡n sáº½ muá»‘n fine-tuned cÃ¡c mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u cá»§a riÃªng mÃ¬nh vÃ  trong cÃ¡c blog khÃ¡c, báº¡n sáº½ há»c cÃ¡ch thá»±c hiá»‡n Ä‘iá»u Ä‘Ã³.
- NhÆ°ng train má»™t mÃ´ hÃ¬nh chá»‰ lÃ  má»™t pháº§n nhá» cá»§a báº¥t ká»³ dá»± Ã¡n NLP nÃ o â€” cÃ³ thá»ƒ xá»­ lÃ½ dá»¯ liá»‡u má»™t cÃ¡ch hiá»‡u quáº£, chia sáº» káº¿t quáº£ vá»›i ngÆ°á»i khÃ¡c vÃ  lÃ m cho cÃ´ng viá»‡c cá»§a báº¡n cÃ³ thá»ƒ tÃ¡i táº¡o cÅ©ng lÃ  nhá»¯ng thÃ nh pháº§n quan trá»ng. May máº¯n thay, ğŸ¤— Transformers Ä‘Æ°á»£c bao quanh bá»Ÿi má»™t há»‡ sinh thÃ¡i lá»›n gá»“m cÃ¡c cÃ´ng cá»¥ há»¯u Ã­ch há»— trá»£ pháº§n lá»›n `pipeline` há»c mÃ¡y hiá»‡n Ä‘áº¡i. ChÃºng ta hÃ£y xem xÃ©t.

## 6. The Hugging Face Ecosystem

- Nhá»¯ng gÃ¬ báº¯t Ä‘áº§u vá»›i ğŸ¤— Transformers Ä‘Ã£ nhanh chÃ³ng phÃ¡t triá»ƒn thÃ nh má»™t há»‡ sinh thÃ¡i bao gá»“m nhiá»u thÆ° viá»‡n vÃ  cÃ´ng cá»¥ Ä‘á»ƒ tÄƒng tá»‘c cÃ¡c dá»± Ã¡n NLP vÃ  mÃ¡y há»c cá»§a báº¡n. Há»‡ sinh thÃ¡i Hugging Face chá»§ yáº¿u bao gá»“m hai pháº§n: má»™t há» thÆ° viá»‡n (libraries) vÃ  Hub, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 1-9. CÃ¡c thÆ° viá»‡n cung cáº¥p code trong khi Hub cung cáº¥p trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c, táº­p dá»¯ liá»‡u, táº­p lá»‡nh cho cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (metrics), v.v. Trong pháº§n nÃ y, chÃºng ta sáº½ cÃ³ má»™t cÃ¡i nhÃ¬n ngáº¯n gá»n vá» cÃ¡c thÃ nh pháº§n khÃ¡c nhau. 

![anh](./image/284.png)

### The Hugging Face Hub

- NhÆ° Ä‘Ã£ nÃªu trÆ°á»›c Ä‘Ã³, há»c chuyá»ƒn tiáº¿p lÃ  má»™t trong nhá»¯ng yáº¿u tá»‘ quan trá»ng thÃºc Ä‘áº©y sá»± thÃ nh cÃ´ng cá»§a transformers vÃ¬ nÃ³ cho phÃ©p sá»­ dá»¥ng láº¡i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cho cÃ¡c nhiá»‡m vá»¥ má»›i. Do Ä‘Ã³, Ä‘iá»u quan trá»ng lÃ  cÃ³ thá»ƒ táº£i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c má»™t cÃ¡ch nhanh chÃ³ng vÃ  cháº¡y thá»­ nghiá»‡m vá»›i chÃºng.
- Hugging Face Hub lÆ°u trá»¯ gáº§n 1.5 triá»‡u mÃ´ hÃ¬nh cÃ³ sáºµn miá»…n phÃ­. NhÆ° trong hÃ¬nh dÆ°á»›i, cÃ³ cÃ¡c filters cho cÃ¡c tasks, framework, dataset, v.v. Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giÃºp báº¡n Ä‘iá»u hÆ°á»›ng Hub vÃ  nhanh chÃ³ng tÃ¬m tháº¥y cÃ¡c cÃ´ng cá»¥ báº¡n muá»‘n. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y vá»›i `pipeline`, viá»‡c táº£i má»™t mÃ´ hÃ¬nh trong code cá»§a báº¡n thá»±c sá»± chá»‰ cáº§n má»™t dÃ²ng code. Äiá»u nÃ y lÃ m cho viá»‡c thá»­ nghiá»‡m vá»›i má»™t loáº¡t cÃ¡c mÃ´ hÃ¬nh trá»Ÿ nÃªn Ä‘Æ¡n giáº£n vÃ  cho phÃ©p báº¡n táº­p trung vÃ o cÃ¡c pháº§n khÃ¡c cá»§a dá»± Ã¡n.

![anh](./image/285.png)

- NgoÃ i trá»ng sá»‘ mÃ´ hÃ¬nh, Hub cÅ©ng lÆ°u trá»¯ cÃ¡c bá»™ dá»¯ liá»‡u vÃ  táº­p lá»‡nh Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ (metrics), cho phÃ©p báº¡n tÃ¡i táº¡o cÃ¡c káº¿t quáº£ Ä‘Ã£ xuáº¥t báº£n hoáº·c táº­n dá»¥ng dá»¯ liá»‡u bá»• sung cho á»©ng dá»¥ng cá»§a mÃ¬nh.

- Hub cÅ©ng cung cáº¥p model vÃ  dataset cards Ä‘á»ƒ ghi láº¡i ná»™i dung cá»§a cÃ¡c mÃ´ hÃ¬nh vÃ  bá»™ dá»¯ liá»‡u, Ä‘á»“ng thá»i giÃºp báº¡n Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh sÃ¡ng suá»‘t vá» viá»‡c liá»‡u chÃºng cÃ³ phÃ¹ há»£p vá»›i báº¡n hay khÃ´ng. Má»™t trong nhá»¯ng tÃ­nh nÄƒng thÃº vá»‹ nháº¥t cá»§a Hub lÃ  báº¡n cÃ³ thá»ƒ thá»­ báº¥t ká»³ mÃ´ hÃ¬nh nÃ o trá»±c tiáº¿p thÃ´ng qua cÃ¡c widget tÆ°Æ¡ng tÃ¡c dÃ nh riÃªng cho nhiá»‡m vá»¥ khÃ¡c nhau nhÆ° trong HÃ¬nh 1-11.

![anh](./image/286.png)

- HÃ£y tiáº¿p tá»¥c tÃ¬m hiá»ƒu vá» ğŸ¤— Tokenizers.

### Hugging Face Tokenizers

- Äáº±ng sau má»—i vÃ­ dá»¥ vá» `pipeline` mÃ  chÃºng ta Ä‘Ã£ tháº¥y trong blog nÃ y lÃ  tokenization - chia vÄƒn báº£n thÃ´ thÃ nh cÃ¡c pháº§n nhá» hÆ¡n Ä‘Æ°á»£c gá»i lÃ  token. ChÃºng ta sáº½ xem chi tiáº¿t Ä‘iá»u nÃ y hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o trong blog khÃ¡c, nhÆ°ng bÃ¢y giá» chá»‰ cáº§n hiá»ƒu ráº±ng token cÃ³ thá»ƒ lÃ  tá»«, má»™t pháº§n cá»§a tá»« hoáº·c cÃ³ thá»ƒ chá»‰ lÃ  cÃ¡c kÃ½ tá»±. CÃ¡c mÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn cÃ¡c biá»ƒu diá»…n sá»‘ cá»§a cÃ¡c token nÃ y, vÃ¬ váº­y viá»‡c thá»±c hiá»‡n Ä‘Ãºng bÆ°á»›c nÃ y lÃ  khÃ¡ quan trá»ng Ä‘á»‘i vá»›i toÃ n bá»™ dá»± Ã¡n NLP!
- [ğŸ¤— Tokenizers](https://github.com/huggingface/transformers) cung cáº¥p nhiá»u chiáº¿n lÆ°á»£c token vÃ  cá»±c ká»³ nhanh chÃ³ng trong viá»‡c tokenize vÄƒn báº£n nhá» pháº§n Rust backend (Rust lÃ  má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh mÃ  cÃ³ hiá»‡u suáº¥t cao). NÃ³ cÅ©ng Ä‘áº£m nháº­n táº¥t cáº£ cÃ¡c bÆ°á»›c xá»­ lÃ½ trÆ°á»›c vÃ  sau, cháº³ng háº¡n nhÆ° chuáº©n hÃ³a Ä‘áº§u vÃ o vÃ  chuyá»ƒn Ä‘á»•i Ä‘áº§u ra mÃ´ hÃ¬nh sang Ä‘á»‹nh dáº¡ng cáº§n thiáº¿t. Vá»›i ğŸ¤— Tokenizers, chÃºng ta cÃ³ thá»ƒ táº£i tokenizer giá»‘ng nhÆ° cÃ¡ch chÃºng ta cÃ³ thá»ƒ táº£i trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vá»›i ğŸ¤— Transformers.
- ChÃºng ta cáº§n dataset vÃ  metrics Ä‘á»ƒ train vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh, vÃ¬ váº­y chÃºng ta hÃ£y xem xÃ©t qua vá» ğŸ¤— Datasets.

### Hugging Face Dataset

- Táº£i, xá»­ lÃ½ vÃ  lÆ°u trá»¯ dataset cÃ³ thá»ƒ lÃ  má»™t quÃ¡ trÃ¬nh rÆ°á»m rÃ , Ä‘áº·c biá»‡t lÃ  khi bá»™ dá»¯ liá»‡u quÃ¡ lá»›n Ä‘á»ƒ vá»«a vá»›i RAM cá»§a mÃ¡y tÃ­nh xÃ¡ch tay cá»§a báº¡n. NgoÃ i ra, báº¡n thÆ°á»ng cáº§n triá»ƒn khai cÃ¡c táº­p lá»‡nh khÃ¡c nhau Ä‘á»ƒ táº£i xuá»‘ng dá»¯ liá»‡u vÃ  chuyá»ƒn Ä‘á»•i nÃ³ thÃ nh Ä‘á»‹nh dáº¡ng tiÃªu chuáº©n.
- [ğŸ¤— Datasets](https://github.com/huggingface/datasets) Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh nÃ y báº±ng cÃ¡ch cung cáº¥p giao diá»‡n tiÃªu chuáº©n cho hÃ ng nghÃ¬n bá»™ dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trÃªn [Hub](https://huggingface.co/datasets). NÃ³ cÅ©ng cung cáº¥p bá»™ nhá»› Ä‘á»‡m thÃ´ng minh (vÃ¬ váº­y báº¡n khÃ´ng pháº£i lÃ m láº¡i quÃ¡ trÃ¬nh xá»­ lÃ½ trÆ°á»›c má»—i khi cháº¡y mÃ£) vÃ  trÃ¡nh cÃ¡c háº¡n cháº¿ vá» RAM báº±ng cÃ¡ch táº­n dá»¥ng má»™t cÆ¡ cháº¿ Ä‘áº·c biá»‡t Ä‘Æ°á»£c gá»i lÃ  Ã¡nh xáº¡ bá»™ nhá»› lÆ°u trá»¯ ná»™i dung cá»§a tá»‡p trong bá»™ nhá»› áº£o vÃ  cho phÃ©p nhiá»u quy trÃ¬nh sá»­a Ä‘á»•i tá»‡p hiá»‡u quáº£ hÆ¡n. ThÆ° viá»‡n cÅ©ng cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c framework phá»• biáº¿n nhÆ° Pandas vÃ  NumPy, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ dÃ¹ng thoáº£i mÃ¡i cÃ¡c framework trÃªn mÃ  khÃ´ng cáº§n pháº£i lo gÃ¬.
- Tuy nhiÃªn, cÃ³ má»™t bá»™ dá»¯ liá»‡u tá»‘t vÃ  mÃ´ hÃ¬nh máº¡nh máº½ lÃ  vÃ´ giÃ¡ trá»‹ náº¿u báº¡n khÃ´ng thá»ƒ Ä‘o lÆ°á»ng hiá»‡u suáº¥t má»™t cÃ¡ch Ä‘Ã¡ng tin cáº­y. Tháº­t khÃ´ng may, cÃ¡c chá»‰ sá»‘ NLP cá»• Ä‘iá»ƒn Ä‘i kÃ¨m vá»›i nhiá»u cÃ¡ch triá»ƒn khai khÃ¡c nhau cÃ³ thá»ƒ thay Ä‘á»•i má»™t chÃºt vÃ  dáº«n Ä‘áº¿n káº¿t quáº£ lá»«a Ä‘áº£o. Báº±ng cÃ¡ch cung cáº¥p cÃ¡c táº­p lá»‡nh cho nhiá»u chá»‰ sá»‘, ğŸ¤— Datasets giÃºp lÃ m cho cÃ¡c thá»­ nghiá»‡m dá»… tÃ¡i táº¡o hÆ¡n vÃ  káº¿t quáº£ Ä‘Ã¡ng tin cáº­y hÆ¡n.
- Vá»›i cÃ¡c thÆ° viá»‡n ğŸ¤— Transformers, ğŸ¤— Tokenizers vÃ  ğŸ¤— Datasets, chÃºng ta Ä‘Ã£ cÃ³ má»i thá»© chÃºng ta cáº§n Ä‘á»ƒ train cÃ¡c mÃ´ hÃ¬nh transformer cá»§a riÃªng mÃ¬nh! Tuy nhiÃªn, nhÆ° chÃºng ta sáº½ tháº¥y trong blog khÃ¡c, cÃ³ nhá»¯ng tÃ¬nh huá»‘ng mÃ  chÃºng ta cáº§n kiá»ƒm soÃ¡t fine-grained Ä‘á»‘i vá»›i training loop. ÄÃ³ lÃ  nÆ¡i thÆ° viá»‡n cuá»‘i cÃ¹ng cá»§a há»‡ sinh thÃ¡i phÃ¡t huy tÃ¡c dá»¥ng: ğŸ¤— Accelerate.

### Hugging Face Accelerate

- Náº¿u báº¡n Ä‘Ã£ tá»«ng pháº£i viáº¿t code báº±ng PyTorch, ráº¥t cÃ³ thá»ƒ báº¡n Ä‘Ã£ gáº·p má»™t sá»‘ váº¥n Ä‘á» Ä‘au Ä‘áº§u khi cá»‘ gáº¯ng chuyá»ƒn code cháº¡y trÃªn mÃ¡y tÃ­nh xÃ¡ch tay cá»§a báº¡n sang code cháº¡y trÃªn cá»¥m cá»§a tá»• chá»©c báº¡n. ğŸ¤— Accelerate thÃªm má»™t lá»›p trá»«u tÆ°á»£ng vÃ o cÃ¡c training loop Ä‘á»ƒ xá»­ lÃ½ táº¥t cáº£ cÃ¡c logic tÃ¹y chá»‰nh cáº§n thiáº¿t cho cÆ¡ sá»Ÿ háº¡ táº§ng Ä‘Ã o táº¡o. Äiá»u nÃ y thá»±c sá»± Ä‘áº©y nhanh quy trÃ¬nh lÃ m viá»‡c cá»§a báº¡n báº±ng cÃ¡ch Ä‘Æ¡n giáº£n hÃ³a viá»‡c thay Ä‘á»•i cÆ¡ sá»Ÿ háº¡ táº§ng khi cáº§n thiáº¿t.

- Äiá»u nÃ y tÃ³m táº¯t cÃ¡c thÃ nh pháº§n cá»‘t lÃµi cá»§a Hugging Face Ecosystem. NhÆ°ng trÆ°á»›c khi káº¿t thÃºc chÆ°Æ¡ng nÃ y, chÃºng ta hÃ£y xem xÃ©t má»™t sá»‘ thÃ¡ch thá»©c phá»• biáº¿n khi cá»‘ gáº¯ng triá»ƒn khai transformer trong tháº¿ giá»›i thá»±c.

## 7. Nhá»¯ng thá»­ thÃ¡ch chÃ­nh vá»›i Transformers

- Trong blog nÃ y, chÃºng ta Ä‘Ã£ cÃ³ cÃ¡i nhÃ¬n thoÃ¡ng qua vá» má»™t loáº¡t cÃ¡c tÃ¡c vá»¥ NLP cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i cÃ¡c mÃ´ hÃ¬nh transformers. Vá»›i sá»± truyá»n thÃ´ng máº¡nh máº½, cÃ³ váº» nhÆ° kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh nÃ y lÃ  vÃ´ háº¡n. Tuy nhiÃªn, báº¥t cháº¥p tÃ­nh há»¯u Ã­ch cá»§a chÃºng, transformers cÃ²n lÃ¢u má»›i lÃ  má»™t mÃ´ hÃ¬nh cháº¥t lÆ°á»£ng Ä‘Ãºng nghÄ©a. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ thÃ¡ch thá»©c liÃªn quan Ä‘áº¿n chÃºng mÃ  chÃºng ta sáº½ khÃ¡m phÃ¡:

1. **Language:**  NghiÃªn cá»©u NLP bá»‹ chi phá»‘i bá»Ÿi ngÃ´n ngá»¯ tiáº¿ng Anh. CÃ³ má»™t sá»‘ mÃ´ hÃ¬nh cho cÃ¡c ngÃ´n ngá»¯ khÃ¡c, nhÆ°ng khÃ³ tÃ¬m tháº¥y cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cho cÃ¡c ngÃ´n ngá»¯ hiáº¿m hoáº·c Ã­t tÃ i nguyÃªn.
2. **Data availability**: Máº·c dÃ¹ chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng há»c chuyá»ƒn tiáº¿p Ä‘á»ƒ giáº£m Ä‘Ã¡ng ká»ƒ lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n mÃ  cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng ta cáº§n, nhÆ°ng nÃ³ váº«n cÃ²n quÃ¡ Ã­t Ä‘á»ƒ so sÃ¡nh vá»›i dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n mÃ  con ngÆ°á»i cáº§n Ä‘á»ƒ thá»±c hiá»‡n nhiá»‡m vá»¥ Ä‘Ã³.
3. **Working with long documents:** Self-attention hoáº¡t Ä‘á»™ng cá»±c ká»³ tá»‘t trÃªn cÃ¡c vÄƒn báº£n dÃ i Ä‘oáº¡n vÄƒn, nhÆ°ng nÃ³ trá»Ÿ nÃªn ráº¥t tá»‘n kÃ©m khi chÃºng ta chuyá»ƒn sang cÃ¡c vÄƒn báº£n dÃ i hÆ¡n ná»¯a nhÆ° toÃ n bá»™ tÃ i liá»‡u.
4. **Opacity**: CÅ©ng nhÆ° cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u khÃ¡c, transformers á»Ÿ má»™t má»©c Ä‘á»™ lá»›n lÃ  má» Ä‘á»¥c (khÃ³ hiá»ƒu táº¡i sao mÃ´ hÃ¬nh láº¡i hoáº¡t Ä‘á»™ng nhÆ° váº­y). Tháº­t khÃ³ hoáº·c khÃ´ng thá»ƒ lÃ m sÃ¡ng tá» "táº¡i sao" má»™t mÃ´ hÃ¬nh Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n nháº¥t Ä‘á»‹nh. ÄÃ¢y lÃ  má»™t thÃ¡ch thá»©c Ä‘áº·c biá»‡t khÃ³ khÄƒn khi cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c triá»ƒn khai Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c quyáº¿t Ä‘á»‹nh quan trá»ng.
5. **Bias**: CÃ¡c mÃ´ hÃ¬nh Transformer chá»§ yáº¿u Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trÃªn dá»¯ liá»‡u vÄƒn báº£n tá»« internet. Äiá»u nÃ y in nhá»¯ng thÃ´ng tin pháº£n cáº£m, sai lá»‡ch cÃ³ trong dá»¯ liá»‡u vÃ o cÃ¡c mÃ´ hÃ¬nh. Äáº£m báº£o ráº±ng nhá»¯ng Ä‘iá»u nÃ y khÃ´ng phÃ¢n biá»‡t chá»§ng tá»™c, phÃ¢n biá»‡t giá»›i tÃ­nh hoáº·c nhá»¯ng thÃ´ng tin tá»‡ hÆ¡n lÃ  má»™t nhiá»‡m vá»¥ Ä‘áº§y thÃ¡ch thá»©c.

- Máº·c dÃ¹ khÃ³ khÄƒn, nhÆ°ng nhiá»u thÃ¡ch thá»©c trong sá»‘ nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c vÆ°á»£t qua. ChÃºng ta sáº½ Ä‘á» cáº­p Ä‘áº¿n cÃ¡ch vÆ°á»£t qua nhá»¯ng thá»­ thÃ¡ch nÃ y á»Ÿ blog khÃ¡c.

## 8. Tá»•ng káº¿t

- Hy vá»ng ráº±ng bÃ¢y giá» báº¡n Ä‘Ã£ hÃ o há»©ng há»c cÃ¡ch báº¯t Ä‘áº§u Ä‘Ã o táº¡o vÃ  tÃ­ch há»£p cÃ¡c mÃ´ hÃ¬nh linh hoáº¡t nÃ y vÃ o cÃ¡c á»©ng dá»¥ng cá»§a riÃªng báº¡n! Báº¡n Ä‘Ã£ tháº¥y trong chÆ°Æ¡ng nÃ y ráº±ng chá»‰ vá»›i má»™t vÃ i dÃ²ng mÃ£, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i Ä‘á»ƒ phÃ¢n loáº¡i, nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn, tráº£ lá»i cÃ¢u há»i, dá»‹ch vÃ  tÃ³m táº¯t, nhÆ°ng Ä‘Ã¢y thá»±c sá»± chá»‰ lÃ  má»™t pháº§n ráº¥t nhá», cÃ²n nhiá»u pháº§n khÃ¡c Ä‘ang chá» báº¡n phÃ­a trÆ°á»›c.

